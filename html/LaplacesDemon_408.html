<div class="container">

<table style="width: 100%;"><tr>
<td>dist.Zellner</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Hyperprior-g Prior and Zellner's g-Prior</h2>

<h3>Description</h3>

<p>These functions provide the density of the hyper-g prior (Liang et
al., 2008), and both the density and random generation of Zellner's
g-prior (Zellner, 1986).
</p>


<h3>Usage</h3>

<pre><code class="language-R">dhyperg(g, alpha=3, log=FALSE)
dzellner(beta, g, sigma, X, log=FALSE)
rzellner(n, g, sigma, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>This is a positive scale hyperhyperparameter that is
proper when <code class="reqn">\alpha &gt; 2</code>. The default is
<code>alpha=3</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>This is regression effects <code class="reqn">\beta</code>, a vector of
length <code class="reqn">J</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>g</code></td>
<td>
<p>This is hyperparameter <code class="reqn">g</code>, a positive scalar.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>This is the number of random deviates to generate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma</code></td>
<td>
<p>This is the residual standard deviation
<code class="reqn">\sigma</code>, a positive scalar.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>This is a full-rank <code class="reqn">N \times J</code> design matrix
<code class="reqn">\textbf{X}</code> for <code class="reqn">N</code> records and <code class="reqn">J</code> predictors,
where <code class="reqn">J+1 &lt; N</code>. Zellner's g-prior has been extended (elsewhere)
via singular value decomposition (SVD) to the case where
<code class="reqn">J &gt; N</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td>
</tr>
</table>
<h3>Details</h3>


<ul>
<li>
<p> Application: Continuous Multivariate
</p>
</li>
<li>
<p> Density: <code class="reqn">p(\theta) = \frac{1}{(2\pi)^{J/2}|(g
      \sigma^2(\textbf{X}^T \textbf{X})^{-1})^{-1}|^{1/2}}
    \exp(-\frac{1}{2}(\theta - \mu)'(g \sigma^2(\textbf{X}^T
    \textbf{X})^{-1})^{-1}(\theta - \mu))</code>
</p>
</li>
<li>
<p> Inventor: Zellner, A. (1986)
</p>
</li>
<li>
<p> Notation 1: <code class="reqn">\theta \sim
    \mathrm{N}_J(0, g \sigma^2(\textbf{X}^T \textbf{X})^{-1})</code>
</p>
</li>
<li>
<p> Notation 2: <code class="reqn">p(\theta) = \mathrm{N}_J(\theta | g, \sigma^2,
    \textbf{X})</code>
</p>
</li>
<li>
<p> Parameter 1: location parameter <code class="reqn">\beta</code>
</p>
</li>
<li>
<p> Parameter 2: scale parameter <code class="reqn">g &gt; 0</code>
</p>
</li>
<li>
<p> Parameter 3: scale parameter <code class="reqn">\sigma^2 &gt; 0</code>
</p>
</li>
<li>
<p> Mean:
</p>
</li>
<li>
<p> Variance:
</p>
</li>
<li>
<p> Mode:
</p>
</li>
</ul>
<p>Zellner's g-prior is a popular, data-dependent, elliptical, improper,
least-informative prior distribution on regression effects
<code class="reqn">\beta</code> in a Gaussian regression model. It is a particular
form in the conjugate Normal-Gamma family. Zellner's g-prior is also
used for estimating Bayes factors (for hypothesis testing) with a
simpler form, as well as in model selection and variable selection. The
marginal posterior distribution of regression effects <code class="reqn">\beta</code>
is multivariate t.
</p>
<p>One of many nice properties of Zellner's g-prior is that it adapts
automatically to near-collinearity between different
predictors. Zellner's g-prior puts most of its prior mass in the
direction that causes the regression coefficients of correlated
predictors to be smoothed away from each other. When coupled with model
selection, Zellner's g-prior discourages highly collinear predictors
from entering the models simultaneously by inducing a negative
correlation between the coefficients. However, when it is desirable for
collinear predictors to enter simultaneously, a modification has been
proposed (though not included here) in which
<code class="reqn">(\textbf{X}^T \textbf{X})^{-1}</code> is replaced with
<code class="reqn">(\textbf{X}^T \textbf{X})^\lambda</code>. For more
information, see Krishna et al. (2009).
</p>
<p>For variable selection, large values of <code class="reqn">g</code>, with a prior mean of
zero for <code class="reqn">\beta</code>, encourage models with few, large
coefficients. Conversely, small values of <code class="reqn">g</code> encourage saturated
models with many, small coefficients.
</p>
<p>The design matrix <code class="reqn">\textbf{X}</code> is converted to Fisher's
information matrix, which is used as a covariance matrix for
<code class="reqn">\beta</code>. This is computationally efficient, because each
element of the covariance matrix does not need to be estimated as a
parameter. When <code class="reqn">\textbf{X}</code> is nearly singular, regression
effects <code class="reqn">\beta</code> may be poorly estimated.
</p>
<p>Hyperparameter <code class="reqn">g</code> acts as an inverse relative prior sample size, or
as a dimensionality penalty. Zellner (1986) recommended that a
hyperprior distribution is assigned to <code class="reqn">g</code> so that it is estimated
from the data, although in practice <code class="reqn">g</code> has often been fixed, usually
to <code class="reqn">N</code> when no information is available, since it has the
interpretation of adding prior information equivalent to one
observation. A variety of hyperpriors have been suggested for <code class="reqn">g</code>,
such as in Bove and Held (2011), Liang et al. (2008), and Maruyama and
George (2011). <code class="reqn">g</code> becomes diffuse as it approaches infinity, and
the Bayes factor approaches zero. The hyper-g prior of Liang et al.
(2008) is proper when <code class="reqn">\alpha &gt; 2</code>, and any value in
the interval <code class="reqn">(2,4]</code> may be reasonable.
</p>


<h3>Value</h3>

<p><code>dhyperg</code> gives the density of the hyper-g prior of Liang et
al. (2008), <code>dzellner</code> gives the density of Zellner's g-prior,
and <code>rzellner</code> generates random deviates.
</p>


<h3>References</h3>

<p>Bove, D.S. and Held, L. (2011). "Hyper-g Priors for Generalized
Linear Models". <em>Bayesian Analysis</em>, 6(3), p. 387–410.
</p>
<p>Krishna, A., Bondell, H.D., and Ghosh, S.K. (2009). "Bayesian Variable
Selection Using an Adaptive Powered Correlation Prior". <em>Journal
of Statistical Planning Inference</em>, 139(8), p. 2665-2674..
</p>
<p>Liang, F., Paulo, R., Molina, G., Clyde, M.A., and Berger,
J.O. (2008). "Mixtures of g Priors for Bayesian Variable
Selection". <em>Journal of the American Statistical Association</em>,
103, p. 410–423.
</p>
<p>Maruyama, Y. and George, E.I. (2011). "Fully Bayes Factors with a
Generalised g-Prior". <em>Annals of Statistics</em>, 39, p. 2740–2765.
</p>
<p>Zellner, A. (1986). "On Assessing Prior Distributions and Bayesian
Regression Analysis with g-Prior Distributions". In <em>Bayesian
Inference and Decision Techniques: Essays in Honor of Bruno de
Finetti</em>, p. 233–243. Elsevier: Amsterdam, North Holland.
</p>


<h3>See Also</h3>

<p><code>BayesFactor</code> and
<code>dmvt</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(LaplacesDemon)
set.seed(667)
beta &lt;- rnorm(10)
g &lt;- 100
sigma &lt;- 2
X &lt;- cbind(1,matrix(rnorm(100*9),100,9))
dhyperg(g, alpha=3)
dzellner(beta, g, sigma, X)
rzellner(1, g, sigma, X)
</code></pre>


</div>