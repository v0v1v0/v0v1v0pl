<div class="container">

<table style="width: 100%;"><tr>
<td>blhs</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Bootstrapped block Latin hypercube subsampling
</h2>

<h3>Description</h3>

<p>Provides bootstrapped block Latin hypercube subsampling under a given 
data set to aid in consistent estimation of a global separable lengthscale 
parameter
</p>


<h3>Usage</h3>

<pre><code class="language-R">  blhs(y, X, m)
  blhs.loop(y, X, m, K, da, g = 1e-3, maxit = 100, verb = 0, plot.it = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p> a vector of responses/dependent values with <code>length(y) = nrow(X)</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p> a <code>matrix</code> or <code>data.frame</code> containing the full (large) design matrix of input locations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p> a positive scalar integer giving the number of divisions on each coordinate of input space defining the block structure </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p> a positive scalar integer specifying the number of Bootstrap replicates desired </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>da</code></td>
<td>
<p> a lengthscale prior, say as generated by <code>darg</code> </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>g</code></td>
<td>
<p> a positive scalar giving the fixed nugget value of the nugget parameter; by default <code>g = 1e-3</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p> a positive scalar integer giving the maximum number of iterations for MLE calculations via <code>"L-BFGS-B"</code>; 
see <code>mleGPsep</code> for more details</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verb</code></td>
<td>
<p> a non-negative integer specifying the verbosity level; <code>verb = 0</code> (by default) is quiet, 
and larger values cause more progress information to be printed to the screen</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot.it</code></td>
<td>
 <p><code>plot.it = FALSE</code> by default; if <code>plot.it = TRUE</code>, then each of the <code>K</code> 
lengthscale estimates from bootstrap iterations will be shown 
via <code>boxplot</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Bootstrapped block Latin hypercube subsampling (BLHS) yields a global lengthscale estimator 
which is asymptotically consistent with the MLE calculated on the full data set. However, since it
works on data subsets, it comes at a much reduced computational cost.  Intuitively, the BLHS 
guarantees a good mix of short and long pairwise distances. A single bootstrap LH subsample 
may be obtained by dividing each dimension of the input space equally into <code>m</code> 
intervals, yielding <code class="reqn">m^d</code> mutually exclusive hypercubes.  It is easy to show 
that the average number of observations in each hypercube is <code class="reqn">Nm^{-d}</code> 
if there are <code class="reqn">N</code> samples in the original design. From each of these hypercubes, 
<code>m</code> <code>blocks</code> are randomly selected following the LH paradigm, i.e., so that 
only one interval is chosen from each of the <code>m</code> segments. The average number of 
observations in the subsample, combining the <code>m</code> randomly selected blocks, 
is <code class="reqn">Nm^{-d+1}</code>. 
</p>
<p>Ensuring a subsample size of at least <code>one</code> requires having <code class="reqn">m\leq N^{\frac{1}{d-1}}</code>, 
thereby linking the parameter <code>m</code> to computational effort.  Smaller <code>m</code> is preferred so long 
as GP inference on data of that size remains tractable.  Since the blocks follow 
an LH structure, the resulting sub-design inherits the usual LHS properties, 
e.g., retaining marginal properties like univariate stratification modulo features present in the original, 
large <code>N</code>, design. 
</p>
<p>For more details, see Liu (2014), Zhao, et al. (2017) and Sun, et al. (2019).
</p>
<p><code>blhs</code> returns the subsampled input space and the corresponding responses. 
</p>
<p><code>blhs.loop</code> returns the median of the <code>K</code> lengthscale maximum likelihood estimates, the subsampled data size to which
that corresponds, and the subsampled data, including the input space and the responses, from the bootstrap iterations
</p>


<h3>Value</h3>

<p><code>blhs</code> returns
</p>
<table>
<tr style="vertical-align: top;">
<td><code>xs</code></td>
<td>
<p>the subsampled input space</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ys</code></td>
<td>
<p>the subsampled responses, <code>length(ys) = nrow(xs)</code></p>
</td>
</tr>
</table>
<p><code>blhs.loop</code> returns
</p>
<table>
<tr style="vertical-align: top;">
<td><code>that</code></td>
<td>
<p>the lengthscale estimate (median), <code>length(that) = ncol(X)</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ly</code></td>
<td>
<p>the subsampled data size (median)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xm</code></td>
<td>
<p>the subsampled input space (median)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ym</code></td>
<td>
<p>the subsampled responses (median)</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>This implementation assums that <code>X</code> has been coded to the unit cube (<code class="reqn">[0,1]^p</code>),
where <code>p = ncol(X)</code>.
</p>
<p><code>X</code> should be relatively homogeneous. A space-filling design (input) <code>X</code>
is ideal, but not required
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> and Furong Sun <a href="mailto:furongs@vt.edu">furongs@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 9.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p>F. Sun, R.B. Gramacy, B. Haaland, E. Lawrence, and A. Walker (2019).
<em>Emulating satellite drag from large simulation experiments</em>,
SIAM/ASA Journal on Uncertainty Quantification, 7(2), pp. 720-759;
preprint on arXiv:1712.00182;
<a href="https://arxiv.org/abs/1712.00182">https://arxiv.org/abs/1712.00182</a>
</p>
<p>Y. Zhao, Y. Hung, and Y. Amemiya (2017).
<em>Efficient Gaussian Process Modeling using Experimental Design-Based Subagging</em>,
Statistica Sinica, to appear;
</p>
<p>Yufan Liu (2014)
<em>Recent Advances in Computer Experiment Modeling</em>.
Ph.D. Thesis at Rutgers, The State University of New Jersey.
<a href="https://dx.doi.org/doi:10.7282/T38G8J1H">https://dx.doi.org/doi:10.7282/T38G8J1H</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">  # input space based on latin-hypercube sampling (not required)
  # two dimensional example with N=216 sized sample
  if(require(lhs)) { X &lt;- randomLHS(216, 2)  
  } else { X &lt;- matrix(runif(216*2), ncol=2) }
  # pseudo responses, not important for visualizing design
  Y &lt;- runif(216) 
  
  ## BLHS sample with m=6 divisions in each coordinate
  sub &lt;- blhs(y=Y, X=X, m=6)
  Xsub &lt;- sub$xs # the bootstrapped subsample
  
  # visualization
  plot(X, xaxt="n", yaxt="n", xlim=c(0,1), ylim=c(0,1), xlab="factor 1", 
    ylab="factor 2", col="cyan", main="BLHS")
  b &lt;- seq(0, 1, by=1/6)
  abline(h=b, v=b, col="black", lty=2)
  axis(1, at=seq (0, 1, by=1/6), cex.axis=0.8, 
    labels=expression(0, 1/6, 2/6, 3/6, 4/6, 5/6, 1))
  axis(2, at=seq (0, 1, by=1/6), cex.axis=0.8, 
    labels=expression(0, 1/6, 2/6, 3/6, 4/6, 5/6, 1), las=1)
  points(Xsub, col="red", pch=19, cex=1.25)
  
  ## Comparing global lengthscale MLE based on BLHS and random subsampling
  ## Not run: 
    # famous borehole function
    borehole &lt;- function(x){
      rw &lt;- x[1] * (0.15 - 0.05) + 0.05
      r &lt;-  x[2] * (50000 - 100) + 100
      Tu &lt;- x[3] * (115600 - 63070) + 63070
      Tl &lt;- x[5] * (116 - 63.1) + 63.1
      Hu &lt;- x[4] * (1110 - 990) + 990
      Hl &lt;- x[6] * (820 - 700) + 700
      L &lt;-  x[7] * (1680 - 1120) + 1120
      Kw &lt;- x[8] * (12045 - 9855) + 9855
      m1 &lt;- 2 * pi * Tu * (Hu - Hl)
      m2 &lt;- log(r / rw)
      m3 &lt;- 1 + 2*L*Tu/(m2*rw^2*Kw) + Tu/Tl
      return(m1/m2/m3)
    }
    
    N &lt;- 100000                   # number of observations
    if(require(lhs)) { xt &lt;- randomLHS(N, 8)   # input space
    } else { xt &lt;- matrix(runif(N*8), ncol=8) }
    yt &lt;- apply(xt, 1, borehole)  # response
    colnames(xt) &lt;- c("rw", "r", "Tu", "Tl", "Hu", "Hl", "L", "Kw")

    ## prior on the GP lengthscale parameter
    da &lt;- darg(list(mle=TRUE, max=100), xt)

    ## make space for two sets of boxplots
    par(mfrow=c(1,2))
    
    # BLHS calculating with visualization of the K MLE lengthscale estimates
    K &lt;- 10  # number of Bootstrap samples; Sun, et al (2017) uses K &lt;- 31
    sub_blhs &lt;- blhs.loop(y=yt, X=xt, K=K, m=2, da=da, maxit=200, plot.it=TRUE)
  
    # a random subsampling analog for comparison
    sn &lt;- sub_blhs$ly # extract a size that is consistent with the BLHS
    that.rand &lt;- matrix(NA, ncol=8, nrow=K)
    for(i in 1:K){
      sub &lt;- sample(1:nrow(xt), sn)
      gpsepi &lt;- newGPsep(xt[sub,], yt[sub], d=da$start, g=1e-3, dK=TRUE)
      mle &lt;- mleGPsep(gpsepi, tmin=da$min, tmax=10*da$max, ab=da$ab, maxit=200)
      deleteGPsep(gpsepi)
      that.rand[i,] &lt;- mle$d
    }

    ## put random boxplots next to BLHS ones
    boxplot(that.rand, xlab="input", ylab="theta-hat", col=2, 
      main="random subsampling")
  
## End(Not run)
</code></pre>


</div>