<div class="container">

<table style="width: 100%;"><tr>
<td>gpCappedL1Cpp</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>gpCappedL1Cpp</h2>

<h3>Description</h3>

<p>Implements cappedL1 regularization for general purpose optimization problems with C++ functions.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \min(| x_j|, \theta)</code>
</p>

<p>where <code class="reqn">\theta &gt; 0</code>. The cappedL1 penalty is identical to the lasso for
parameters which are below <code class="reqn">\theta</code> and identical to a constant for parameters
above <code class="reqn">\theta</code>. As adding a constant to the fitting function will not change its
minimum, larger parameters can stay unregularized while smaller ones are set to zero.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gpCappedL1Cpp(
  par,
  fn,
  gr,
  additionalArguments,
  regularized,
  lambdas,
  thetas,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>par</code></td>
<td>
<p>labeled vector with starting values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>additionalArguments</code></td>
<td>
<p>list with additional arguments passed to fn and gr</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The interface is inspired by optim, but a bit more restrictive. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels), a fitting
function, and a gradient function. These fitting functions <em>must</em> take an const Rcpp::NumericVector&amp; with parameter
values as first argument and an Rcpp::List&amp; as second argument
</p>
<p>CappedL1 regularization:
</p>

<ul><li>
<p> Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization.
Journal of Machine Learning Research, 11, 1081–1107.
</p>
</li></ul>
<p>For more details on GLMNET, see:
</p>

<ul>
<li>
<p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li>
<p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li>
<p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li>
</ul>
<p>For more details on ISTA, see:
</p>

<ul>
<li>
<p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li>
<p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li>
<p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li>
</ul>
<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# This example shows how to use the optimizers
# for C++ objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(Rcpp)
library(lessSEM)

linreg &lt;- '
// [[Rcpp::depends(RcppArmadillo)]]
#include &lt;RcppArmadillo.h&gt;

// [[Rcpp::export]]
double fitfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // compute the sum of squared errors:
    arma::mat sse = arma::trans(y-X*b)*(y-X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    sse *= 1.0/(2.0 * y.n_elem);
    
    // note: We must return a double, but the sse is a matrix
    // To get a double, just return the single value that is in 
    // this matrix:
      return(sse(0,0));
}

// [[Rcpp::export]]
arma::rowvec gradientfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // note: we want to return our gradients as row-vector; therefore,
  // we have to transpose the resulting column-vector:
    arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    gradients *= (.5/y.n_rows);
    
    return(gradients);
}

// https://gallery.rcpp.org/articles/passing-cpp-function-pointers/
typedef double (*fitFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;fitFunPtr&gt; fitFunPtr_t;

typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                      Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;gradientFunPtr&gt; gradientFunPtr_t;

// [[Rcpp::export]]
fitFunPtr_t fitfunPtr() {
        return(fitFunPtr_t(new fitFunPtr(&amp;fitfunction)));
}

// [[Rcpp::export]]
gradientFunPtr_t gradfunPtr() {
        return(gradientFunPtr_t(new gradientFunPtr(&amp;gradientfunction)));
}
'

Rcpp::sourceCpp(code = linreg)

ffp &lt;- fitfunPtr()
gfp &lt;- gradfunPtr()

N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4), 
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

data &lt;- list("y" = y,
             "X" = cbind(1,X))
parameters &lt;- rep(0, ncol(data$X))
names(parameters) &lt;- paste0("b", 0:(length(parameters)-1))

cL1 &lt;- gpCappedL1Cpp(par = parameters, 
                 regularized = paste0("b", 1:(length(b)-1)),
                 fn = ffp, 
                 gr = gfp, 
                 lambdas = seq(0,1,.1), 
                 thetas = seq(0.1,1,.1),
                 additionalArguments = data)

cL1@parameters

</code></pre>


</div>