<div class="container">

<table style="width: 100%;"><tr>
<td>BigData</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Big Data</h2>

<h3>Description</h3>

<p>This function enables Bayesian inference with data that is too large
for computer memory (RAM) with the simplest method: reading in batches
of data (where each batch is a section of rows), applying a function
to the batch, and combining the results.
</p>


<h3>Usage</h3>

<pre><code class="language-R">BigData(file, nrow, ncol, size=1, Method="add", CPUs=1, Type="PSOCK",
FUN, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>file</code></td>
<td>
<p>This required argument accepts a path and filename that
must refer to a .csv file, and that must contain only a numeric
matrix without a header, row names, or column names.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nrow</code></td>
<td>
<p>This required argument accepts a scalar integer that
indicates the number of rows in the big data matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ncol</code></td>
<td>
<p>This required argument accepts a scalar integer that
indicates the number of columns in the big data matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>size</code></td>
<td>
<p>This argument accepts a scalar integer that specifies the
number of rows of each batch. The last batch is not required to have
the same number of rows as the other batches. The largest possible
size, and therefore the fewest number of batches, should be
preferred.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Method</code></td>
<td>
<p>This argument accepts a scalar string, defaults to
"add", and alternatively accepts "rbind". When
<code>Method="rbind"</code>, the user-specified function <code>FUN</code> is
applied to each batch, and results are combined together by rows.
For example, if calculating <code class="reqn">\mu = \textbf{X}\beta</code> in,
say, 10 batches, then the output column vector <code class="reqn">\mu</code> is equal
to the number of rows of the big data set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FUN</code></td>
<td>
<p>This required argument accepts a user-specified function
that will be performed on each batch. The first argument in the
function must be the data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments are used within the user-specified
function. Additional arguments often refer to parameters.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Big data is defined loosely here as data that is too large for
computer memory (RAM). The <code>BigData</code> function uses the
split-apply-combine strategy with a big data set. The unmanageable
big data set is split into smaller, manageable pieces (batches),
a function is applied to each batch, and results are combined.
</p>
<p>Each iteration, the <code>BigData</code> function opens a connection to a
big data set and keeps the connection open while the <code>scan</code>
function reads in each batch of data (elsewhere, batches are often
referred to chunks). A user-specified function is applied to each
batch of data, the results are combined together, the connection is
closed, and the results are returned.
</p>
<p>As an introductory example, suppose a statistician updates a linear
regression model, but the design matrix <code class="reqn">\textbf{X}</code> is too
large for computer memory. Suppose the design matrix has 100 million
rows, and the statistician specifies <code>size=1e6</code>. The statistician
combines dependent variable <code class="reqn">\textbf{y}</code> with design matrix
<code class="reqn">\textbf{X}</code>. Each iteration in <code>IterativeQuadrature</code>,
<code>LaplaceApproximation</code>, <code>LaplacesDemon</code>,
<code>PMC</code>, or <code>VariationalBayes</code>, the
<code>BigData</code> function sequentially reads in one million rows of the
combined data <code class="reqn">\textbf{X}</code>, calculates expectation vector
<code class="reqn">\mu</code>, and finally returns the sum of the log-likelihood. The sum
of the log-likelihood is added together for all batches, and returned.
</p>
<p>There are many limitations with this function.
</p>
<p>This function is not fast, in the sense that the entire big data set
is processed in batches, each iteration. With iterative methods, this
may perform well, albeit slowly.
</p>
<p>There are many functions that cannot be performed on batches, though
most models in the Examples vignette may easily be updated with big
data.
</p>
<p>Large matrices of samples are unaddressed, only the data.
</p>
<p>Although many (but not all) models may be estimated, many additional
functions in this package will not work when applied after the model
has updated. Instead, a batch or random sample of data (see the
<code>read.matrix</code> function for sampling from big data) should
be used in the usual way, in the <code>Data</code> argument, and the
<code>Model</code> function coded in the usual way without the
<code>BigData</code> function.
</p>
<p>Parallel processing may be performed when the user specifies
<code>CPUs</code> to be greater than one, implying that the specified number
of CPUs exists and is available. Parallelization may be performed on a
multicore computer or a computer cluster. Either a Simple Network of
Workstations (SNOW) or Message Passing Interface (MPI) is used. Each
call to <code>BigData</code> establishes and closes the parallelization,
which is costly, and unfortunately results in copious output to the
console. With small data sets, parallel processing may be slower, due
to computer network communication. With larger data sets, the user
should experience a faster run-time.
</p>
<p>There have been several alternative approaches suggested for big data.
</p>
<p>Huang and Gelman (2005) propose that the user creates batches by
sampling from big data, updating a separate Bayesian model on each
batch, and combining the results into a consensus posterior. This
many-mini-model approach may be faster when feasible, because multiple
models may be updated in parallel, say one per CPU. Such results will
work with all functions in this package. With the many-mini-model
approach, several methods are proposed for combining posterior samples
from batch-level models, such as by using a normal approximation,
updating from prior to posterior sequentially (the posterior from the
last batch becomes the prior of the next batch), sample from the full
posterior via importance sampling from the batched posteriors, and
more.
</p>
<p>Scott et al. (2013) propose a method that they call Consensus Monte
Carlo, which consists of breaking the data down into chunks, calling
each chunk a shard, and use a many-mini-model approach as well, but
propose their own method of weighting the posteriors back together.
</p>
<p>Balakrishnan and Madigan (2006) introduced a Sequential Monte Carlo
(SMC) sampler, a refinement of an earlier proposal, that was designed
for big data. It makes one pass through the massive data set, after an
initial MCMC estimation on a small sample. Each particle is updated
for each record, resulting in numerous evaluations per record.
</p>
<p>Welling and Teh (2011) proposed a new class of MCMC sampler in which
only a random sample of big data is used each iteration. The
stochastic gradient Langevin dynamics (SGLD) algorithm is available
in the <code>LaplacesDemon</code> function.
</p>
<p>An important alternative to consider is using the <code>ff</code> package,
where "ff" stands for fast access file. The <code>ff</code> package has been
tested successfully with updating a model in <code>LaplacesDemon</code>.
Once the big data set, say <code class="reqn">\textbf{X}</code>, is an object of
class <code>ff_matrix</code>, simply include it in the list of data as
usual, and modify the <code>Model</code> specification function
appropriately. For example, change <code>mu &lt;- tcrossprod(X, t(beta))</code>
to <code>mu &lt;- tcrossprod(X[], t(beta))</code>. The <code>ff</code> package is
not included as a dependency in the <code>LaplacesDemon</code> package, so
it must be installed and activated.
</p>


<h3>Value</h3>

<p>The <code>BigData</code> function returns output that is the result of
performing a user-specified function on batches of big data. Output is
a matrix, and may have one or more column vectors.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Balakrishnan, S. and Madigan, D. (2006). "A One-Pass Sequential Monte
Carlo Method for Bayesian Analysis of Massive Datasets".
<em>Bayesian Analysis</em>, 1(2), p. 345–362.
</p>
<p>Huang, Z. and Gelman, A. (2005) "Sampling for Bayesian Computation
with Large Datasets". <em>SSRN eLibrary</em>.
</p>
<p>Scott, S.L., Blocker, A.W. and Bonassi, F.V. (2013). "Bayes and Big
Data: The Consensus Monte Carlo Algorithm". In <em>Bayes 250</em>.
</p>
<p>Welling, M. and Teh, Y.W. (2011). "Bayesian Learning via Stochastic
Gradient Langevin Dynamics". <em>Proceedings of the 28th
International Conference on Machine Learning (ICML)</em>, p. 681–688.
</p>


<h3>See Also</h3>

<p><code>IterativeQuadrature</code>,
<code>LaplaceApproximation</code>,
<code>LaplacesDemon</code>,
<code>LaplacesDemon.RAM</code>,
<code>PMC</code>,
<code>PMC.RAM</code>,
<code>read.matrix</code>, and
<code>VariationalBayes</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">### Below is an example of a linear regression model specification
### function in which BigData reads in a batch of 1,000 records of
### Data$N records from a data set that is too large to fully open
### in memory. The example simulates on 10,000 records, which is
### not big data; it's just a toy example. The data set is file X.csv,
### and the first column of matrix X is the dependent variable y. The
### user supplies a function to BigData along with parameters beta and
### sigma. When each batch of 1,000 records is read in,
### mu = XB is calculated, and then the LL is calculated as
### y ~ N(mu, sigma^2). These results are added together from all
### batches, and returned as LL.

library(LaplacesDemon)
N &lt;- 10000
J &lt;- 10 #Number of predictors, including the intercept
X &lt;- matrix(1,N,J)
for (j in 2:J) {X[,j] &lt;- rnorm(N,runif(1,-3,3),runif(1,0.1,1))}
beta.orig &lt;- runif(J,-3,3)
e &lt;- rnorm(N,0,0.1)
y &lt;- as.vector(tcrossprod(beta.orig, X) + e)
mon.names &lt;- c("LP","sigma")
parm.names &lt;- as.parm.names(list(beta=rep(0,J), log.sigma=0))
PGF &lt;- function(Data) return(c(rnormv(Data$J,0,0.01),
     log(rhalfcauchy(1,1))))
MyData &lt;- list(J=J, PGF=PGF, N=N, mon.names=mon.names,
     parm.names=parm.names) #Notice that X and y are not included here
filename &lt;- tempfile("X.csv")  
write.table(cbind(y,X), filename, sep=",", row.names=FALSE,
  col.names=FALSE)

Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[1:Data$J]
     sigma &lt;- exp(parm[Data$J+1])
     ### Log(Prior Densities)
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     LL &lt;- BigData(file=filename, nrow=Data$N, ncol=Data$J+1, size=1000,
          Method="add", CPUs=1, Type="PSOCK",
          FUN=function(x, beta, sigma) sum(dnorm(x[,1], tcrossprod(x[,-1],
               t(beta)), sigma, log=TRUE)), beta, sigma)
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma),
          yhat=0,#rnorm(length(mu), mu, sigma),
          parm=parm)
     return(Modelout)
     }

### From here, the user may update the model as usual.
</code></pre>


</div>