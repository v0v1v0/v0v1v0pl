<div class="container">

<table style="width: 100%;"><tr>
<td>kNN</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>	k-Nearest Neighbour Classification </h2>

<h3>Description</h3>

<p><code>kNN</code> is used to perform k-nearest neighbour classification for test set using training set. For each row of the test set, the <code>k</code> nearest (based on Euclidean distance) training set vectors are found. then, the classification is done by majority vote (ties broken at random). This function provides a formula interface to the <code>class::knn()</code> function of <code>R</code> package <code>class</code>. In addition, it allows normalization of the given data using the <code>transform</code> function.
</p>


<h3>Usage</h3>

<pre><code class="language-R">kNN( formula, train, test, k = 1, transform = FALSE, type = "class", l = 0, 
     use.all = TRUE, na.rm = FALSE )
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p> a formula, with a response but no interaction terms. For the case of data frame, it is taken as the model frame (see <code>model.frame)</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train</code></td>
<td>
<p> data frame or matrix of train set cases. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test</code></td>
<td>
<p> data frame or matrix of test set cases. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p> number of neighbours considered. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>transform</code></td>
<td>
<p>a character with options <code>FALSE</code> (default), <code>"minmax"</code>, and <code>"zscore"</code>. 
Option <code>"minmax"</code> means no transformation. This option allows the users to use normalized version of the train and test sets for the kNN aglorithm. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>either <code>"class"</code> (default) for the predicted class or
<code>"prob"</code> for model confidence values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>l</code></td>
<td>
<p> minimum vote for definite decision, otherwise <code>doubt</code>. (More
precisely, less than <code>k-l</code> dissenting votes are allowed, even if <code>k</code>
is increased by ties.) </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use.all</code></td>
<td>
<p> controls handling of ties. If true, all distances equal to the <code>k</code>th largest are included. If false, a random selection of distances equal to the <code>k</code>th is chosen to use exactly <code>k</code> neighbours.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.rm</code></td>
<td>
<p>a logical value indicating whether NA values in <code>x</code> should be stripped before the computation proceeds. </p>
</td>
</tr>
</table>
<h3>Value</h3>

 
<p>When <code>type = "class"</code> (default), a factor vector is returned, 
in which the <code>doubt</code> will be returned as <code>NA</code>.
When <code>type = "prob"</code>, a matrix of confidence values is returned
(one column per class).
</p>


<h3>Author(s)</h3>

<p> Reza Mohammadi <a href="mailto:a.mohammadi@uva.nl">a.mohammadi@uva.nl</a> and Kevin Burke <a href="mailto:kevin.burke@ul.ie">kevin.burke@ul.ie</a> </p>


<h3>References</h3>

<p>Ripley, B. D. (1996) <em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002) <em>Modern Applied Statistics with S.</em> Fourth edition. Springer.
</p>


<h3>See Also</h3>

 <p><code>kNN</code>, <code>transform</code> </p>


<h3>Examples</h3>

<pre><code class="language-R">data( risk )

train = risk[ 1:100, ]
test  = risk[   101, ]

kNN( risk ~ income + age, train = train, test = test )
</code></pre>


</div>