<div class="container">

<table style="width: 100%;"><tr>
<td>rstats2.lmridge</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Ordinary Ridge Regression Statistics 2</h2>

<h3>Description</h3>

<p>The <code>rstats2</code> function computes the ordinary ridge related statistics such as <code class="reqn">Ck</code>, <code class="reqn">\sigma^2</code>, ridge degrees of freedom, effective degrees of freedom (EDF), and prediction residual error sum of squares PRESS statistics for scalar or vector value of biasing parameter <code class="reqn">K</code> (See Allen, 1974 &lt;<a href="https://doi.org/10.2307/1267500">doi:10.2307/1267500</a>&gt;; Lee, 1979; Hoerl and Kennard, 1970 &lt;<a href="https://doi.org/10.2307/1267351">doi:10.2307/1267351</a>&gt;).</p>


<h3>Usage</h3>

<pre><code class="language-R">   rstats2(x, ...)
   ## S3 method for class 'lmridge'
rstats2(x, ...)
   ## S3 method for class 'rstats2'
print(x, digits = max(5,getOption("digits") - 5), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>For the <code>rstats2</code> method, an object of class "lmridge", i.e., a fitted model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>digits</code></td>
<td>
<p>Minimum number of significant digits to be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>rstats2</code> function computes the ridge regression related different statistics which may help in selecting the optimal value of biasing parameter <code class="reqn">K</code>. If value of <code class="reqn">K</code> is zero then these statistics are equivalent to the relevant OLS statistics.
</p>


<h3>Value</h3>

<p>Following are ridge related statistics computed for given scalar or vector value of biasing parameter <code class="reqn">K</code> provided as argument to <code>lmridge</code> or <code>lmridgeEst</code> function.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>CK</code></td>
<td>
<p><code class="reqn">Ck</code> similar to Mallows <code class="reqn">Cp</code> statistics for given biasing parameter <code class="reqn">K</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dfridge</code></td>
<td>
<p>DF of ridge for given biasing parameter <code class="reqn">K</code>, i.e., <code class="reqn">Trace[Hat_{R,k}]</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>EP</code></td>
<td>
<p>Effective number of Parameters for given biasing parameter <code class="reqn">K</code>, i.e., <code class="reqn">Trace[2Hat_{R,k}-Hat_{R,k}t(Hat_{R,k})]</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>redf</code></td>
<td>
<p>Residual effective degrees of freedom for given biasing parameter <code class="reqn">K</code> from Hastie and Tibshirani, (1990), i.e., <code class="reqn">n-Trace[2Hat_{R,k}-Hat_{R,k}t(Hat_{R,k})]</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>EF</code></td>
<td>
<p>Effectiveness index for given biasing parameter <code class="reqn">K</code>, also called the ratio of reduction in total variance in the total squared bias by the ridge regression, i.e., <code class="reqn">EF=\frac{\sigma^2 trace(X'X)^{-1}-\sigma^2 trace(VIF_R)}{Bias^2(\hat{\beta}_R)}</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ISRM</code></td>
<td>
<p>Quantification of concept of stable region proposed by Vinod and Ullah, 1981, i.e., <code class="reqn">ISRM_k=\sum_{j=1}^p (\frac{p(\frac{\lambda_j}{\lambda_j+k})^2}{\sum_{j=1}^p \frac{\lambda_j}{(\lambda_j+k)^2} \lambda_j}-1)^2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>m-scale for given value of biasing parameter proposed by Vinod (1976) alternative to plotting of the ridge coefficients, i.e., <code class="reqn">p-\sum_{j-1}^p \frac{\lambda_j}{\lambda_j+k}</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>PRESS</code></td>
<td>
<p>PRESS statistics for ridge regression introduced by Allen, 1971, 1974, i.e., <code class="reqn">PRESS_k=\sum_{i=1}^n e^2_{i,-i} </code> for scalar or vector value of biasing parameter <code class="reqn">K</code>.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Allen, D. M. (1971). Mean Square Error of Prediction as a Criterion for Selecting Variables. <em>Technometrics</em>, <strong>13</strong>, 469-475. <a href="https://doi.org/10.1080/00401706.1971.10488811">doi:10.1080/00401706.1971.10488811</a>.
</p>
<p>Allen, D. M. (1974). The Relationship between Variable Selection and Data Augmentation and Method for Prediction. <em>Technometrics</em>, <strong>16</strong>, 125-127. <a href="https://doi.org/10.1080/00401706.1974.10489157">doi:10.1080/00401706.1974.10489157</a>.
</p>
<p>Cule, E. and De lorio, M. (2012). A semi-Automatic method to guide the choice of ridge parameter in ridge regression.  <em>arXiv:1205.0686v1 [stat.AP]</em>.
</p>
<p>Hastie, T. and Tibshirani, R. (1990). <em>Generalized Additive Models</em>. Chapman &amp; Hall.
</p>
<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>
<p>Kalivas, J. H., and Palmer, J. (2014). Characterizing Multivariate Calibration Tradeoffs (Bias, Variance, Selectivity, and Sensitivity) to Select Model Tuning Parameters. <em>Journal of Chemometrics</em>, <strong>28</strong>(5), 347â€“357. <a href="https://doi.org/10.1002/cem.2555">doi:10.1002/cem.2555</a>.
</p>
<p>Lee, W. F. (1979). Model Estimation Using Ridge Regression with the Variane Normalization Criterion. <em>Master thesis, Department of Educational Foundation Memorial University of Newfoundland.</em>
</p>


<h3>See Also</h3>

<p>Ridge related statistics <code>rstats1</code>, ridge model fitting <code>lmridge</code></p>


<h3>Examples</h3>

<pre><code class="language-R">data(Hald)
mod &lt;- lmridge(y~., data=as.data.frame(Hald), K = seq(0,0.2, 0.001) )

rstats2(mod)

</code></pre>


</div>