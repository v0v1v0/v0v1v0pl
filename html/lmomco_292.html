<div class="container">

<table style="width: 100%;"><tr>
<td>pfactor.bernstein</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Estimation of Optimal p-factor of Distributional Support Estimation for Smoothed Quantiles from the Bernstein or Kantorovich Polynomials </h2>

<h3>Description</h3>

<p>Compute the optimal p-factor through numerical integration of the smoothed empirical quantile function to estimate the L-moments of the distribution. This function attempts to report an optimal “p-factor”  (author's term) for the given parent distribution in <code>para</code> based on estimating the crossing of the origin of an error between the given L-moment ratio <code class="reqn">\tau_r</code> for 3, 4, and 5 that will come from either the distribution parameter object or given as an argument in <code>lmr.dist</code>.  The estimated support of the distribution is that shown by  Turnbull and Ghosh (2014) and is computed as follows
</p>
<p style="text-align: center;"><code class="reqn">\biggl(x_{0:n},\: x_{n+1:n}\biggr) = \biggl(x_{1:n} - \frac{(x_{2:n} - x_{1:n})}{(1 - p)^{-2} - 1},\: x_{n:n} + \frac{(x_{n:n} - x_{n-1:n})}{(1 - p)^{-2} - 1}\biggr)\mbox{,}</code>
</p>

<p>where <code class="reqn">p</code> is the p-factor. The support will honor natural bounds if given by either <code>fix.lower</code> or <code>fix.upper</code>. The polynomial type for smooth is provided in <code>poly.type</code>. These three arguments are the same as those for <code>dat2bernqua</code> and <code>lmoms.bernstein</code>. The statistic type used to measure central tendency of the errors for the <code>nsim</code> simulations per <code class="reqn">p</code>. The function has its own hardwired p-factors to compute but these can be superseded by the <code>pfactors</code> argument. The <code>p.lo</code> and <code>p.hi</code> are the lower and upper bounds to truncate on immediately after the p-factors to use are assembled. These are made for three purposes: (1) protection against numerical problems for mathematical upper limits (unity), (2) to potentially provide for much faster execution if the user already knows the approximate optimal value for the p-factor, and (3) to potentially use this function in a direct optimization framework using the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> functions <code>optim</code> or <code>uniroot</code>. It is strongly suggested to keep <code>plot.em</code> set so the user can inspect the computations.
</p>


<h3>Usage</h3>

<pre><code class="language-R">pfactor.bernstein(para, x=NULL, n=NULL,
                        bern.control=NULL,
                        poly.type=c("Bernstein", "Kantorovich"),
                        stat.type=c("Mean", "Median"),
                        fix.lower=NULL, fix.upper=NULL,
                        lmr.dist=NULL, lmr.n=c("3", "4", "5"),
                        nsim=500, plot.em=TRUE, pfactors=NULL,
                        p.lo=.Machine$double.eps, p.hi=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>para</code></td>
<td>
<p>A mandatory “parent” distribution defined by a usual <span class="pkg">lmomco</span> distribution parameter object for a distribution. The simulations are based on this distribution, although optimization for <code class="reqn">p</code> can be set to a different L-moment value by <code>lmr.dist</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>An optional vector of data values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>An optional sample size to run the simulations on. This value is computed by <code>length(x)</code> if <code>x</code> is provided. If set by argument, then that size supersedes the length of the optional observed sample.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bern.control</code></td>
<td>
<p>A <code>list</code> that holds <code>poly.type</code>, <code>stat.type</code>, <code>fix.lower</code>, and <code>fix.upper</code>. And this list will supersede the respective
values provided as separate arguments. There is an implicit <code>bound.type</code> of <code>"Carv"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>poly.type</code></td>
<td>
<p>Same argument as for <code>dat2bernqua</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stat.type</code></td>
<td>
<p>The central estimation statistic for each p-factor evaluated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fix.lower</code></td>
<td>
<p>Same argument as for <code>dat2bernqua</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fix.upper</code></td>
<td>
<p>Same argument as for <code>dat2bernqua</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lmr.dist</code></td>
<td>
<p>This is the value for the <code>lmr.n</code> of the distribution in <code>para</code> unless explicitly set through <code>lmr.dist</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lmr.n</code></td>
<td>
<p>The L-moment ratio number for p-factor optimization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nsim</code></td>
<td>
<p>The number of simulations to run. Experiments suggest the default is adequate for reasonably small sample sizes—the simulation count can be reduced as <code>n</code> becomes large.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot.em</code></td>
<td>
<p>A logical to trigger the diagnostic plot of the simulated errors and a smooth line through these errors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pfactors</code></td>
<td>
<p>An optional vector of p-factors to loop through for the simulations. The vector computing internall is this is set to <code>NULL</code> seems to be more than adequate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p.lo</code></td>
<td>
<p>An computational lower boundary for which the <code>pfactors</code> by argument or default are truncated to. The default for <code>lo</code> is to be quite small and does no truncate the default <code>pfactors</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p.hi</code></td>
<td>
<p>An computational upper boundary for which the <code>pfactors</code> by argument or default are truncated to. The default for <code>hi</code> is unity, which is the true upper limit that results in a 0 slope between the <code class="reqn">x_{0:n}</code> to <code class="reqn">x_{1:n}</code> or <code class="reqn">x_{n:n}</code> to <code class="reqn">x_{n+1:n}</code> order statistics.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> <code>list</code> or <code>real</code> is returned.  If <code>pfactors</code> is a single value, then the single value for the error statistic is returned, otherwise the list described will be. If the returned <code>pfactor</code> is <code>NA</code>, then likely the smooth line did not cross zero and the reason the user should keep <code>plot.em=TRUE</code> and inspect the plot. Perhaps revisions to the arguments will become evident. The contents of the list are
</p>
<table>
<tr style="vertical-align: top;">
<td><code>pfactor</code></td>
<td>
<p>The estimated value of <code class="reqn">p</code> smoothed by <code>lowess</code> that has an error of zero, see <code>err.stat</code> as a function of <code>ps</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bounds.type</code></td>
<td>
<p><code>Carv</code>, which is the same bound type as needed by <code>dat2bernqua</code> and <br><code>lmoms.bernstein</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>poly.type</code></td>
<td>
<p>The given <code>poly.type</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stat.type</code></td>
<td>
<p>The given <code>stat.type</code>. The “Mean” seems to be preferable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lmom.type</code></td>
<td>
<p>A string of the L-moment type: “Tau3”, “Tau4”, “Tau5”.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fix.lower</code></td>
<td>
<p>The given fixed lower boundary, which could stay <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fix.upper</code></td>
<td>
<p>The given fixed upper boundary, which could stay <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>source</code></td>
<td>
<p>An attribute identifying the computational source of the L-moments: “pfactor.bernstein”.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ps</code></td>
<td>
<p>The p-factors actually evaluated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>err.stat</code></td>
<td>
<p>The error statistic computed by <code>stat.type</code> of the simulated <code class="reqn">\hat{\tau_r}</code> by integration provided by <code>lmoms.bernstein</code> minus the “true” value <code class="reqn">\tau_r</code> provided by either <code>para</code> or given by <code>lmr.dist</code> where <code class="reqn">r</code> is <code>lmr.n</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>err.smooth</code></td>
<td>
<p>The <code>lowess</code>-smoothed values for <code>err.stat</code> and the <code>pfactor</code> comes from a linear interpolation of this smooth for the error being zero.</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>Repeated application of this function for various <code>n</code> would result in the analyst having a vector of <code class="reqn">n</code> and <code class="reqn">p</code> (<code>pfactor</code>). The analyst could then fit a regression equation and refine the estimated <code class="reqn">p(n)</code>. For example, a dual-logarithmic regression is suggested <code>lm(log(p)~log(n))</code>.
</p>
<p>Also, symmetrical data likely see little benefit from optimizing on the symmetry-measuring L-moments Tau3 and Tau5; the analyst might prefer to optimize on peakedness measured by Tau4.
</p>


<h3>Note</h3>

<p>This function is highly experimental and subject to extreme overhaul. Please contact the author if you are an interested party in Bernstein and Kantorovich polynomials.
</p>


<h3>Author(s)</h3>

<p>W.H. Asquith</p>


<h3>References</h3>

<p>Turnbull, B.C., and Ghosh, S.K., 2014, Unimodal density estimation using Bernstein polynomials. Computational Statistics and Data Analysis, v. 72, pp. 13–29.
</p>


<h3>See Also</h3>

<p><code>lmoms.bernstein</code>, <code>dat2bernqua</code>, <code>lmoms</code> </p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
pdf("pfactor_exampleB.pdf")
X &lt;- exp(rnorm(200)); para &lt;- parexp(lmoms(X))
# nsim is too small, but makes the following three not take too long
pfactor.bernstein(para, n=20, lmr.n="3", nsim=100, p.lo=.06, p.hi=.3)
pfactor.bernstein(para, n=20, lmr.n="4", nsim=100, p.lo=.06, p.hi=.3)
pfactor.bernstein(para, n=20, lmr.n="5", nsim=100, p.lo=.06, p.hi=.3)
dev.off()

## End(Not run)
## Not run: 
# Try intra-sample p-factor optimization from two perspectives. The 3-parameter
# GEV "over fits" the data and provides the parent.  Then use Tau3 of the fitted
# GEV for peakedness restraint and then use Tau3 of the data. Then repeat but use
# the apparent "exact" value of Tau3 for the true exponential parent.
pdf("pfactor_exampleB.pdf")
lmr &lt;- vec2lmom(c(60,20)); paraA &lt;- parexp(lmr); n &lt;- 40
tr &lt;- lmorph(par2lmom(paraA))$ratios[3]
X &lt;- rlmomco(n, paraA); para &lt;- pargev(lmoms(X))
F &lt;- seq(0.001,0.999, by=0.001)
plot(qnorm(pp(X, a=0.40)), sort(X), type="n", log="y",
      xlab="Standard normal variate", ylab="Quantile",
      xlim=qnorm(range(F)), ylim=range(qlmomco(F,paraA)))
lines(qnorm(F), qlmomco(F, paraA), col=8, lwd=2)
lines(qnorm(F), qlmomco(F, para), lty=2)
points(qnorm(pp(X, a=0.40)), sort(X))

# Make sure to fill in the p-factor when needed!
bc &lt;- list(poly.type = "Bernstein", bound.type="Carv",
           stat.type="Mean", fix.lower=0, fix.upper=NULL, p=NULL)
kc &lt;- list(poly.type = "Kantorovich", bound.type="Carv",
           stat.type="Mean", fix.lower=0, fix.upper=NULL, p=NULL)

# Bernstein
A &lt;- pfactor.bernstein(para,      n=n, nsim=100,              bern.control=bc)
B &lt;- pfactor.bernstein(para, x=X, n=n, nsim=100,              bern.control=bc)
C &lt;- pfactor.bernstein(para,      n=n, nsim=100, lmr.dist=tr, bern.control=bc)
D &lt;- pfactor.bernstein(para, x=X, n=n, nsim=100, lmr.dist=tr, bern.control=bc)
plot(qnorm(pp(X, a=0.40)), sort(X), type="n", log="y",
      xlab="Standard normal variate", ylab="Quantile",
      xlim=qnorm(range(F)), ylim=range(qlmomco(F,paraA)))
lines(qnorm(F), qlmomco(F, paraA), col=8, lwd=2)
lines(qnorm(F), qlmomco(F, para), lty=2)
points(qnorm(pp(X, a=0.40)), sort(X))
      bc$p &lt;- A$pfactor
lines(qnorm(F), dat2bernqua(F,X, bern.control=bc), col=2)
      bc$p &lt;- B$pfactor
lines(qnorm(F), dat2bernqua(F,X, bern.control=bc), col=3)
      bc$p &lt;- C$pfactor
lines(qnorm(F), dat2bernqua(F,X, bern.control=bc), col=2, lty=2)
      bc$p &lt;- D$pfactor
lines(qnorm(F), dat2bernqua(F,X, bern.control=bc), col=3, lty=2)
# Kantorovich
A &lt;- pfactor.bernstein(para,      n=n, nsim=100,              bern.control=kc)
B &lt;- pfactor.bernstein(para, x=X, n=n, nsim=100,              bern.control=kc)
C &lt;- pfactor.bernstein(para,      n=n, nsim=100, lmr.dist=tr, bern.control=kc)
D &lt;- pfactor.bernstein(para, x=X, n=n, nsim=100, lmr.dist=tr, bern.control=kc)
plot(qnorm(pp(X, a=0.40)), sort(X), type="n", log="y",
      xlab="Standard normal variate", ylab="Quantile",
      xlim=qnorm(range(F)), ylim=range(qlmomco(F,paraA)))
lines(qnorm(F), qlmomco(F, paraA), col=8, lwd=2)
lines(qnorm(F), qlmomco(F, para), lty=2)
points(qnorm(pp(X, a=0.40)), sort(X))
      kc$p &lt;- A$pfactor
lines(qnorm(F), dat2bernqua(F,X, bern.control=kc), col=2)
      kc$p &lt;- B$pfactor
lines(qnorm(F), dat2bernqua(F,X, bern.control=kc), col=3)
      kc$p &lt;- C$pfactor
lines(qnorm(F), dat2bernqua(F,X, bern.control=kc), col=2, lty=2)
      kc$p &lt;- D$pfactor
lines(qnorm(F), dat2bernqua(F,X, bern.control=kc), col=3, lty=2)
dev.off()

## End(Not run)
## Not run: 
X &lt;- exp(rnorm(200)); para &lt;- parexp(lmoms(X))
"pfactor.root" &lt;- function(para, p.lo, p.hi, ...) {
    afunc &lt;- function(p, para=NULL, x=NULL, ...) {
      return(pfactor.bernstein(para=para, x=x, pfactors=p, ...)) }
    rt &lt;- uniroot(afunc, c(p.lo, p.hi),
                  tol=0.001, maxiter=30, nsim=500, para=para, ...)
    return(rt)
}
pfactor.root(para, 0.05, 0.15, n=10, lmr.n="4")
pfactor.bernstein(para, n=10, lmr.n="4", nsim=200, p.lo=.05, p.hi=.15)

## End(Not run)
</code></pre>


</div>