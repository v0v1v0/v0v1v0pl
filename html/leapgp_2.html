<div class="container">

<table style="width: 100%;"><tr>
<td>predict_leapGP</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Predict Method for leapGP</h2>

<h3>Description</h3>

<p>Predict method for an object of class leapGP.
Returns a (possibly modified) leapGP object as well as a prediction (with uncertainty, if requested).
</p>


<h3>Usage</h3>

<pre><code class="language-R">predict_leapGP(
  object,
  newdata,
  rho = 0.95,
  scale = FALSE,
  n = ceiling(sqrt(length(y))),
  start = NA,
  M_max = Inf,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>An object of class <code>leapGP</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>
<p>New data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>parameter controlling time-accuracy tradeoff (default is <code>rho=0.95</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p>logical. Do we want the scale parameter to be returned for predictions? If TRUE,
the matrix <code class="reqn">K^{-1}</code> will be stored for each hub.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>local neighborhood size</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>start</code></td>
<td>
<p>number of starting points for neighborhood (between 6 and n inclusive)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>M_max</code></td>
<td>
<p>the maximum number of hubs allowed (used to upper bound the run time)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>optional arguments to be passed to <code>laGP()</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The leapGP is extends the laGP framework of Gramacy &amp; Apley (2015). The methods are equivalent for <code>rho=1</code>,
but leapGP trades memory for speed when <code>rho &lt; 1</code>. The method is described in Rumsey et al. (2023) where they demonstrate
that leapGP is faster than laGP for sequential predictions and is also generally more accurate for some settings of <code>rho</code>.
</p>


<h3>Value</h3>

<p>A list containing values <code>mean</code>, <code>hubs</code> <code>X</code> and <code>y</code>. If <code>scale=TRUE</code> the list also contains field <code>sd</code>.
</p>


<h3>References</h3>

<p>Gramacy, R. B., &amp; Apley, D. W. (2015). Local Gaussian process approximation for large computer experiments. Journal of Computational and Graphical Statistics, 24(2), 561-578.
</p>
<p>Rumsey, K. N., Huerta, G., &amp; Derek Tucker, J. (2023). A localized ensemble of approximate Gaussian processes for fast sequential emulation. Stat, 12(1), e576.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Generate data
f &lt;- function(x){
   1.3356*(1.5*(1-x[1]) + exp(2*x[1] - 1)*sin(3*pi*(x[1] - 0.6)^2) +
   exp(3*(x[2]-0.5))*sin(4*pi*(x[2] - 0.9)^2))
}
X &lt;- matrix(runif(200), ncol=2)
y &lt;- apply(X, 1, f)

# Generate data for prediction
Xtest &lt;- matrix(runif(200), ncol=2)
ytest &lt;- apply(Xtest, 1, f)

# Train initial model
mod &lt;- leapGP(X, y, M0 = 30)
# Make sequential predictions
pred &lt;- rep(NA, 100)
for(i in 1:100){
  mod &lt;- predict_leapGP(mod, matrix(Xtest[i,], nrow=1), rho=0.9)
  pred[i] &lt;- mod$mean
}
</code></pre>


</div>