<div class="container">

<table style="width: 100%;"><tr>
<td>Kapitel  9</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kapitel 9: Fairer Vergleich in der Rueckmeldung</h2>

<h3>Description</h3>

<p>Das ist die Nutzerseite zum Kapitel 9, <em>Fairer Vergleich in der 
Rückmeldung</em>, im Herausgeberband Large-Scale Assessment mit <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>: Methodische 
Grundlagen der österreichischen Bildungsstandardüberprüfung. 
Im Abschnitt <strong>Details</strong> werden die im Kapitel verwendeten <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>-Syntaxen zur 
Unterstützung für Leser/innen kommentiert und dokumentiert. 
Im Abschnitt <strong>Examples</strong> werden die <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>-Syntaxen des Kapitels vollständig 
wiedergegeben und gegebenenfalls erweitert.
</p>


<h3>Details</h3>



<h4>Vorbereitungen</h4>

<p>Der zur Illustration verwendete Datensatz <code>dat</code> beinhaltet (imputierte) 
aggregierte Leistungs- und Hintergrunddaten von 244 Schulen, bestehend aus 74 
ländlichen allgemeinbildenden Pflichtschulen (APS, Stratum 1), 69 städtischen 
APS (Stratum 2), 52 ländlichen allgemeinbildenden höheren Schulen (AHS, Stratum 
3) und 49 städtischen AHS (Stratum 4). 
Im Kapitel wird zur Bildung von Interaktionseffekten und quadratischen Termen 
der Kovariaten eine neue Funktion <code>covainteraction</code> verwendet.
</p>
<p><code style="white-space: pre;">⁠data(datenKapitel09)
dat &lt;- datenKapitel09

covainteraction &lt;- function(dat,covas,nchar){
  for(ii in 1:(length(covas))){
    vv1 &lt;- covas[ii]
    # Interaktion von vv1 mit sich selbst
    subname1 &lt;- substr(vv1,1,nchar)
    intvar &lt;- paste0(subname1, subname1)
    if(vv1 == covas[1]){
      dat.int &lt;- dat[,vv1]*dat[,vv1];
      newvars &lt;- intvar } else {
        dat.int &lt;- cbind(dat.int,dat[,vv1]*dat[,vv1]); 
        newvars &lt;- c(newvars,intvar) 
      }
    # Interaktion von vv1 mit restlichen Variablen
    if(ii &lt; length(covas)){
      for(jj in ((ii+1):length(covas))){
        vv2 &lt;- covas[jj]
        subname2 &lt;- substr(vv2,1,nchar)
        intvar &lt;- paste0(subname1, subname2)
        newvars &lt;- c(newvars, intvar)
        dat.int &lt;- cbind(dat.int,dat[,vv1]*dat[,vv2])
      }
    }
    
  }
  dat.int &lt;- data.frame(dat.int)
  names(dat.int) &lt;- newvars
  return(dat.int)
}
⁠</code>

</p>



<h4>Abschnitt 9.2.5.1: Kovariaten und Interaktionsterme</h4>



<h5>Listing 1: Kovariatenauswahl und z-Standardisierung</h5>

<p>Als Variablen zur Beschreibung von Kontext und Schülerzusammensetzung in den 
Schulen werden in diesem Beispiel die logarithmierte Schulgröße <code>groesse</code>, 
der Anteil an Mädchen <code>female</code>, der Anteil an Schülerinnen und Schülern mit 
Migrationshintergrund <code>mig</code> und der mittlere sozioökonomische Status (SES) 
<code>sozstat</code> eingeführt. 
Die abhängige Variable ist die aggregierte 
Schülerleistung der Schule <code>TWLE</code>. Alle Kovariaten (<code>vars</code>) werden 
zunächst z-standardisiert (<code>zvars</code>).
</p>
<p><code style="white-space: pre;">⁠vars &lt;- c("groesse","female","mig","sozstat")
zvars &lt;- paste0("z",vars)
dat[,zvars] &lt;- scale(dat[,vars],scale = TRUE)
⁠</code>

</p>



<h5>Listing 2: Interaktionen bilden, z-standardisieren</h5>

<p>Zur Optimierung der Modellspezifikation werden Interaktionseffekte und 
quadratische Terme der Kovariaten gebildet, dann z-standardisiert und in den 
Gesamtdatensatz hinzugefügt. 
Die neuen Variablennamen sind in der Liste <code>intvars</code> aufgelistet.
</p>
<p><code style="white-space: pre;">⁠dat1 &lt;- LSAmitR::covainteraction(dat = dat,covas = zvars,nchar = 4)
intvars &lt;- names(dat1) # Interaktionsvariablen
dat1[,intvars] &lt;- scale(dat1[,intvars],scale = TRUE)
dat &lt;- cbind(dat,dat1)
⁠</code>

</p>



<h5>Listing 3: Haupt- und Interaktionseffekte</h5>

<p><code style="white-space: pre;">⁠maineff &lt;- zvars # Haupteffekte 
alleff &lt;- c(zvars,intvars) # Haupt- und Interaktionseffekte
⁠</code>

</p>




<h4>Abschnitt 9.2.5.2: OLS-Regression</h4>



<h5>Listing 4: OLS-Regression mit Haupteffekten</h5>

<p>Das OLS-Regressionsmodell mit den Haupteffekten als Modellprädiktoren 
(<code>ols.mod1</code>) für Schulen im Stratum (<code>st</code>) 4
</p>
<p><code style="white-space: pre;">⁠fm.ols1 &lt;- paste0("TWLE ~ ",paste(maineff,collapse=" + "))
fm.ols1 &lt;- as.formula(fm.ols1) # Modellgleichung
st &lt;- 4
pos &lt;- which(dat$stratum == st) # Schulen im Stratum st
ols.mod1 &lt;- lm(formula = fm.ols1,data = dat[pos,]) # Regression
⁠</code>

</p>




<h4>Abschnitt 9.2.5.3: Lasso-Regression</h4>



<h5>Listing 5: Datenaufbereitung</h5>

<p>Für die Durchführung der Lasso-Regression wird das R-Paket glmnet 
(Friedman et al., 2010) eingesetzt. Die Kovariatenmatrix (<code>Z</code>) sowie der 
Vektor der abhängigen Leistungswerte (<code>Y</code>) müssen definiert werden.
</p>
<p><code style="white-space: pre;">⁠library(glmnet)
Z &lt;- as.matrix(dat[pos,alleff]) # Kovariatenmatrix
Y &lt;- dat$TWLE[pos] # Abhängige Variable
⁠</code>

</p>



<h5>Listing 6: Bestimmung Teilmengen für Kreuzvalidierung, 
Lasso-Regression</h5>

<p>Das Lasso-Verfahren wird mit der Funktion <code>cv.glmnet()</code> durchgeführt. 
Zur Auswahl eines optimalen shrinkage <code class="reqn">\lambda</code> wird das Verfahren der 
K-fachen Kreuzvalidierung verwendet. 
Die Schulstichprobe wird durch ID-Zuweisung (<code>foldid</code>) verschiedenen 
Teilmengen zugewiesen.
</p>
<p><code style="white-space: pre;">⁠nid &lt;- floor(length(pos)/3) # Teilmengen definieren 
foldid &lt;- rep(c(1:nid),3,length.out=length(pos)) # Zuweisung
lasso.mod2 &lt;- glmnet::cv.glmnet(x=Z,y=Y,alpha = 1, foldid = foldid)
⁠</code>

</p>



<h5>Listing 7: Erwartungswerte der Schulen</h5>

<p>Entsprechend <code>lambda.min</code> werden die Regressionskoeffizienten und die 
entsprechenden Erwartungswerte der Schulen bestimmt.
</p>
<p><code style="white-space: pre;">⁠lasso.pred2 &lt;- predict(lasso.mod2,newx = Z,s="lambda.min")
dat$expTWLE.Lasso2[pos] &lt;- as.vector(lasso.pred2)
⁠</code>

</p>



<h5>Listing 8: Bestimmung R^2</h5>

<p>Bestimmung des aufgeklärten Varianzanteils der Schulleistung <code>R^2</code>.
</p>
<p><code style="white-space: pre;">⁠varY &lt;- var(dat$TWLE[pos])
varY.lasso.mod2 &lt;- var(dat$expTWLE.Lasso2[pos])
R2.lasso.mod2 &lt;- varY.lasso.mod2/varY
⁠</code>

</p>




<h4>Abschnitt 9.2.5.4: Nichtparametrische Regression</h4>



<h5>Listing 9: Distanzberechnung</h5>

<p>Der erste Schritt zur Durchführung einer nichtparametrischen Regression ist die 
Erstellung der Distanzmatrix zwischen Schulen. In diesem Beispiel wird die 
euklidische Distanz als Distanzmaß berechnet, alle standardisierten Haupteffekte 
sind eingeschlossen. Außerdem setzen wir die Gewichte von allen Kovariaten 
(<code>gi</code>) auf 1. <code>dfr.i</code> in diesem Beispiel ist die Distanzmatrix für
erste Schule im Stratum.
</p>
<p><code style="white-space: pre;">⁠N &lt;- length(pos) # Anzahl Schulen im Stratum
schools &lt;- dat$idschool[pos] # Schulen-ID
i &lt;- 1
# Teildatensatz von Schule i
dat.i &lt;- dat[pos[i],c("idschool","TWLE",maineff)]
names(dat.i) &lt;- paste0(names(dat.i),".i")
# Daten der Vergleichsschulen
dat.vgl &lt;- dat[pos[-i],c("idschool","TWLE",maineff)]
index.vgl &lt;- match(dat.vgl$idschool,schools)
# Daten zusammenfügen
dfr.i &lt;- data.frame("index.i"=i,dat.i,"index.vgl"=index.vgl,
                    dat.vgl, row.names=NULL)
# Distanz zur Schule i
dfr.i$dist &lt;- 0
gi &lt;- c(1,1,1,1)
for(ii in 1:length(maineff)){
  vv &lt;- maineff[ii]
  pair.vv &lt;- grep(vv, names(dfr.i), value=T)
  dist.vv &lt;- gi[ii]*((dfr.i[,pair.vv[1]]-dfr.i[,pair.vv[2]])^2)
  dfr.i$dist &lt;- dfr.i$dist + dist.vv }
⁠</code>

</p>



<h5>Listing 10: H initiieren</h5>

 
<p><code class="reqn">p(x) = \frac{\lambda^x e^{-\lambda}}{x!}</code>.
</p>
<p>Die Gewichte <code class="reqn">w_{ik}</code> für jedes Paar (i, k) von Schulen werden mithilfe der 
Distanz, der Gauß’schen Kernfunktion (<code>dnorm</code>) als Transformationsfunktion
sowie einer schulspezifischen Bandweite <code class="reqn">h_i</code> berechnet. Die Auswahl des optimalen
Werts <code class="reqn">\hat{h_i}</code> für jede Schule i erfolgt nach Vieu (1991). Zunächst wird
ein Vektor <code>H</code> so gewählt, dass der optimale Wert <code class="reqn">\hat{h_i}</code> größer 
als der kleinste und kleiner als der größte Wert in <code>H</code> ausfällt. 
Je kleiner das Intervall zwischen den Werten in <code>H</code> ist, desto 
wahrscheinlicher ist, dass ein Listenelement den optimalen Wert erlangt. 
Auf der anderen Seite korrespondiert die Rechenzeit mit der Länge von <code>H</code>. 
Gemäß der Größe der Vergleichsgruppe wählen wir eine Länge von 30 für <code>H</code>, 
zusätzlich wird ein sehr großer Wert (100000) für die Fälle hinzugefügt, bei 
denen alle Gewichte beinahe gleich sind.
</p>
<p><code style="white-space: pre;">⁠d.dist &lt;- max(dfr.i$dist)-min(dfr.i$dist)
H &lt;- c(seq(d.dist/100,d.dist,length=30),100000)
V1 &lt;- length(H) 
# Anzahl Vergleichsschulen
n &lt;- nrow(dfr.i) 
⁠</code>

</p>



<h5>Listing 11: Leave-One-Out-Schätzer der jeweiligen Vergleichsschule k
nach h in H</h5>

<p>Auf Basis aller Werte in <code>H</code> und dem jeweils entsprechenden Gewicht <code class="reqn">w_{ik}</code>
(<code>wgt.h</code>) werden die Leave-One-Out-Schätzer der jeweiligen Vergleichsschule
(<code>pred.k</code>) berechnet.
</p>
<p><code style="white-space: pre;">⁠sumw &lt;- 0*H # Vektor w_{ik} initiieren, h in H
av &lt;- "TWLE"
dfr0.i &lt;- dfr.i[,c("idschool",av)]
# Schleife über alle h-Werte
for (ll in 1:V1 ){
  h &lt;- H[ll]
  # Gewicht w_{ik} bei h
  dfr.i$wgt.h &lt;- dnorm(sqrt(dfr.i$dist), mean=0, sd=sqrt(h))
  # Summe von w_{ik} bei h
  sumw[which(H==h)] &lt;- sum(dfr.i$wgt.h)
  # Leave-one-out-Schätzer von Y_k
  for (k in 1:n){
    # Regressionsformel
    fm &lt;- paste0(av,"~",paste0(maineff,collapse="+"))
    fm &lt;- as.formula(fm)
    # Regressionsanalyse ohne Beitrag von Schule k
    dfr.i0 &lt;- dfr.i[-k,]
    mod.k &lt;- lm(formula=fm,data=dfr.i0,weights=dfr.i0$wgt.h)
    # Erwartungswert anhand Kovariaten der Schule k berechnen
    pred.k &lt;- predict(mod.k, dfr.i)[k]
    dfr0.i[k,paste0( "h_",h) ] &lt;- pred.k
}}
# Erwartungswerte auf Basis verschiedener h-Werte
dfr1 &lt;- data.frame("idschool.i"=dfr.i$idschool.i[1],"h"=H )
⁠</code>

</p>



<h5>Listing 12: Kreuzvalidierungskriterium nach h in H</h5>

<p>Zur Berechnung der Kreuzvalidierungskriterien (<code>CVh</code>, je kleiner, desto 
reliabler sind die Schätzwerte) für alle Werte h in <code>H</code> verwenden wir in 
diesem Beispiel die Plug-in-Bandweite nach Altman und Leger (1995) (<code>hAL</code>), 
die mit der Funktion <code>ALbw()</code> des R-Pakets <code>kerdiest</code> aufrufbar ist.
</p>
<p><code style="white-space: pre;">⁠library(kerdiest)
hAL &lt;- kerdiest::ALbw("n",dfr.i$dist) # Plug-in Bandweite
dfr.i$cross.h &lt;- hAL
dfr.i$crosswgt &lt;- dnorm( sqrt(dfr.i$dist), mean=0, sd = sqrt(hAL) ) 
# Kreuzvalidierungskriterium CVh
vh &lt;- grep("h_",colnames(dfr0.i),value=TRUE)
for (ll in 1:V1){
  dfr1[ll,"CVh"] &lt;- sum( (dfr0.i[,av] - dfr0.i[,vh[ll]])^2 * 
                           dfr.i$crosswgt) / n}
⁠</code>

</p>



<h5>Listing 13: Bestimmung des optimalen Wertes von h</h5>

<p>Der optimale Wert von h in <code>H</code> (<code>h.min</code>) entspricht dem mit dem 
kleinsten resultierenden <code>CVh</code>.
</p>
<p><code style="white-space: pre;">⁠dfr1$min.h.index &lt;- 0
ind &lt;-  which.min( dfr1$CVh )
dfr1$min.h.index[ind] &lt;- 1
dfr1$h.min &lt;- dfr1$h[ind]
⁠</code>

</p>



<h5>Listing 14: Kleinste Quadratsumme der Schätzfehler</h5>

<p>Kleinste Quadratsumme der Schätzfehler der nichtparametrischen Regression mit
h=<code>h.min</code>.
</p>
<p><code style="white-space: pre;">⁠dfr1$CVhmin &lt;- dfr1[ ind , "CVh" ]
⁠</code>

</p>



<h5>Listing 15: Effizienzsteigerung</h5>

<p>Die Effizienz (Steigerung der Schätzungsreliabilität) der nichtparametrischen 
Regression gegenüber der linearen Regression (äquivalent zu <code>CVh</code> bei 
h=100000).
</p>
<p><code style="white-space: pre;">⁠dfr1$eff_gain &lt;-  100 * ( dfr1[V1,"CVh"] / dfr1$CVhmin[1] - 1 )
⁠</code>

</p>



<h5>Listing 16: Durchführung der nichtparametrischen Regression</h5>

<p><code style="white-space: pre;">⁠h &lt;- dfr1$h.min[1]  # h.min
dfr.i$wgt.h &lt;- dnorm(sqrt(dfr.i$dist),sd=sqrt(h))/
  dnorm(0,sd= sqrt(h)) # w_{ik} bei h.min      
dfr.i0 &lt;- dfr.i
# Lokale Regression    
mod.ii &lt;- lm(formula=fm,data=dfr.i0,weights=dfr.i0$wgt.h)
# Kovariaten Schule i
predM &lt;- data.frame(dfr.i[1,paste0(maineff,".i")])    
names(predM) &lt;- maineff
pred.ii &lt;- predict(mod.ii, predM) # Schätzwert Schule i
dat$expTWLE.np[match(dfr1$idschool.i[1],dat$idschool)] &lt;- pred.ii
⁠</code>

</p>




<h4>Abschnitt 9.2.7, Berücksichtigung der Schätzfehler</h4>

<p>Der Erwartungsbereich wird nach der im Buch beschriebenen Vorgehensweise
bestimmt.
</p>


<h5>Listing 17: Bestimmung des Erwartungsbereichs</h5>

<p>Bestimmung der Breite des Erwartungsbereichs aller Schulen auf Basis der 
Ergebnisse der OLS-Regression mit Haupteffekten.
</p>
<p><code style="white-space: pre;">⁠vv &lt;- "expTWLE.OLS1" # Variablenname
mm &lt;- "OLS1" # Kurzname des Modells
dfr &lt;- NULL # Ergebnistabelle
# Schleife über alle möglichen Breite von 10 bis 60
for(w in 10:60){
  # Variablen für Ergebnisse pro w
  var &lt;- paste0(mm,".pos.eb",w) # Position der Schule
  var.low &lt;- paste0(mm,".eblow",w) # Untere Grenze des EBs
  var.upp &lt;- paste0(mm,".ebupp",w) # Obere Grenze des EBs
  # Berechnen
  dat[,var.low] &lt;- dat[,vv]-w/2 # Untere Grenze des EBs
  dat[,var.upp] &lt;- dat[,vv]+w/2 # Obere Grenze des EBs 
  # Position: -1=unterhalb, 0=innerhalb, 1=oberhalb des EBs 
  dat[,var] &lt;- -1*(dat$TWLE &lt; dat[,var.low]) + 1*(dat$TWLE &gt; dat[,var.upp])
  # Verteilung der Schulpositionen
  tmp &lt;- data.frame(t(matrix(prop.table(table(dat[,var])))))
  names(tmp) &lt;- c("unterhalb","innerhalb","oberhalb")
  tmp &lt;- data.frame("ModellxBereich"=var,tmp); dfr &lt;- rbind(dfr,tmp) }

# Abweichung zur Wunschverteilung 25-50-25 
dfr1 &lt;- dfr 
dfr1[,c(2,4)] &lt;- (dfr1[,c(2,4)] - .25)^2 
dfr1[,3] &lt;- (dfr1[,3] - .5)^2 
dfr1$sumquare &lt;- rowSums(dfr1[,-1]) 
# Auswahl markieren 
dfr$Auswahl &lt;- 1*(dfr1$sumquare == min(dfr1$sumquare) )
⁠</code>

</p>




<h3>Author(s)</h3>

<p>Giang Pham, Alexander Robitzsch, Ann Cathrice George, Roman Freunberger
</p>


<h3>References</h3>

<p>Pham, G., Robitzsch, A., George, A. C. &amp; Freunberger, R. (2016).
Fairer Vergleich in der Rückmeldung. 
In S. Breit &amp; C. Schreiner (Hrsg.), <em>Large-Scale Assessment mit <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>:  
Methodische Grundlagen der österreichischen Bildungsstandardüberprüfung</em> 
(pp. 295–332). Wien: facultas.
</p>


<h3>See Also</h3>


<p>Zu <code>datenKapitel09</code>, den im Kapitel verwendeten Daten.<br>
Zurück zu <code>Kapitel 8</code>, Fehlende Daten und Plausible Values.<br>
Zu <code>Kapitel 10</code>, Reporting und Analysen.<br>
Zur <code>Übersicht</code>.



</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
library(miceadds)
library(glmnet)
library(kerdiest)

covainteraction &lt;- function(dat,covas,nchar){
  for(ii in 1:(length(covas))){
    vv1 &lt;- covas[ii]
    # Interaktion von vv1 mit sich selbst
    subname1 &lt;- substr(vv1,1,nchar)
    intvar &lt;- paste0(subname1, subname1)
    if(vv1 == covas[1]){
      dat.int &lt;- dat[,vv1]*dat[,vv1];
      newvars &lt;- intvar } else {
        dat.int &lt;- cbind(dat.int,dat[,vv1]*dat[,vv1]); 
        newvars &lt;- c(newvars,intvar) 
      }
    # Interaktion von vv1 mit restlichen Variablen
    if(ii &lt; length(covas)){
      for(jj in ((ii+1):length(covas))){
        vv2 &lt;- covas[jj]
        subname2 &lt;- substr(vv2,1,nchar)
        intvar &lt;- paste0(subname1, subname2)
        newvars &lt;- c(newvars, intvar)
        dat.int &lt;- cbind(dat.int,dat[,vv1]*dat[,vv2])
      }
    }
    
  }
  dat.int &lt;- data.frame(dat.int)
  names(dat.int) &lt;- newvars
  return(dat.int)
}

data(datenKapitel09)
dat &lt;- datenKapitel09

# Platzhalter für Leistungsschätzwerte aller Modelle
dat$expTWLE.OLS1 &lt;- NA
dat$expTWLE.OLS2 &lt;- NA
dat$expTWLE.Lasso1 &lt;- NA
dat$expTWLE.Lasso2 &lt;- NA
dat$expTWLE.np &lt;- NA

## -------------------------------------------------------------
## Abschnitt 9.2.5, Umsetzung in R
## -------------------------------------------------------------

# -------------------------------------------------------------
# Abschnitt 9.2.5.1, Listing 1: Kovariatenauswahl und
#                               z-Standardisierung
#

vars &lt;- c("groesse","female","mig","sozstat")
zvars &lt;- paste0("z",vars)
dat[,zvars] &lt;- scale(dat[,vars],scale = TRUE)

# -------------------------------------------------------------
# Abschnitt 9.2.5.1, Listing 2: 
#

# Interaktionen bilden, z-standardisieren  
dat1 &lt;- LSAmitR::covainteraction(dat = dat,covas = zvars,nchar = 4)
intvars &lt;- names(dat1) # Interaktionsvariablen
dat1[,intvars] &lt;- scale(dat1[,intvars],scale = TRUE)
dat &lt;- cbind(dat,dat1)

# -------------------------------------------------------------
# Abschnitt 9.2.5.1, Listing 3: Modellprädiktoren: Haupt- und
#                               Interaktionseffekte
#

maineff &lt;- zvars # Haupteffekte 
alleff &lt;- c(zvars,intvars) # Haupt- und Interaktionseffekte

# -------------------------------------------------------------
# Abschnitt 9.2.5.2, Listing 4: OLS-Regression mit Haupteffekten
# 

fm.ols1 &lt;- paste0("TWLE ~ ",paste(maineff,collapse=" + "))
fm.ols1 &lt;- as.formula(fm.ols1) # Modellgleichung
st &lt;- 4
pos &lt;- which(dat$stratum == st) # Schulen im Stratum st
ols.mod1 &lt;- lm(formula = fm.ols1,data = dat[pos,]) # Regression

# -------------------------------------------------------------
# Abschnitt 9.2.5.3, Listing 5: Lasso-Regression
# Datenaufbereitung
#

library(glmnet)
Z &lt;- as.matrix(dat[pos,alleff]) # Kovariatenmatrix
Y &lt;- dat$TWLE[pos] # Abhängige Variable

# -------------------------------------------------------------
# Abschnitt 9.2.5.3, Listing 6: Lasso-Regression
# Bestimmung Teilmengen für Kreuzvalidierung, Lasso-Regression
#

nid &lt;- floor(length(pos)/3) # Teilmengen definieren 
foldid &lt;- rep(c(1:nid),3,length.out=length(pos)) # Zuweisung
lasso.mod2 &lt;- glmnet::cv.glmnet(x=Z,y=Y,alpha = 1, foldid = foldid)

# -------------------------------------------------------------
# Abschnitt 9.2.5.3, Listing 7: Lasso-Regression
# Erwartungswerte der Schulen
#

lasso.pred2 &lt;- predict(lasso.mod2,newx = Z,s="lambda.min")
dat$expTWLE.Lasso2[pos] &lt;- as.vector(lasso.pred2)

# -------------------------------------------------------------
# Abschnitt 9.2.5.3, Listing 8: Lasso-Regression
# Bestimmung R^2
#

varY &lt;- var(dat$TWLE[pos])
varY.lasso.mod2 &lt;- var(dat$expTWLE.Lasso2[pos])
R2.lasso.mod2 &lt;- varY.lasso.mod2/varY

# -------------------------------------------------------------
# Abschnitt 9.2.5.4, Listing 9: Nichtparametrische Regression
# Distanzberechnung zur Schule i (Stratum st)
#

N &lt;- length(pos) # Anzahl Schulen im Stratum
schools &lt;- dat$idschool[pos] # Schulen-ID
i &lt;- 1
# Teildatensatz von Schule i
dat.i &lt;- dat[pos[i],c("idschool","TWLE",maineff)]
names(dat.i) &lt;- paste0(names(dat.i),".i")
# Daten der Vergleichsschulen
dat.vgl &lt;- dat[pos[-i],c("idschool","TWLE",maineff)]
index.vgl &lt;- match(dat.vgl$idschool,schools)
# Daten zusammenfügen
dfr.i &lt;- data.frame("index.i"=i,dat.i,"index.vgl"=index.vgl,
                    dat.vgl, row.names=NULL)
# Distanz zur Schule i
dfr.i$dist &lt;- 0
gi &lt;- c(1,1,1,1)
for(ii in 1:length(maineff)){
  vv &lt;- maineff[ii]
  pair.vv &lt;- grep(vv, names(dfr.i), value=T)
  dist.vv &lt;- gi[ii]*((dfr.i[,pair.vv[1]]-dfr.i[,pair.vv[2]])^2)
  dfr.i$dist &lt;- dfr.i$dist + dist.vv }

# -------------------------------------------------------------
# Abschnitt 9.2.5.4, Listing 10: Nichtparametrische Regression
#

# H initiieren
d.dist &lt;- max(dfr.i$dist)-min(dfr.i$dist)
H &lt;- c(seq(d.dist/100,d.dist,length=30),100000)
V1 &lt;- length(H) 
# Anzahl Vergleichsschulen
n &lt;- nrow(dfr.i) 

# -------------------------------------------------------------
# Abschnitt 9.2.5.4, Listing 11: Nichtparametrische Regression
# Berechnung der Leave-One-Out-Schätzer der jeweiligen 
# Vergleichsschule k nach h in H
#

sumw &lt;- 0*H # Vektor w_{ik} initiieren, h in H
av &lt;- "TWLE"
dfr0.i &lt;- dfr.i[,c("idschool",av)]
# Schleife über alle h-Werte
for (ll in 1:V1 ){
  h &lt;- H[ll]
  # Gewicht w_{ik} bei h
  dfr.i$wgt.h &lt;- dnorm(sqrt(dfr.i$dist), mean=0, sd=sqrt(h))
  # Summe von w_{ik} bei h
  sumw[which(H==h)] &lt;- sum(dfr.i$wgt.h)
  # Leave-one-out-Schätzer von Y_k
  for (k in 1:n){
    # Regressionsformel
    fm &lt;- paste0(av,"~",paste0(maineff,collapse="+"))
    fm &lt;- as.formula(fm)
    # Regressionsanalyse ohne Beitrag von Schule k
    dfr.i0 &lt;- dfr.i[-k,]
    mod.k &lt;- lm(formula=fm,data=dfr.i0,weights=dfr.i0$wgt.h)
    # Erwartungswert anhand Kovariaten der Schule k berechnen
    pred.k &lt;- predict(mod.k, dfr.i)[k]
    dfr0.i[k,paste0( "h_",h) ] &lt;- pred.k
}}
# Erwartungswerte auf Basis verschiedener h-Werte
dfr1 &lt;- data.frame("idschool.i"=dfr.i$idschool.i[1],"h"=H )

# -------------------------------------------------------------
# Abschnitt 9.2.5.4, Listing 12: Nichtparametrische Regression
# Berechnung des Kreuzvalidierungskriteriums nach h in H
#

library(kerdiest)
hAL &lt;- kerdiest::ALbw("n",dfr.i$dist) # Plug-in Bandweite
dfr.i$cross.h &lt;- hAL
dfr.i$crosswgt &lt;- dnorm( sqrt(dfr.i$dist), mean=0, sd = sqrt(hAL) ) 
# Kreuzvalidierungskriterium CVh
vh &lt;- grep("h_",colnames(dfr0.i),value=TRUE)
for (ll in 1:V1){
  dfr1[ll,"CVh"] &lt;- sum( (dfr0.i[,av] - dfr0.i[,vh[ll]])^2 * 
                           dfr.i$crosswgt) / n}

# -------------------------------------------------------------
# Abschnitt 9.2.5.4, Listing 13: Nichtparametrische Regression
# Bestimmung optimales Wertes von h (h.min)
#

dfr1$min.h.index &lt;- 0
ind &lt;-  which.min( dfr1$CVh )
dfr1$min.h.index[ind ] &lt;- 1
dfr1$h.min &lt;- dfr1$h[ind]

# -------------------------------------------------------------
# Abschnitt 9.2.5.4, Listing 14: Nichtparametrische Regression
# Kleinste Quadratsumme der Schätzfehler
#

dfr1$CVhmin &lt;- dfr1[ ind , "CVh" ]

# -------------------------------------------------------------
# Abschnitt 9.2.5.4, Listing 15: Nichtparametrische Regression
# Effizienzsteigerung berechnen
#

dfr1$eff_gain &lt;-  100 * ( dfr1[V1,"CVh"] / dfr1$CVhmin[1] - 1 )

# -------------------------------------------------------------
# Abschnitt 9.2.5.4, Listing 16: Nichtparametrische Regression
# Durchführung der nichtparametrischen Regression bei h=h.min
#

h &lt;- dfr1$h.min[1]  # h.min
dfr.i$wgt.h &lt;- dnorm(sqrt(dfr.i$dist),sd=sqrt(h))/
  dnorm(0,sd= sqrt(h)) # w_{ik} bei h.min      
dfr.i0 &lt;- dfr.i
# Lokale Regression    
mod.ii &lt;- lm(formula=fm,data=dfr.i0,weights=dfr.i0$wgt.h)
# Kovariaten Schule i
predM &lt;- data.frame(dfr.i[1,paste0(maineff,".i")])    
names(predM) &lt;- maineff
pred.ii &lt;- predict(mod.ii, predM) # Schätzwert Schule i
dat[match(dfr1$idschool.i[1],dat$idschool), "expTWLE.np"] &lt;- pred.ii   

## -------------------------------------------------------------
## Abschnitt 9.2.5, Umsetzung in R, Ergänzung zum Buch
## -------------------------------------------------------------

# Korrelationen zwischen Haupteffekten
cor(dat[,maineff]) # gesamt
# Pro Stratum
for(s in 1:4) print(cor(dat[which(dat$stratum == s),maineff]))

# -------------------------------------------------------------
# Abschnitt 9.2.5.2, Ergänzung zum Buch
# OLS-Regression
#

# Modellgleichung nur mit Haupteffekten
fm.ols1 &lt;- paste0("TWLE ~ ",paste(maineff,collapse=" + "))
fm.ols1 &lt;- as.formula(fm.ols1)

# Modellgleichung mit Haupteffekten ohne zgroesse
fm.ols1a &lt;- paste0("TWLE ~ ",paste(setdiff(maineff,c("zgroesse")),
                                   collapse=" + "))
fm.ols1a &lt;- as.formula(fm.ols1a)

# Modellgleichung mit Haupt- und Interaktionseffekten
fm.ols2 &lt;- paste0("TWLE ~ ",paste(alleff,collapse=" + "))
fm.ols2 &lt;- as.formula(fm.ols2)

# Ergebnistabelle über 4 Strata hinweg vorbereiten
tab1 &lt;- data.frame("Variable"=c("(Intercept)",maineff))
tab2 &lt;- data.frame("Variable"=c("(Intercept)",alleff))

# Durchführung: Schleife über vier Strata
for(st in 1:4){
  # st &lt;- 4
  # Position Schulen des Stratums st im Datensatz
  pos &lt;- which(dat$stratum == st)
  
  #---------------------------------
  # OLS-Modell 1
  
  # Durchführung
  ols.mod1 &lt;- lm(formula = fm.ols1,data = dat[pos,])
  ols.mod1a &lt;- lm(formula = fm.ols1a,data = dat[pos,])
  
  # Modellergebnisse anzeigen
  summary(ols.mod1)
  summary(ols.mod1a)
  
  # Erwartungswerte der Schulen 
  dat$expTWLE.OLS1[pos] &lt;- fitted(ols.mod1)
  
  # Ergebnisse in Tabelle speichern
  par &lt;- summary(ols.mod1)
  tab.s &lt;- data.frame(par$coef,R2=par$r.squared,R2.adj=par$adj.r.squared)
  names(tab.s) &lt;- paste0("stratum",st,
                         c("_coef","_SE","_t","_p","_R2","_R2.adj"))
  tab1 &lt;- cbind(tab1, tab.s)
  
  # Durchführung OLS-Modell 2
  ols.mod2 &lt;- lm(formula = fm.ols2,data = dat[pos,])
  
  # Modellergebnisse anzeigen
  summary(ols.mod2)
  
  # Erwartungswerte der Schulen
  dat$expTWLE.OLS2[pos] &lt;- fitted(ols.mod2)
  
  # Ergebnisse in Tabelle speichern
  par &lt;- summary(ols.mod2)
  tab.s &lt;- data.frame(par$coef,R2=par$r.squared,R2.adj=par$adj.r.squared)
  names(tab.s) &lt;- paste0("stratum",st,
                         c("_coef","_SE","_t","_p","_R2","_R2.adj"))
  tab2 &lt;- cbind(tab2, tab.s) 
  
}

# Daten Schule 1196 ansehen
dat[which(dat$idschool == 1196),]

# Schätzwerte nach ols.mod1 und ols.mod2 vergleichen
summary(abs(dat$expTWLE.OLS1 - dat$expTWLE.OLS2))
cor.test(dat$expTWLE.OLS1,dat$expTWLE.OLS2)

# Grafische Darstellung des Vergleich (Schule 1196 rot markiert)
plot(dat$expTWLE.OLS1,dat$expTWLE.OLS2,xlim=c(380,650),ylim=c(380,650),
     col=1*(dat$idschool == 1196)+1,pch=15*(dat$idschool == 1196)+1)
abline(a=0,b=1)

# -------------------------------------------------------------
# Abschnitt 9.2.5.3, Ergänzung zum Buch
# Lasso-Regression
#

library(glmnet)

# Variablen für Erwartungswerte
dat$expTWLE.Lasso2 &lt;- dat$expTWLE.Lasso1 &lt;- NA

# Tabelle für Modellergebnisse
tab3 &lt;- data.frame("Variable"=c("(Intercept)",maineff))
tab4 &lt;- data.frame("Variable"=c("(Intercept)",alleff))

for(st in 1:4){
  # st &lt;- 4
  
  # Position Schulen des Stratums st im Datensatz
  pos &lt;- which(dat$stratum == st)
  
  #------------------------------------------------------------#
  # Lasso-Regression mit den Haupteffekten
  
  # Kovariatenmatrix
  Z &lt;- as.matrix(dat[pos,maineff])
  # Abhängige Variable
  Y &lt;- dat$TWLE[pos]
  
  # Kreuzvalidierung: Teilmengen definieren
  nid &lt;- floor(length(pos)/3)
  # Schulen zu Teilmengen zuordnen
  foldid &lt;- rep(c(1:nid),3,length.out=length(pos))
  
  # Regression
  lasso.mod1 &lt;- cv.glmnet(x=Z,y=Y,alpha = 1, foldid = foldid)
  
  # Ergebnisse ansehen
  print(lasso.mod1)
  
  # Lasso-Koeffizienten bei lambda.min
  print(lasso.beta &lt;- coef(lasso.mod1,s="lambda.min"))
  
  # Erwartungswerte der Schulen
  lasso.pred1 &lt;- predict(lasso.mod1,newx = Z,s="lambda.min")
  dat$expTWLE.Lasso1[pos] &lt;- as.vector(lasso.pred1)
  
  # R2 bestimmen
  varY &lt;- var(dat$TWLE[pos])
  varY.lasso.mod1 &lt;- var(dat$expTWLE.Lasso1[pos])
  print(R2.lasso.mod1 &lt;- varY.lasso.mod1/varY)
  
  # Ergebnistabelle
  vv &lt;- paste0("coef.stratum",st); tab3[,vv] &lt;- NA
  tab3[lasso.beta@i+1,vv] &lt;- lasso.beta@x
  vv &lt;- paste0("lambda.stratum",st); tab3[,vv] &lt;- lasso.mod1$lambda.min
  vv &lt;- paste0("R2.stratum",st); tab3[,vv] &lt;- R2.lasso.mod1
  
  #------------------------------------------------------------#
  # Lasso-Regression mit Haupt- und Interaktionseffekten
  
  # Kovariatenmatrix
  Z &lt;- as.matrix(dat[pos,alleff])
  
  # Regression
  lasso.mod2 &lt;- cv.glmnet(x=Z,y=Y,alpha = 1, foldid = foldid)
  
  # Ergebnisausdruck
  print(lasso.mod2)
  
  # Lasso-Koeffizienten bei lambda.min
  print(lasso.beta &lt;- coef(lasso.mod2,s="lambda.min"))
  
  # Erwartungswerte der Schulen
  lasso.pred2 &lt;- predict(lasso.mod2,newx = Z,s="lambda.min")
  dat$expTWLE.Lasso2[pos] &lt;- as.vector(lasso.pred2)
  
  # R2 bestimmen
  varY.lasso.mod2 &lt;- var(dat$expTWLE.Lasso2[pos])
  R2.lasso.mod2 &lt;- varY.lasso.mod2/varY
  R2.lasso.mod2
  
  # Ergebnistabelle
  vv &lt;- paste0("coef.stratum",st); tab4[,vv] &lt;- NA
  tab4[lasso.beta@i+1,vv] &lt;- lasso.beta@x
  vv &lt;- paste0("lambda.stratum",st); tab4[,vv] &lt;- lasso.mod2$lambda.min
  vv &lt;- paste0("R2.stratum",st); tab4[,vv] &lt;- R2.lasso.mod2
  
  
}

# Regressionresiduen = Schätzung von SChul- und Unterrichtseffekt
dat$resTWLE.Lasso1 &lt;- dat$TWLE - dat$expTWLE.Lasso1
dat$resTWLE.Lasso2 &lt;- dat$TWLE - dat$expTWLE.Lasso2

# -------------------------------------------------------------
# Abschnitt 9.2.5.4, Ergänzung zum Buch
# Nichtparametrische Regression
#

#
# Achtung: Der nachfolgende Algorithmus benötigt viel Zeit!
# 

av &lt;- "TWLE" # Abhängige Variable
dfr3 &lt;- NULL # Ergebnistabelle

# Variable für Leistungsschätzwerte

# Schleife über 4 Strata
for(st in 1:4){
  # st &lt;- 1
  pos &lt;- which(dat$stratum == st)
  N &lt;- length(pos)
  schools &lt;- dat$idschool[pos]
  
  ###
  # Distanzmatrix dfr für alle Schulen im Stratum erstellen
  dfr &lt;- NULL
  
  for (i in 1:N){
    # i &lt;- 1
    # Teildatensatz von Schule i
    dat.i &lt;- dat[pos[i],c("idschool","TWLE",maineff)]
    # Daten der Vergleichsgruppe
    dat.vgl &lt;- dat[pos[-i],c("idschool","TWLE",maineff)]
    # Variablennamen von dat.vgl umbenennen
    # names(dat.vgl) &lt;- paste0("vgl.",names(dat.vgl))
    # Variablennamen von dat.i umbenennen
    names(dat.i) &lt;- paste0(names(dat.i),".i")
    
    # Daten zusammenfügen
    index.vgl &lt;- match(dat.vgl$idschool,schools)
    dfr.i &lt;- data.frame("index.i"=i,dat.i,
                        "index.vgl"=index.vgl,dat.vgl,
                        row.names=NULL)
    
    # Distanz zur i
    dfr.i$dist &lt;- 0
    gi &lt;- c(1,1,1,1)
    for(ii in 1:length(maineff)){
      vv &lt;- maineff[ii]
      pair.vv &lt;- grep(vv, names(dfr.i), value=T)
      dist.vv &lt;- gi[ii]*((dfr.i[,pair.vv[1]]-dfr.i[,pair.vv[2]])^2)
      dfr.i$dist &lt;- dfr.i$dist + dist.vv
    }
    
    print(i) ; flush.console()
    dfr &lt;- rbind( dfr , dfr.i )
  }
  
  dfr1 &lt;- index.dataframe( dfr , systime=TRUE )
  
  ###
  # h-Auswahl und Nichtparametrische Regression pro Schule i
  dfr1.list &lt;- list()
  for (i in 1:N){
    # i &lt;- 1
    dfr.i &lt;- dfr[ dfr$index.i == i , ]
    n &lt;- nrow(dfr.i)
    
    # Startwertliste für h initiieren
    d.dist &lt;- max(dfr.i$dist)-min(dfr.i$dist)
    H &lt;- c(seq(d.dist/100,d.dist,length=30),100000)
    V1 &lt;- length(H) # Anzahl der Startwerte in H
    
    # Startwerte: Summe von w_ik
    sumw &lt;- 0*H
    dfr0.i &lt;- dfr.i[,c("idschool",av)]
    # Schleife über alle h-Werte
    for (ll in 1:V1 ){
      h &lt;- H[ll]
      # Gewicht w_ik bei h
      dfr.i$wgt.h &lt;- dnorm(sqrt(dfr.i$dist), mean=0, sd=sqrt(h))
      # Summe von w_ik bei h
      sumw[which(H==h)] &lt;- sum(dfr.i$wgt.h)
      # Leave-one-out-Schätzer von Y_k
      for (k in 1:n){
        # Regressionsformel
        fm &lt;- paste0(av,"~",paste0(maineff,collapse="+"))
        fm &lt;- as.formula(fm)
        # Regressionsanalyse ohne Beitrag von Schule k
        dfr.i0 &lt;- dfr.i[-k,]
        mod.k &lt;- lm(formula=fm,data=dfr.i0,weights=dfr.i0$wgt.h)
        # Erwartungswert anhand Kovariaten der Schule k berechnen
        pred.k &lt;- predict(mod.k, dfr.i)[k]
        dfr0.i[k,paste0( "h_",h) ] &lt;- pred.k
      }
      print(paste0("i=",i,", h_",ll))
    }
    # Erwartungswerte auf Basis verschiedener h-Werte
    dfr1 &lt;- data.frame("idschool.i"=dfr.i$idschool.i[1],"h"=H )
    
    # Berechnung des Kreuzvalidierungskriteriums
    library(kerdiest)
    hAL &lt;- kerdiest::ALbw("n",dfr.i$dist) # Plug-in Bandbreite nach Altman und 
                                          # Leger
    name &lt;- paste0( "bandwidth_choice_school" , dfr.i$idschool.i[1] ,  
                     "_cross.h_" , round2(hAL,1) )
    # Regressionsgewichte auf Basis cross.h
    dfr.i$cross.h &lt;- hAL
    dfr.i$crosswgt &lt;- dnorm( sqrt(dfr.i$dist), mean=0, sd = sqrt(hAL) ) 
    
    dfr.i &lt;- index.dataframe( dfr.i , systime=TRUE )

    # Kreuzvalidierungskriterium CVh
    vh &lt;- grep("h_",colnames(dfr0.i),value=TRUE)
    for (ll in 1:V1){
      # ll &lt;- 5
      dfr1[ll,"CVh"] &lt;- sum( (dfr0.i[,av] - dfr0.i[,vh[ll]])^2 * 
                               dfr.i$crosswgt) / n
      print(ll)
    }
    
    # Bestimmung h.min
    dfr1$min.h.index &lt;- 0
    ind &lt;-  which.min( dfr1$CVh )
    dfr1$min.h.index[ind ] &lt;- 1
    dfr1$h.min &lt;- dfr1$h[ind]
    # Kleinste Quadratsumme der Schätzfehler
    dfr1$CVhmin &lt;- dfr1[ ind , "CVh" ]
    
    # Effizienzsteigerung berechnen
    dfr1$eff_gain &lt;-  100 * ( dfr1[V1,"CVh"] / dfr1$CVhmin[1] - 1 )
    
    # h auswählen
    h &lt;- dfr1$h.min[1]
    
    # Gewichte anhand h berechnen
    dfr.i$wgt.h &lt;- dnorm( sqrt( dfr.i$dist ) , sd = sqrt( h) ) / 
                   dnorm( 0 , sd = sqrt( h) )     
    dfr.i0 &lt;- dfr.i
    mod.ii &lt;- lm(formula = fm,data = dfr.i0,weights = dfr.i0$wgt.h)
    
    # Leistungsschätzwerte berechnen
    predM &lt;- data.frame(dfr.i[1,paste0(maineff,".i")])
    names(predM) &lt;- maineff
    
    pred.ii &lt;- predict( mod.ii ,  predM )
    dfr1$fitted_np &lt;- pred.ii  
    dfr1$h.min_sumwgt &lt;- sum( dfr.i0$wgt.h )
    dfr1$h_sumwgt  &lt;- sumw
    
    # Leistungsschätzwerte zum Datensatz hinzufügen
    dat$expTWLE.np[match(dfr1$idschool.i[1],dat$idschool)] &lt;- pred.ii
    dfr1.list[[i]] &lt;- dfr1
  }
  
  ###
  # Ergebnisse im Stratum st zusammenfassen
  dfr2 &lt;- NULL

  for(i in 1:length(dfr1.list)){
    dat.ff &lt;- dfr1.list[[i]]
    dfr2.ff &lt;- dat.ff[1,c("idschool.i","h.min","fitted_np","h.min_sumwgt",
                          "CVhmin","eff_gain")]
    dfr2.ff$CVlinreg &lt;- dat.ff[V1,"CVh"]
    names(dfr2.ff) &lt;- c("idschool","h.min","fitted_np","h.min_sumwgt",
                        "CVhmin","eff_gain","CVlinreg")
    dfr2 &lt;- rbind(dfr2, dfr2.ff)
    print(i)
  }
  
  #---------------------------------------------------##
  # R2 berechnen
  varY &lt;- var(dat$TWLE[pos])
  varY.np &lt;- var(dat$expTWLE.np[pos])
  dfr2$R2.np &lt;- varY.np/varY
  
  #---------------------------------------------------##
  # Zur Gesamtergebnistabelle
  dfr3 &lt;- rbind(dfr3,cbind("Stratum"=st,dfr2))
  
}

# Effizienz der NP-Regression gegenüber OLS-Regression
summary(dfr3$eff_gain)
table(dfr3$eff_gain &gt; 5)
table(dfr3$eff_gain &gt; 10)
table(dfr3$eff_gain &gt; 20)

# Regressionsresiduen
dat$resTWLE.np &lt;- dat$TWLE - dat$expTWLE.np

## -------------------------------------------------------------
## Abschnitt 9.2.6, Ergänzung zum Buch
## Ergebnisse im Vergleich
## -------------------------------------------------------------

# Output-Variablen
out &lt;- grep("expTWLE",names(dat),value=T)
lt &lt;- length(out)

# Korrelationsmatrix
tab &lt;- tab1 &lt;- as.matrix(round2(cor(dat[,out]),3))

# Varianzmatrix
tab2 &lt;- as.matrix(round2(sqrt(var(dat[,out])),1))

tab3 &lt;- matrix(NA,lt,lt)
# Differenzmatrix
for(ii in 1:(lt-1))
  for(jj in (ii+1):lt) tab3[ii,jj] &lt;- round2(mean(abs(dat[,out[jj]] - 
                                                      dat[,out[ii]])),1)

tab4 &lt;- matrix(NA,lt,lt)
# Differenzmatrix
for(ii in 1:(lt-1))
  for(jj in (ii+1):lt) tab4[ii,jj] &lt;- round2(sd(abs(dat[,out[jj]] - 
                                                    dat[,out[ii]])),1)

# Ergebnistabelle
diag(tab) &lt;- diag(tab2)
tab[upper.tri(tab)] &lt;- tab3[upper.tri(tab3)]

# R2 Gesamt
varY &lt;- var(dat$TWLE)
varexp.OLS1 &lt;- var(dat$expTWLE.OLS1); R2.OLS1 &lt;- varexp.OLS1/varY
varexp.OLS2 &lt;- var(dat$expTWLE.OLS2); R2.OLS2 &lt;- varexp.OLS2/varY
varexp.Lasso1 &lt;- var(dat$expTWLE.Lasso1); R2.Lasso1 &lt;- varexp.Lasso1/varY
varexp.Lasso2 &lt;- var(dat$expTWLE.Lasso2); R2.Lasso2 &lt;- varexp.Lasso2/varY
varexp.np &lt;- var(dat$expTWLE.np); R2.np &lt;- varexp.np/varY
R2 &lt;- c(R2.OLS1,R2.OLS2,R2.Lasso1,R2.Lasso2,R2.np)
tab &lt;- cbind(tab,R2)

# R2 pro Stratum
dat0 &lt;- dat
for(st in 1:4){
  # st &lt;- 1
  dat &lt;- dat0[which(dat0$stratum == st),]
  varY &lt;- var(dat$TWLE)
  varexp.OLS1 &lt;- var(dat$expTWLE.OLS1); R2.OLS1 &lt;- varexp.OLS1/varY
  varexp.OLS2 &lt;- var(dat$expTWLE.OLS2); R2.OLS2 &lt;- varexp.OLS2/varY
  varexp.Lasso1 &lt;- var(dat$expTWLE.Lasso1); R2.Lasso1 &lt;- varexp.Lasso1/varY
  varexp.Lasso2 &lt;- var(dat$expTWLE.Lasso2); R2.Lasso2 &lt;- varexp.Lasso2/varY
  varexp.np &lt;- var(dat$expTWLE.np); R2.np &lt;- varexp.np/varY
  R2 &lt;- c(R2.OLS1,R2.OLS2,R2.Lasso1,R2.Lasso2,R2.np)
  tab &lt;- cbind(tab,R2)
}

colnames(tab)[7:10] &lt;- paste0("R2_stratum",1:4)

## -------------------------------------------------------------
## Abschnitt 9.2.7, Berücksichtigung der Schätzfehler
## -------------------------------------------------------------

# -------------------------------------------------------------
# Abschnitt 9.2.7, Listing 17: Bestimmung des Erwartungsbereichs
#

vv &lt;- "expTWLE.OLS1" # Variablenname
mm &lt;- "OLS1" # Kurzname des Modells
dfr &lt;- NULL # Ergebnistabelle
# Schleife über alle möglichen Breite von 10 bis 60
for(w in 10:60){
  # Variablen für Ergebnisse pro w
  var &lt;- paste0(mm,".pos.eb",w) # Position der Schule
  var.low &lt;- paste0(mm,".eblow",w) # Untere Grenze des EBs
  var.upp &lt;- paste0(mm,".ebupp",w) # Obere Grenze des EBs
  # Berechnen
  dat[,var.low] &lt;- dat[,vv]-w/2 # Untere Grenze des EBs
  dat[,var.upp] &lt;- dat[,vv]+w/2 # Obere Grenze des EBs 
  # Position: -1=unterhalb, 0=innerhalb, 1=oberhalb des EBs 
  dat[,var] &lt;- -1*(dat$TWLE &lt; dat[,var.low]) + 1*(dat$TWLE &gt; dat[,var.upp])
  # Verteilung der Schulpositionen
  tmp &lt;- data.frame(t(matrix(prop.table(table(dat[,var])))))
  names(tmp) &lt;- c("unterhalb","innerhalb","oberhalb")
  tmp &lt;- data.frame("ModellxBereich"=var,tmp); dfr &lt;- rbind(dfr,tmp) }

# Abweichung zur Wunschverteilung 25-50-25 
dfr1 &lt;- dfr 
dfr1[,c(2,4)] &lt;- (dfr1[,c(2,4)] - .25)^2 
dfr1[,3] &lt;- (dfr1[,3] - .5)^2 
dfr1$sumquare &lt;- rowSums(dfr1[,-1]) 
# Auswahl markieren 
dfr$Auswahl &lt;- 1*(dfr1$sumquare == min(dfr1$sumquare) )

# -------------------------------------------------------------
# Abschnitt 9.2.7, Ergänzung zum Buch
# Bestimmung des Erwartungsbereichs
# 

# Ergebnisse aller Schulen werden aus Ursprungsdatensatz geladen.
dat &lt;- datenKapitel09 

# Liste der Erwartungswerte-Variablen
exp.vars &lt;- grep("expTWLE",names(dat),value=T)
# Modellnamen
m.vars &lt;- gsub("expTWLE.","",exp.vars, fixed = TRUE)

# Liste der Ergebnistabelle
list0 &lt;- list()

# Ergebnisse
tab.erg &lt;- NULL

# Schleife über alle Erwartungswerte aller Modelle
for(ii in 1:length(exp.vars)){
  # ii &lt;- 1
  vv &lt;- exp.vars[ii]
  mm &lt;- m.vars[ii]
  
  # Ergebnistabelle
  dfr &lt;- NULL
  
  # Schleife über alle möglichen Breite von 10 bis 60
  for(w in 10:60){
    # eb &lt;- 10
    var &lt;- paste0(mm,".pos.eb",w) # Position der Schule
    var.low &lt;- paste0(mm,".eblow",w) # Untere Grenze des EBs
    var.upp &lt;- paste0(mm,".ebupp",w) # Obere Grenze des EBs
    # Untere Grenze des EBs = Erwartungswert - w/2
    dat[,var.low] &lt;- dat[,vv]-w/2
    # Obere Grenze des EBs = Erwartungswert + w/2
    dat[,var.upp] &lt;- dat[,vv]+w/2
    # Position der Schule bestimmen
    # -1 = unterhalb, 0 = innterhalb, 1 = oberhalb des EBs
    dat[,var] &lt;- -1*(dat$TWLE &lt; dat[,var.low]) + 1*(dat$TWLE &gt; dat[,var.upp])
    # Verteilung der Positionen
    tmp &lt;- data.frame(t(matrix(prop.table(table(dat[,var])))))
    names(tmp) &lt;- c("unterhalb","innerhalb","oberhalb")
    tmp &lt;- data.frame("ModellxBereich"=var,tmp)
    dfr &lt;- rbind(dfr,tmp)
  }
  
  # Vergleich mit Wunschverteilung 25-50-25
  dfr1 &lt;- dfr
  dfr1[,c(2,4)] &lt;- (dfr1[,c(2,4)] - .25)^2
  dfr1[,3] &lt;- (dfr1[,3] - .5)^2
  dfr1$sumquare &lt;- rowSums(dfr1[,-1])
  # Auswahl markieren
  dfr$Auswahl &lt;- 1*(dfr1$sumquare == min(dfr1$sumquare) )
  
  # Zum Liste hinzufügen
  list0[[ii]] &lt;- dfr
  print(dfr[which(dfr$Auswahl == 1),])
  tab.erg &lt;- rbind(tab.erg, dfr[which(dfr$Auswahl == 1),])
  
}

# Nur gewählte Ergebnisse im Datensatz beibehalten
all.vars &lt;- grep("eb",names(dat),value=T)
# Untere und Obere Grenze mit speichern
eb.vars &lt;- tab.erg[,1]
low.vars &lt;- gsub("pos.eb","eblow",eb.vars)
upp.vars &lt;- gsub("pos.eb","ebupp",eb.vars)
del.vars &lt;- setdiff(all.vars, c(eb.vars,low.vars,upp.vars))
dat &lt;- dat[,-match(del.vars,names(dat))]


## -------------------------------------------------------------
## Appendix: Abbildungen
## -------------------------------------------------------------

# -------------------------------------------------------------
# Abbildung 9.4
#

# Koeffizienten bei der ersten 50 lambdas ausdrucken
# Stratum 4

lambda &lt;- lasso.mod2$lambda[1:50]
a &lt;- round2(lambda,2)
a1 &lt;- a[order(a)]
L &lt;- length(a)

dfr &lt;- NULL

for(ll in 1:L){
  dfr.ll &lt;- as.matrix(coef(lasso.mod2,newx = Z,s=a[ll] ))
  colnames(dfr.ll) &lt;- paste0("a_",ll)
  dfr.ll &lt;- data.frame("coef"=rownames(dfr.ll),dfr.ll)
  rownames(dfr.ll) &lt;- NULL
  if(ll == 1) dfr &lt;- dfr.ll else dfr &lt;- merge(dfr, dfr.ll)
}

# Ohne Intercept
dfr &lt;- dfr[-1,]
rownames(dfr) &lt;- 1:nrow(dfr)

cl &lt;- colors()
cl &lt;- grep("grey",cl,value=T)

# Umgekehrte Reihenfolge
dfr1 &lt;- dfr
for(x in 2:(L+1)) {dfr1[,x] &lt;- dfr[,(L+3)-x]; 
names(dfr1)[x] &lt;- names(dfr)[(L+3)-x]}

###
plot(x = log(a), y = rep(0,L), xlim = rev(range(log(a))), ylim=c(-20,22), 
     type = "l", xaxt ="n", xlab = expression(paste(lambda)), 
     ylab="Geschätzte Regressionskoeffizienten")
axis(1, at=log(a), labels=a,cex=1)

tmp &lt;- nrow(dfr)
for(ll in 1:tmp){
  # ll &lt;- 1
  lines(x=log(a),y=dfr[ll,2:(L+1)],type="l",pch=15-ll,col=cl[15-ll])
  points(x=log(a),y=dfr[ll,2:(L+1)],type="p",pch=15-ll)
  legend(x=2.8-0.7*(ll&gt;tmp/2),y=25-2*(ifelse(ll&gt;7,ll-7,ll)),
         legend =dfr$coef[ll],pch=15-ll,bty="n",cex=0.9)
}

# Kennzeichung der gewählten lambda
v &lt;- log(lasso.mod2$lambda.min)
lab2 &lt;- expression(paste("ausgewähltes ",lambda," = .43"))
text(x=v+0.6,y=-8,labels=lab2)

abline(v = v,lty=2,cex=1.2)

# -------------------------------------------------------------
# Abbildung 9.5
# Auswahl Lambda anhand min(cvm)
#

xlab &lt;- expression(paste(lambda))
plot(lasso.mod2, xlim = rev(range(log(lambda))), 
     ylim=c(550,1300),xlab=xlab,xaxt ="n",
     ylab = "Mittleres Fehlerquadrat der Kreuzvalidierung (cvm)",
     font.main=1,cex.main=1)
axis(1, at=log(a), labels=a,cex=1)

lab1 &lt;- expression(paste(lambda," bei min(cvm)"))
text(x=log(lasso.mod2$lambda.min)+0.5,y=max(lasso.mod2$cvm)-50,
     labels=lab1,cex=1)

lab2 &lt;- expression(paste("(ausgewähltes ",lambda," = .43)"))
text(x=log(lasso.mod2$lambda.min)+0.6,y=max(lasso.mod2$cvm)-100,
     labels=lab2,cex=1)

abline(v = log(lasso.mod2$lambda.min),lty=2)

text(x=log(lasso.mod2$lambda.min)-0.3,y = min(lasso.mod2$cvm)-30,
     labels="min(cvm)",cex=1 )
abline(h = min(lasso.mod2$cvm),lty=2)

text &lt;- expression(paste("Anzahl der Nicht-null-Koeffizienten (",
                         lambda," entsprechend)"))
mtext(text=text,side=3,line=3)


# -------------------------------------------------------------
# Abbildung 9.6
# Rohwert-Schätzwert Schule 1196 &amp; 1217 im Vergleich
#

id &lt;- c(1196, 1217)
par(mai=c(1.2,3,1,.5))
plot(x=rep(NA,2),y=c(1:2),xlim=c(470,610),yaxt ="n",type="l",
     xlab="Erwartungswerte je nach Modell und Schulleistung",ylab="")
legend &lt;- c("Schulleistung (TWLE)",paste0("", c("OLS1","OLS2","Lasso1",
                                                "Lasso2","NP"),
                                          "-Modell"))
axis(2, at=c(seq(1,1.4,0.08),seq(1.6,2,0.08)), las=1,cex=0.7,
     labels=rep(legend,2))
text &lt;- paste0("Schule ",id)
mtext(text=text,side=2,at = c(1.2,1.8),line = 10)

exp.vars &lt;- c("TWLE", 
              paste0("expTWLE.", c("OLS1","OLS2","Lasso1","Lasso2","np")))

pch = c(19, 0,3,2,4,5)
ii &lt;- 1
col = c("grey", rep("lightgrey",5))
for(vv in exp.vars){
  # vv &lt;- "TWLE"
  x &lt;- dat0[which(dat0$idschool %in% id),vv]
  abline(h = c(0.92+ii*0.08,1.52+ii*0.08), lty=1+1*(ii&gt;1),col=col[ii])
  points(x=x,y=c(0.92+ii*0.08,1.52+ii*0.08),type="p",pch=pch[ii])
  ii &lt;- ii + 1
}

## End(Not run)
</code></pre>


</div>