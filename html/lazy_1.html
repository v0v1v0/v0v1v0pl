<div class="container">

<table style="width: 100%;"><tr>
<td>lazy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Lazy learning for local regression</h2>

<h3>Description</h3>

<p>By combining constant, linear, and quadratic local models,
<code>lazy</code> estimates the value of an unknown multivariate function on
the basis of a set of possibly noisy samples of the function itself.
This implementation of lazy learning automatically adjusts the
bandwidth on a query-by-query basis through a leave-one-out
cross-validation.</p>


<h3>Usage</h3>

<pre><code class="language-R">lazy(formula, data=NULL, weights, subset, na.action,
        control=lazy.control(...), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>A formula specifying the response and some numeric
predictors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>An optional data frame within which to look first for the
response, predictors, and weights (the latter will be
ignored).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>Optional weights for each case (ignored).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>
<p>An optional specification of a subset of the data to be
used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>
<p>The action to be taken with missing values in the response
or predictors.  The default is to stop.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>Control parameters: see <code>lazy.control</code></p>
</td>
</tr>
</table>
<p>.
</p>
<table><tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Control parameters can also be supplied directly.</p>
</td>
</tr></table>
<h3>Details</h3>

<p>For one or more query points, <code>lazy</code> estimates the value of
an unknown multivariate function on the basis of a set of possibly
noisy samples of the function itself.  Each sample is an input/output
pair where the input is a vector and the output is a number.  For each
query point, the estimation of the function is obtained by combining
different local models.  Local models considered for combination by
<code>lazy</code> are polynomials of zeroth, first, and second degree that
fit a set of samples in the neighborhood of the query point. The
neighbors are selected according to either the Manhattan or the
Euclidean distance. It is possible to assign weights to the different
directions of the input domain for modifying their importance in the
computation of the distance.  The number of neighbors used for
identifying local models is automatically adjusted on a query-by-query
basis through a leave-one-out validations of models, each fitting a
different numbers of neighbors.  The local models are identified using
the recursive least-squares algorithm, and the leave-one-out
cross-validation is obtained through the PRESS statistic.
</p>
<p>As the name <code>lazy</code> suggests, this function does not do
anything... apart from checking the options and properly packing
the data. All the actual computation is done when a prediction is
request for a specific query point, or for a set of query points: see
<code>predict.lazy</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>lazy</code>.</p>


<h3>Author(s)</h3>

<p>Mauro Birattari and Gianluca Bontempi</p>


<h3>References</h3>

<p>D.W. Aha (1997) Editorial. <em>Artificial Intelligence Review</em>,
<b>11</b>(1–5), pp. 1–6. Special Issue on Lazy Learning.
</p>
<p>C.G. Atkeson, A.W. Moore, and S. Schaal (1997) Locally Weighted
Learning. <em>Artificial Intelligence Review</em>, <b>11</b>(1–5),
pp. 11–73. Special Issue on Lazy Learning.
</p>
<p>W.S. Cleveland, S.J. Devlin, and S.J. Grosse (1988) Regression by
Local Fitting: Methods, Prospectives and Computational
Algorithms. <em>Journal of Econometrics</em>, <b>37</b>, pp. 87–114.
</p>
<p>M. Birattari, G. Bontempi, and H. Bersini (1999) Lazy learning meets
the recursive least squares algorithm. <em>Advances in Neural
Information Processing Systems 11</em>, pp. 375–381. MIT Press.
</p>
<p>G. Bontempi, M. Birattari, and H. Bersini (1999) Lazy learning for
modeling and control design. <em>International Journal of Control</em>,
<b>72</b>(7/8), pp. 643–658.
</p>
<p>G. Bontempi, M. Birattari, and H. Bersini (1999) Local learning for
iterated time-series prediction. <em>International Conference on
Machine Learning</em>, pp. 32–38. Morgan Kaufmann.
</p>


<h3>See Also</h3>

<p><code>lazy.control</code>, <code>predict.lazy</code></p>


<h3>Examples</h3>

<pre><code class="language-R">library("lazy")
data(cars)
cars.lazy &lt;- lazy(dist ~ speed, cars)
predict(cars.lazy, data.frame(speed = seq(5, 30, 1)))
</code></pre>


</div>