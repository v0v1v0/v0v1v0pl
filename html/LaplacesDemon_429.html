<div class="container">

<table style="width: 100%;"><tr>
<td>VariationalBayes</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Variational Bayes</h2>

<h3>Description</h3>

<p>The <code>VariationalBayes</code> function is a numerical approximation
method for deterministically estimating the marginal posterior
distributions, target distributions, in a Bayesian model with
approximated distributions by minimizing the Kullback-Leibler
Divergence (<code>KLD</code>) between the target and its
approximation.
</p>


<h3>Usage</h3>

<pre><code class="language-R">VariationalBayes(Model, parm, Data, Covar=NULL, Interval=1.0E-6,
     Iterations=1000, Method="Salimans2", Samples=1000, sir=TRUE,
     Stop.Tolerance=1.0E-5, CPUs=1, Type="PSOCK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Model</code></td>
<td>
<p>This required argument receives the model from a
user-defined function. The user-defined function is where the model
is specified. <code>VariationalBayes</code> passes two arguments to
the model function, <code>parms</code> and <code>Data</code>. For more
information, see the <code>LaplacesDemon</code> function and
“LaplacesDemon Tutorial” vignette.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parm</code></td>
<td>
<p>This argument requires a vector of initial values equal in
length to the number of parameters. <code>VariationalBayes</code> will
attempt to optimize these initial values for the parameters, where
the optimized values are the posterior means, for later use with the
<code>IterativeQuadrature</code>, <code>LaplacesDemon</code>, or
<code>PMC</code> function. The <code>GIV</code> function may be
used to randomly generate initial values. Parameters must be
continuous.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Data</code></td>
<td>
<p>This required argument accepts a list of data. The list of
data must include <code>mon.names</code> which contains monitored variable
names, and <code>parm.names</code> which contains parameter
names. <code>VariationalBayes</code> must be able to determine the
sample size of the data, and will look for a scalar sample size
variable <code>n</code> or <code>N</code>. If not found, it will look for
variable <code>y</code> or <code>Y</code>, and attempt to take its number of
rows as sample size. <code>VariationalBayes</code> needs to determine
sample size due to the asymptotic nature of this method. Sample size
should be at least <code class="reqn">\sqrt{J}</code> with <code class="reqn">J</code> exchangeable
parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Covar</code></td>
<td>
<p>This argument defaults to <code>NULL</code>, but may otherwise
accept a <code class="reqn">K \times K</code> covariance matrix (where <code class="reqn">K</code>
is the number of dimensions or parameters) of the parameters. When
the model is updated for the first time and prior variance or
covariance is unknown, then <code>Covar=NULL</code> should be used. Once
<code>VariationalBayes</code> has finished updating, it may be desired
to continue updating where it left off, in which case the covariance
matrix from the last run can be input into the next run.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Interval</code></td>
<td>
<p>This argument receives an interval for estimating
approximate gradients. The logarithm of the unnormalized joint
posterior density of the Bayesian model is evaluated at the current
parameter value, and again at the current parameter value plus this
interval.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Iterations</code></td>
<td>
<p>This argument accepts an integer that determines the
number of iterations that <code>VariationalBayes</code> will attempt
to maximize the logarithm of the unnormalized joint posterior
density. <code>Iterations</code> defaults to 1000.
<code>VariationalBayes</code> will stop before this number of
iterations if the tolerance is less than or equal to the
<code>Stop.Tolerance</code> criterion. The required amount of computer
memory increases with <code>Iterations</code>. If computer memory is
exceeded, then all will be lost.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Method</code></td>
<td>
<p>This optional argument currently accepts only
<code>Salimans2</code>, which is the second algorithm in Salimans and
Knowles (2013).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Samples</code></td>
<td>
<p>This argument indicates the number of posterior samples
to be taken with sampling importance resampling via the
<code>SIR</code> function, which occurs only when
<code>sir=TRUE</code>. Note that the number of samples should increase
with the number and intercorrelations of the parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sir</code></td>
<td>
<p>This logical argument indicates whether or not Sampling
Importance Resampling (SIR) is conducted via the <code>SIR</code>
function to draw independent posterior samples. This argument
defaults to <code>TRUE</code>. Even when <code>TRUE</code>, posterior samples
are drawn only when <code>VariationalBayes</code> has
converged. Posterior samples are required for many other functions,
including <code>plot.vb</code> and <code>predict.vb</code>. The only
time that it is advantageous for <code>sir=FALSE</code> is when
<code>VariationalBayes</code> is used to help the initial values for
<code>IterativeQuadrature</code>, <code>LaplacesDemon</code>, or
<code>PMC</code>, and it is unnecessary for time to be spent on
sampling. Less time can be spent on sampling by increasing
<code>CPUs</code>, which parallelizes the sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Stop.Tolerance</code></td>
<td>
<p>This argument accepts any positive number and
defaults to 1.0E-3. Tolerance is calculated each iteration, and the
criteria varies by algorithm. The algorithm is considered to have
converged to the user-specified <code>Stop.Tolerance</code> when the
tolerance is less than or equal to the value of
<code>Stop.Tolerance</code>, and the algorithm terminates at the end of
the current iteration. Often, multiple criteria are used, in
which case the maximum of all criteria becomes the tolerance. For
example, when partial derivatives are taken, it is commonly required
that the Euclidean norm of the partial derivatives is a criterion,
and another common criterion is the Euclidean norm of the
differences between the current and previous parameter
values. Several algorithms have other, specific tolerances.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur. Parallelization occurs only for
sampling with <code>SIR</code> when <code>sir=TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Variational Bayes (VB) is a family of numerical approximation
algorithms that is a subset of variational inference algorithms, or
variational methods. Some examples of variational methods include the
mean-field approximation, loopy belief propagation, tree-reweighted
belief propagation, and expectation propagation (EP).
</p>
<p>Variational inference for probabilistic models was introduced in the
field of machine learning, influenced by statistical physics
literature (Saul et al., 1996; Saul and Jordan, 1996; Jaakkola, 1997).
The mean-field methods in Neal and Hinton (1999) led to variational
algorithms.
</p>
<p>Variational inference algorithms were later generalized for conjugate
exponential-family models (Attias, 1999, 2000; Wiegerinck, 2000;
Ghahramani and Beal, 2001; Xing et al., 2003). These algorithms still
require different designs for different model forms. Salimans and
Knowles (2013) introduced general-purpose VB algorithms for Gaussian
posteriors.
</p>
<p>A VB algorithm deterministically estimates the marginal posterior
distributions (target distributions) in a Bayesian model with
approximated distributions by minimizing the Kullback-Leibler
Divergence (<code>KLD</code>) between the target and its
approximation. The complicated posterior distribution is approximated
with a simpler distribution. The simpler, approximated distribution is
called the variational approximation, or approximation distribution,
of the posterior. The term variational is derived from the calculus of
variations, and regards optimization algorithms that select the best
function (which is a distribution in VB), rather than merely selecting
the best parameters.
</p>
<p>VB algorithms often use Gaussian distributions as approximating
distributions. In this case, both the mean and variance of the
parameters are estimated.
</p>
<p>Usually, a VB algorithm is slower to convergence than a Laplace
Approximation algorithm, and faster to convergence than a Monte Carlo
algorithm such as Markov chain Monte Carlo (MCMC). VB often provides
solutions with comparable accuracy to MCMC in less time. Though Monte
Carlo algorithms provide a numerical approximation to the exact
posterior using a set of samples, VB provides a locally-optimal,
exact analytical solution to an approximation of the posterior. VB is
often more applicable than MCMC to big data or large-dimensional
models.
</p>
<p>Since VB is deterministic, it is asymptotic and subject to the same
limitations with respect to sample size as Laplace Approximation.
However, VB estimates more parameters than Laplace Approximation,
such as when Laplace Approximation optimizes the posterior mode of a
Gaussian distribution, while VB optimizes both the Gaussian mean and
variance.
</p>
<p>Traditionally, VB algorithms required customized equations. The
<code>VariationalBayes</code> function uses general-purpose algorithms. A
general-purpose VB algorithm is less efficient than an algorithm
custom designed for the model form. However, a general-purpose
algorithm is applied consistently and easily to numerous model forms.
</p>
<p>When <code>Method="Salimans2"</code>, the second algorithm of Salimans and
Knowles (2013) is used. This requires the gradient and Hessian, which
is more efficient with a small number of parameters as long as the
posterior is twice differentiable. The step size is constant. This
algorithm is suitable for marginal posterior distributions that are
Gaussian and unimodal. A stochastic approximation algorithm is used
in the context of fixed-form VB, inspired by considering fixed-form VB
to be equivalent to performing a linear regression with the sufficient
statistics of the approximation as independent variables and the
unnormalized logarithm of the joint posterior density as the dependent
variable. The number of requested iterations should be large, since the
step-size decreases for larger requested iterations, and a small
step-size will eventually converge. A large number of requested
iterations results in a smaller step-size and better convergence
properties, so hope for early convergence. However convergence is
checked only in the last half of the iterations after the algorithm
begins to average the mean and variance from the samples of the
stochastic approximation. The history of stochastic samples is
returned.
</p>


<h3>Value</h3>

<p><code>VariationalBayes</code> returns an object of class <code>vb</code>
that is a list with the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Call</code></td>
<td>
<p>This is the matched call of <code>VariationalBayes</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Converged</code></td>
<td>
<p>This is a logical indicator of whether or not
<code>VariationalBayes</code> converged within the specified
<code>Iterations</code> according to the supplied <code>Stop.Tolerance</code>
criterion. Convergence does not indicate that the global maximum has
been found, but only that the tolerance was less than or equal to
the <code>Stop.Tolerance</code> criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Covar</code></td>
<td>
<p>This is the estimated covariance matrix. The
<code>Covar</code> matrix may be scaled and input into the <code>Covar</code>
argument of the <code>LaplacesDemon</code> or <code>PMC</code>
function for further estimation, or the diagonal of this matrix may
be used to represent the posterior variance of the parameters,
provided the algorithm converged and matrix inversion was
successful. To scale this matrix for use with Laplace's Demon or
PMC, multiply it by <code class="reqn">2.38^2/d</code>, where <code class="reqn">d</code> is the number
of initial values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Deviance</code></td>
<td>
<p>This is a vector of the iterative history of the
deviance in the <code>VariationalBayes</code> function, as it sought
convergence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>History</code></td>
<td>
<p>This is an array of the iterative history of the
parameters in the <code>VariationalBayes</code> function, as it sought
convergence. The first matrix is for means and the second matrix is
for variances.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Initial.Values</code></td>
<td>
<p>This is the vector of initial values that was
originally given to <code>VariationalBayes</code> in the <code>parm</code>
argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>LML</code></td>
<td>
<p>This is an approximation of the logarithm of the marginal
likelihood of the data (see the <code>LML</code> function for more
information). When the model has converged and <code>sir=TRUE</code>, the
NSIS method is used. When the model has converged and
<code>sir=FALSE</code>, the LME method is used. This is the
logarithmic form of equation 4 in Lewis and Raftery (1997). As a
rough estimate of Kass and Raftery (1995), the LME-based LML is
worrisome when the sample size of the data is less than five times
the number of parameters, and <code>LML</code> should be adequate in most
problems when the sample size of the data exceeds twenty times the
number of parameters (p. 778). The LME is inappropriate with
hierarchical models. However <code>LML</code> is estimated, it is useful
for comparing multiple models with the <code>BayesFactor</code> function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>LP.Final</code></td>
<td>
<p>This reports the final scalar value for the logarithm
of the unnormalized joint posterior density.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>LP.Initial</code></td>
<td>
<p>This reports the initial scalar value for the
logarithm of the unnormalized joint posterior density.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Minutes</code></td>
<td>
<p>This is the number of minutes that
<code>VariationalBayes</code> was running, and this includes the
initial checks as well as drawing posterior samples and creating
summaries.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Monitor</code></td>
<td>
<p>When <code>sir=TRUE</code>, a number of independent
posterior samples equal to <code>Samples</code> is taken, and the draws
are stored here as a matrix. The rows of the matrix are the samples,
and the columns are the monitored variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Posterior</code></td>
<td>
<p>When <code>sir=TRUE</code>, a number of independent
posterior samples equal to <code>Samples</code> is taken, and the draws
are stored here as a matrix. The rows of the matrix are the samples,
and the columns are the parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Step.Size.Final</code></td>
<td>
<p>This is the final, scalar <code>Step.Size</code>
value at the end of the <code>VariationalBayes</code> algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Step.Size.Initial</code></td>
<td>
<p>This is the initial, scalar <code>Step.Size</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Summary1</code></td>
<td>
<p>This is a summary matrix that summarizes the
point-estimated posterior means and variances. Uncertainty around
the posterior means is estimated from the estimated covariance
matrix. Rows are parameters. The following columns are included:
Mean, SD (Standard Deviation), LB (Lower Bound), and UB (Upper
Bound). The bounds constitute a 95% probability interval.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Summary2</code></td>
<td>
<p>This is a summary matrix that summarizes the
posterior samples drawn with sampling importance resampling
(<code>SIR</code>) when <code>sir=TRUE</code>, given the point-estimated
posterior means and covariance matrix. Rows are parameters. The
following columns are included: Mean, SD (Standard Deviation),
LB (Lower Bound), and UB (Upper Bound). The bounds constitute a 95%
probability interval.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Tolerance.Final</code></td>
<td>
<p>This is the last <code>Tolerance</code> of the
<code>VariationalBayes</code> algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Tolerance.Stop</code></td>
<td>
<p>This is the <code>Stop.Tolerance</code> criterion.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Attias, H. (1999). "Inferring Parameters and Structure of Latent
Variable Models by Variational Bayes". In <em>Uncertainty in
Artificial Intelligence</em>.
</p>
<p>Attias, H. (2000). "A Variational Bayesian Framework for Graphical
Models". In <em>Neural Information Processing Systems</em>.
</p>
<p>Ghahramani, Z. and Beal, M. (2001). "Propagation Algorithms for
Variational Bayesian Learning". In <em>Neural Information Processing
Systems</em>, p. 507–513.
</p>
<p>Jaakkola, T. (1997). "Variational Methods for Inference and Estimation
in Graphical Models". PhD thesis, Massachusetts Institute of
Technology.
</p>
<p>Salimans, T. and Knowles, D.A. (2013). "Fixed-Form Variational
Posterior Approximation through Stochastic Linear Regression".
<em>Bayesian Analysis</em>, 8(4), p. 837–882.
</p>
<p>Neal, R. and Hinton, G. (1999). "A View of the EM Algorithm that
Justifies Incremental, Sparse, and Other Variants". In Learning in
Graphical Models, p. 355–368. MIT Press, 1999.
</p>
<p>Saul, L. and Jordan, M. (1996). "Exploiting Tractable Substructures in
Intractable Networks". <em>Neural Information Processing Systems</em>.
</p>
<p>Saul, L., Jaakkola, T., and Jordan, M. (1996). "Mean Field Theory for
Sigmoid Belief Networks". <em>Journal of Artificial Intelligence
Research</em>, 4, p. 61–76.
</p>
<p>Wiegerinck, W. (2000). "Variational Approximations Between Mean Field
Theory and the Junction Tree Algorithm". In <em>Uncertainty in
Artificial Intelligence</em>.
</p>
<p>Xing, E., Jordan, M., and Russell, S. (2003). "A Generalized Mean
Field Algorithm for Variational Inference in Exponential Families". In
<em>Uncertainty in Artificial Intelligence</em>.
</p>


<h3>See Also</h3>

<p><code>BayesFactor</code>,
<code>IterativeQuadrature</code>,
<code>LaplaceApproximation</code>,
<code>LaplacesDemon</code>,
<code>GIV</code>,
<code>LML</code>,
<code>PMC</code>, and
<code>SIR</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># The accompanying Examples vignette is a compendium of examples.
####################  Load the LaplacesDemon Library  #####################
library(LaplacesDemon)

##############################  Demon Data  ###############################
data(demonsnacks)
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(log(demonsnacks[,10]+1)))
J &lt;- ncol(X)
for (j in 2:J) X[,j] &lt;- CenterScale(X[,j])

#########################  Data List Preparation  #########################
mon.names &lt;- "mu[1]"
parm.names &lt;- as.parm.names(list(beta=rep(0,J), sigma=0))
pos.beta &lt;- grep("beta", parm.names)
pos.sigma &lt;- grep("sigma", parm.names)
PGF &lt;- function(Data) {
     beta &lt;- rnorm(Data$J)
     sigma &lt;- runif(1)
     return(c(beta, sigma))
     }
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y)

##########################  Model Specification  ##########################
Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[Data$pos.beta]
     sigma &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf)
     parm[Data$pos.sigma] &lt;- sigma
     ### Log-Prior
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu &lt;- tcrossprod(Data$X, t(beta))
     LL &lt;- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=mu[1],
          yhat=rnorm(length(mu), mu, sigma), parm=parm)
     return(Modelout)
     }

############################  Initial Values  #############################
#Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)
Initial.Values &lt;- rep(0,J+1)

#Fit &lt;- VariationalBayes(Model, Initial.Values, Data=MyData, Covar=NULL,
#     Iterations=1000, Method="Salimans2", Stop.Tolerance=1e-3, CPUs=1)
#Fit
#print(Fit)
#PosteriorChecks(Fit)
#caterpillar.plot(Fit, Parms="beta")
#plot(Fit, MyData, PDF=FALSE)
#Pred &lt;- predict(Fit, Model, MyData, CPUs=1)
#summary(Pred, Discrep="Chi-Square")
#plot(Pred, Style="Covariates", Data=MyData)
#plot(Pred, Style="Density", Rows=1:9)
#plot(Pred, Style="Fitted")
#plot(Pred, Style="Jarque-Bera")
#plot(Pred, Style="Predictive Quantiles")
#plot(Pred, Style="Residual Density")
#plot(Pred, Style="Residuals")
#Levene.Test(Pred)
#Importance(Fit, Model, MyData, Discrep="Chi-Square")

#Fit$Covar is scaled (2.38^2/d) and submitted to LaplacesDemon as Covar.
#Fit$Summary[,1] is submitted to LaplacesDemon as Initial.Values.
#End
</code></pre>


</div>