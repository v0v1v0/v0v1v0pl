<div class="container">

<table style="width: 100%;"><tr>
<td>KLD</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kullback-Leibler Divergence (KLD)</h2>

<h3>Description</h3>

<p>This function calculates the Kullback-Leibler divergence (KLD) between
two probability distributions, and has many uses, such as in lowest
posterior loss probability intervals, posterior predictive checks,
prior elicitation, reference priors, and Variational Bayes.
</p>


<h3>Usage</h3>

<pre><code class="language-R">KLD(px, py, base)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>px</code></td>
<td>
<p>This is a required vector of probability densities,
considered as <code class="reqn">p(\textbf{x})</code>. Log-densities are also
accepted, in which case both <code>px</code> and <code>py</code> must be
log-densities.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>py</code></td>
<td>
<p>This is a required vector of probability densities,
considered as <code class="reqn">p(\textbf{y})</code>. Log-densities are also
accepted, in which case both <code>px</code> and <code>py</code> must be
log-densities.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>base</code></td>
<td>
<p>This optional argument specifies the logarithmic base,
which defaults to <code>base=exp(1)</code> (or <code class="reqn">e</code>) and represents
information in natural units (nats), where <code>base=2</code> represents
information in binary units (bits).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The Kullback-Leibler divergence (KLD) is known by many names, some of
which are Kullback-Leibler distance, K-L, and logarithmic divergence.
KLD is an asymmetric measure of the difference, distance, or direct
divergence between two probability distributions
<code class="reqn">p(\textbf{y})</code> and <code class="reqn">p(\textbf{x})</code> (Kullback and
Leibler, 1951). Mathematically, however, KLD is not a distance,
because of its asymmetry.
</p>
<p>Here, <code class="reqn">p(\textbf{y})</code> represents the
“true” distribution of data, observations, or theoretical
distribution, and <code class="reqn">p(\textbf{x})</code> represents a theory,
model, or approximation of <code class="reqn">p(\textbf{y})</code>.
</p>
<p>For probability distributions <code class="reqn">p(\textbf{y})</code> and
<code class="reqn">p(\textbf{x})</code> that are discrete (whether the underlying
distribution is continuous or discrete, the observations themselves
are always discrete, such as from <code class="reqn">i=1,\dots,N</code>),
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{KLD}[p(\textbf{y}) || p(\textbf{x})] = \sum^N_i
  p(\textbf{y}_i)
  \log\frac{p(\textbf{y}_i)}{p(\textbf{x}_i)}</code>
</p>

<p>In Bayesian inference, KLD can be used as a measure of the information
gain in moving from a prior distribution, <code class="reqn">p(\theta)</code>,
to a posterior distribution, <code class="reqn">p(\theta | \textbf{y})</code>. As such, KLD is the basis of reference priors and lowest
posterior loss intervals (<code>LPL.interval</code>), such as in
Berger, Bernardo, and Sun (2009) and Bernardo (2005). The intrinsic
discrepancy was introduced by Bernardo and Rueda (2002). For more
information on the intrinsic discrepancy, see
<code>LPL.interval</code>.
</p>


<h3>Value</h3>

<p><code>KLD</code> returns a list with the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>KLD.px.py</code></td>
<td>
<p>This is <code class="reqn">\mathrm{KLD}_i[p(\textbf{x}_i) ||
      p(\textbf{y}_i)]</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KLD.py.px</code></td>
<td>
<p>This is <code class="reqn">\mathrm{KLD}_i[p(\textbf{y}_i) ||
      p(\textbf{x}_i)]</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mean.KLD</code></td>
<td>
<p>This is the mean of the two components above. This is
the expected posterior loss in <code>LPL.interval</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sum.KLD.px.py</code></td>
<td>
<p>This is <code class="reqn">\mathrm{KLD}[p(\textbf{x}) ||
      p(\textbf{y})]</code>. This is a directed
divergence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sum.KLD.py.px</code></td>
<td>
<p>This is <code class="reqn">\mathrm{KLD}[p(\textbf{y}) ||
      p(\textbf{x})]</code>. This is a directed divergence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mean.sum.KLD</code></td>
<td>
<p>This is the mean of the two components above.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intrinsic.discrepancy</code></td>
<td>
<p>This is minimum of the two directed
divergences.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Berger, J.O., Bernardo, J.M., and Sun, D. (2009). "The Formal
Definition of Reference Priors". <em>The Annals of Statistics</em>,
37(2), p. 905–938.
</p>
<p>Bernardo, J.M. and Rueda, R. (2002). "Bayesian Hypothesis Testing: A
Reference Approach". <em>International Statistical Review</em>, 70,
p. 351–372.
</p>
<p>Bernardo, J.M. (2005). "Intrinsic Credible Regions: An Objective
Bayesian Approach to Interval Estimation". <em>Sociedad de
Estadistica e Investigacion Operativa</em>, 14(2), p. 317–384.
</p>
<p>Kullback, S. and Leibler, R.A. (1951). "On Information and
Sufficiency". <em>The Annals of Mathematical Statistics</em>, 22(1),
p. 79–86.
</p>


<h3>See Also</h3>

<p><code>LPL.interval</code> and
<code>VariationalBayes</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(LaplacesDemon)
px &lt;- dnorm(runif(100),0,1)
py &lt;- dnorm(runif(100),0.1,0.9)
KLD(px,py)
</code></pre>


</div>