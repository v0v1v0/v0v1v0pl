<div class="container">

<table style="width: 100%;"><tr>
<td>layer</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Leabra layer class</h2>

<h3>Description</h3>

<p>This class simulates a biologically realistic layer of neurons in the
Leabra framework. It consists of several <code>unit</code> objects
in the variable (field) <code>units</code> and some layer-specific
variables.
</p>


<h3>Usage</h3>

<pre><code class="language-R">layer
</code></pre>


<h3>Format</h3>

<p><code>R6Class</code> object</p>


<h3>Value</h3>

<p>Object of <code>R6Class</code> with methods for calculating changes
of activation in a layer of neurons.
</p>


<h3>Fields</h3>


<dl>
<dt><code>units</code></dt>
<dd>
<p>A list with all <code>unit</code> objects of the layer.</p>
</dd>
<dt><code>avg_act</code></dt>
<dd>
<p>The average activation of all units in the layer
(this is an active binding).</p>
</dd>
<dt><code>n</code></dt>
<dd>
<p>Number of units in layer.</p>
</dd>
<dt><code>weights</code></dt>
<dd>
<p>A receiving x sending weight matrix, where the receiving units
(rows) has the current weight values for the sending units (columns). The
weights will be set by the <code>network</code> object, because they
depend on the connection to other layers.</p>
</dd>
<dt><code>ce_weights</code></dt>
<dd>
<p>Sigmoidal contrast-enhanced version of the weight matrix
<code>weights</code>. These weights will be set by the <code>network</code>
object.</p>
</dd>
<dt><code>layer_number</code></dt>
<dd>
<p>Layer number in network (this is 1 if you create
a layer on your own, without the network class).</p>
</dd>
</dl>
<h3>Methods</h3>


<dl>
<dt><code>new(dim, g_i_gain = 2)</code></dt>
<dd>
<p>Creates an object of this class with
default parameters.
</p>

<dl>
<dt><code>dim</code></dt>
<dd>
<p>A pair of numbers giving the dimensions (rows and
columns) of the layer.</p>
</dd>
<dt><code>g_i_gain</code></dt>
<dd>
<p>Gain factor for inhibitory conductance, if you
want less activation in a layer, set this higher.</p>
</dd>
</dl>
</dd>
<dt><code>get_unit_acts()</code></dt>
<dd>
<p>Returns a vector with the activations of all
units of a layer.
</p>
</dd>
<dt><code>get_unit_scaled_acts()</code></dt>
<dd>
<p>Returns a vector with the scaled
activations of all units of a layer. Scaling is done with
<code>recip_avg_act_n</code>, a reciprocal function of the number of active
units.
</p>
</dd>
<dt><code>cycle(intern_input, ext_input)</code></dt>
<dd>
<p>Iterates one time step with
layer object.
</p>

<dl>
<dt><code>intern_input</code></dt>
<dd>
<p>Vector with inputs from all other layers.
Each input has already been scaled by a reciprocal function of the
number of active units (<code>recip_avg_act_n</code>) of the sending layer
and by the connection strength between the receiving and sending
layer. The weight matrix <code>ce_weights</code> is multiplied with this
input vector to get the excitatory conductance for each unit in the
layer.
</p>
</dd>
<dt><code>ext_input</code></dt>
<dd>
<p>Vector with inputs not coming from another
layer, with length equal to the number of units in this layer. If
empty (<code>NULL</code>), no external inputs are processed. If the external
inputs are not clamped, this is actually an excitatory conductance
value, which is added to the conductance produced by the internal
input and weight matrix.
</p>
</dd>
</dl>
</dd>
<dt><code>clamp_cycle(activations)</code></dt>
<dd>
<p>Iterates one time step with layer
object with clamped activations, meaning that activations are
instantaneously set without time integration.
</p>

<dl>
<dt><code>activations</code></dt>
<dd>
<p>Activations you want to clamp to the units in
the layer.
</p>
</dd>
</dl>
</dd>
<dt><code>get_unit_act_avgs()</code></dt>
<dd>
<p>Returns a list with the short, medium and
long term activation averages of all units in the layer as vectors. The
super short term average is not returned, and the long term average is not
updated before being returned (this is done in the function <code>chg_wt()</code>
with the method<code>updt_unit_avg_l</code>). These averages are used by the
network class to calculate weight changes.
</p>
</dd>
<dt><code>updt_unit_avg_l()</code></dt>
<dd>
<p>Updates the long-term average
(<code>avg_l</code>) of all units in the layer, usually done after a plus phase.
</p>
</dd>
<dt><code>updt_recip_avg_act_n()</code></dt>
<dd>
<p>Updates the <code>avg_act_inert</code> and
<code>recip_avg_act_n</code> variables, these variables update before the weights
are changed instead of cycle by cycle. This version of the function assumes
full connectivity between layers.
</p>
</dd>
<dt><code>reset(random = FALSE)</code></dt>
<dd>
<p>Sets the activation and activation
averages of all units to 0. Used to begin trials from a stationary point.
</p>

<dl>
<dt><code>random</code></dt>
<dd>
<p>Logical variable, if TRUE the activations are set
randomly between .05 and .95 for every unit instead of 0.
</p>
</dd>
</dl>
</dd>
<dt><code>set_ce_weights()</code></dt>
<dd>
<p>Sets contrast enhanced weight values.
</p>
</dd>
<dt><code>get_unit_vars(show_dynamics = TRUE, show_constants =
  FALSE)</code></dt>
<dd>
<p>Returns a data frame with the current state of all unit variables
in the layer. Every row is a unit. You can choose whether you want dynamic
values and / or constant values. This might be useful if you want to
analyze what happens in units of a layer, which would otherwise not be
possible, because most of the variables (fields) are private in the unit
class.
</p>

<dl>
<dt><code>show_dynamics</code></dt>
<dd>
<p>Should dynamic values be shown? Default is
TRUE.
</p>
</dd>
<dt><code>show_constants</code></dt>
<dd>
<p>Should constant values be shown? Default
is FALSE.
</p>
</dd>
</dl>
</dd>
<dt><code>get_layer_vars(show_dynamics = TRUE, show_constants =
  FALSE)</code></dt>
<dd>
<p>Returns a data frame with 1 row with the current state of the
variables in the layer. You can choose whether you want dynamic values and
/ or constant values. This might be useful if you want to analyze what
happens in a layer, which would otherwise not be possible, because some of
the variables (fields) are private in the layer class.
</p>

<dl>
<dt><code>show_dynamics</code></dt>
<dd>
<p>Should dynamic values be shown? Default is
TRUE.
</p>
</dd>
<dt><code>show_constants</code></dt>
<dd>
<p>Should constant values be shown? Default
is FALSE.
</p>
</dd>
</dl>
</dd>
</dl>
<h3>References</h3>

<p>O'Reilly, R. C., Munakata, Y., Frank, M. J., Hazy, T. E., and
Contributors (2016). Computational Cognitive Neuroscience. Wiki Book, 3rd
(partial) Edition. URL: <a href="http://ccnbook.colorado.edu">http://ccnbook.colorado.edu</a>
</p>
<p>Have also a look at
<a href="https://grey.colorado.edu/emergent/index.php/Leabra">https://grey.colorado.edu/emergent/index.php/Leabra</a> (especially the
link to the 'MATLAB' code) and <a href="https://en.wikipedia.org/wiki/Leabra">https://en.wikipedia.org/wiki/Leabra</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">l &lt;- layer$new(c(5, 5)) # create a 5 x 5 layer with default leabra values

l$g_e_avg # private values cannot be accessed
# if you want to see alle variables, you need to use the function
l$get_layer_vars(show_dynamics = TRUE, show_constants = TRUE)
# if you want to see a summary of all units without constant values
l$get_unit_vars(show_dynamics = TRUE, show_constants = FALSE)

# let us clamp the activation of the 25 units to some random values between
# 0.05 and 0.95
l &lt;- layer$new(c(5, 5))
activations &lt;- runif(25, 0.05, .95)
l$avg_act
l$clamp_cycle(activations)
l$avg_act
# what happened to the unit activations?
l$get_unit_acts()
# compare with activations
activations
# scaled activations are scaled by the average activation of the layer and
# should be smaller
l$get_unit_scaled_acts()

</code></pre>


</div>