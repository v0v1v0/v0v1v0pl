<div class="container">

<table style="width: 100%;"><tr>
<td>gpMcp</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>gpMcp</h2>

<h3>Description</h3>

<p>Implements mcp regularization for general purpose optimization problems.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \begin{cases}
\lambda |x_j| - x_j^2/(2\theta) &amp; \text{if } |x_j| \leq \theta\lambda\\
\theta\lambda^2/2 &amp; \text{if } |x_j| &gt; \lambda\theta
\end{cases}</code>
</p>
<p> where <code class="reqn">\theta &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gpMcp(
  par,
  fn,
  gr = NULL,
  ...,
  regularized,
  lambdas,
  thetas,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>par</code></td>
<td>
<p>labeled vector with starting values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments passed to fn and gr</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thetas</code></td>
<td>
<p>numeric vector: values for the tuning parameter theta</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The interface is similar to that of optim. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels) and a fitting
function. This fitting functions <em>must</em> take a labeled vector with parameter
values as first argument. The remaining arguments are passed with the ... argument.
This is similar to optim.
</p>
<p>The gradient function gr is optional. If set to NULL, the <span class="pkg">numDeriv</span> package
will be used to approximate the gradients. Supplying a gradient function
can result in considerable speed improvements.
</p>
<p>mcp regularization:
</p>

<ul><li>
<p> Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.
The Annals of Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729
</p>
</li></ul>
<p>For more details on GLMNET, see:
</p>

<ul>
<li>
<p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li>
<p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li>
<p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li>
</ul>
<p>For more details on ISTA, see:
</p>

<ul>
<li>
<p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li>
<p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li>
<p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li>
</ul>
<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class="language-R"># This example shows how to use the optimizers
# for other objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(lessSEM)
set.seed(123)

# first, we simulate data for our
# linear regression.
N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4),
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

# First, we must construct a fiting function
# which returns a single value. We will use
# the residual sum squared as fitting function.

# Let's start setting up the fitting function:
fittingFunction &lt;- function(par, y, X, N){
  # par is the parameter vector
  # y is the observed dependent variable
  # X is the design matrix
  # N is the sample size
  pred &lt;- X %*% matrix(par, ncol = 1) #be explicit here:
  # we need par to be a column vector
  sse &lt;- sum((y - pred)^2)
  # we scale with .5/N to get the same results as glmnet
  return((.5/N)*sse)
}

# let's define the starting values:
# first, let's add an intercept
X &lt;- cbind(1, X)

b &lt;- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates
names(b) &lt;- paste0("b", 0:(length(b)-1))
# names of regularized parameters
regularized &lt;- paste0("b",1:p)

# optimize
mcpPen &lt;- gpMcp(
  par = b,
  regularized = regularized,
  fn = fittingFunction,
  lambdas = seq(0,1,.1),
  thetas = c(1.001, 1.5, 2),
  X = X,
  y = y,
  N = N
)

# optional: plot requires plotly package
# plot(mcpPen)

</code></pre>


</div>