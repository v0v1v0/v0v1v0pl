<div class="container">

<table style="width: 100%;"><tr>
<td>dist.Multivariate.Normal.Precision.Cholesky</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multivariate Normal Distribution: Precision-Cholesky Parameterization</h2>

<h3>Description</h3>

<p>These functions provide the density and random number generation for
the multivariate normal distribution, given the precision-Cholesky
parameterization.
</p>


<h3>Usage</h3>

<pre><code class="language-R">dmvnpc(x, mu, U, log=FALSE) 
rmvnpc(n=1, mu, U)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>This is data or parameters in the form of a vector of length
<code class="reqn">k</code> or a matrix with <code class="reqn">k</code> columns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>This is the number of random draws.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>This is mean vector <code class="reqn">\mu</code> with length <code class="reqn">k</code> or
matrix with <code class="reqn">k</code> columns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>U</code></td>
<td>
<p>This is the <code class="reqn">k \times k</code> upper-triangular of the
precision matrix that is Cholesky factor <code class="reqn">\textbf{U}</code> of
precision matrix <code class="reqn">\Omega</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>log</code></td>
<td>
<p>Logical. If <code>log=TRUE</code>, then the logarithm of the
density is returned.</p>
</td>
</tr>
</table>
<h3>Details</h3>


<ul>
<li>
<p> Application: Continuous Multivariate
</p>
</li>
<li>
<p> Density: <code class="reqn">p(\theta) = (2\pi)^{-p/2} |\Omega|^{1/2}
    \exp(-\frac{1}{2} (\theta-\mu)^T \Omega (\theta-\mu))</code>
</p>
</li>
<li>
<p> Inventor: Unknown (to me, anyway)
</p>
</li>
<li>
<p> Notation 1: <code class="reqn">\theta \sim \mathcal{MVN}(\mu, \Omega^{-1})</code>
</p>
</li>
<li>
<p> Notation 2: <code class="reqn">\theta \sim \mathcal{N}_k(\mu, \Omega^{-1})</code>
</p>
</li>
<li>
<p> Notation 3: <code class="reqn">p(\theta) = \mathcal{MVN}(\theta | \mu, \Omega^{-1})</code>
</p>
</li>
<li>
<p> Notation 4: <code class="reqn">p(\theta) = \mathcal{N}_k(\theta | \mu, \Omega^{-1})</code>
</p>
</li>
<li>
<p> Parameter 1: location vector <code class="reqn">\mu</code>
</p>
</li>
<li>
<p> Parameter 2: positive-definite <code class="reqn">k \times k</code> precision matrix <code class="reqn">\Omega</code>
</p>
</li>
<li>
<p> Mean: <code class="reqn">E(\theta) = \mu</code>
</p>
</li>
<li>
<p> Variance: <code class="reqn">var(\theta) = \Omega^{-1}</code>
</p>
</li>
<li>
<p> Mode: <code class="reqn">mode(\theta) = \mu</code>
</p>
</li>
</ul>
<p>The multivariate normal distribution, or multivariate Gaussian
distribution, is a multidimensional extension of the one-dimensional
or univariate normal (or Gaussian) distribution. It is usually
parameterized with mean and a covariance matrix, or in Bayesian
inference, with mean and a precision matrix, where the precision matrix
is the matrix inverse of the covariance matrix. These functions
provide the precision-Cholesky parameterization for convenience and
familiarity. It is easier to calculate a multivariate normal density
with the precision parameterization, because a matrix inversion can be
avoided. The precision matrix is replaced with an upper-triangular
<code class="reqn">k \times k</code> matrix that is Cholesky factor
<code class="reqn">\textbf{U}</code>, as per the <code>chol</code> function for Cholesky
decomposition.
</p>
<p>A random vector is considered to be multivariate normally distributed
if every linear combination of its components has a univariate normal
distribution. This distribution has a mean parameter vector
<code class="reqn">\mu</code> of length <code class="reqn">k</code> and a <code class="reqn">k \times k</code>
precision matrix <code class="reqn">\Omega</code>, which must be positive-definite.
</p>
<p>In practice, <code class="reqn">\textbf{U}</code> is fully unconstrained for proposals
when its diagonal is log-transformed. The diagonal is exponentiated
after a proposal and before other calculations. Overall, Cholesky
parameterization is faster than the traditional parameterization.
Compared with <code>dmvnp</code>, <code>dmvnpc</code> must additionally
matrix-multiply the Cholesky back to the covariance matrix, but it
does not have to check for or correct the precision matrix to
positive-definiteness, which overall is slower. Compared with
<code>rmvnp</code>, <code>rmvnpc</code> is faster because the Cholesky decomposition
has already been performed.
</p>
<p>For models where the dependent variable, Y, is specified to be
distributed multivariate normal given the model, the Mardia test (see
<code>plot.demonoid.ppc</code>, <code>plot.laplace.ppc</code>, or
<code>plot.pmc.ppc</code>) may be used to test the residuals.
</p>


<h3>Value</h3>

<p><code>dmvnpc</code> gives the density and 
<code>rmvnpc</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>See Also</h3>

<p><code>chol</code>,
<code>dmvn</code>,
<code>dmvnc</code>,
<code>dmvnp</code>,
<code>dnorm</code>,
<code>dnormp</code>,
<code>dnormv</code>,
<code>dwishartc</code>,
<code>plot.demonoid.ppc</code>,
<code>plot.laplace.ppc</code>, and
<code>plot.pmc.ppc</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(LaplacesDemon)
Omega &lt;- diag(3)
U &lt;- chol(Omega)
x &lt;- dmvnpc(c(1,2,3), c(0,1,2), U)
X &lt;- rmvnpc(1000, c(0,1,2), U)
joint.density.plot(X[,1], X[,2], color=TRUE)
</code></pre>


</div>