<div class="container">

<table style="width: 100%;"><tr>
<td>lev_weighted_token_ratio</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Weighted token similarity measure</h2>

<h3>Description</h3>

<p>Computes similarity but allows you to assign weights to specific tokens. This is useful, for
example, when you have a frequently-occurring string that doesn't contain useful information. See
examples.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lev_weighted_token_ratio(a, b, weights = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>a, b</code></td>
<td>
<p>The input strings</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>List of token weights. For example, <code>weights = list(foo = 0.9, bar = 0.1)</code>. Any
tokens omitted from <code>weights</code> will be given a weight of 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments to be passed to <code>stringdist::stringdistmatrix()</code> or
<code>stringdist::stringsimmatrix()</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A float
</p>


<h3>Details</h3>

<p>The algorithm used here is as follows:
</p>

<ul>
<li>
<p> Tokenise the input strings
</p>
</li>
<li>
<p> Compute the edit distance between each pair of tokens
</p>
</li>
<li>
<p> Compute the maximum edit distance between each pair of tokens
</p>
</li>
<li>
<p> Apply any weights from the <code>weights</code> argument
</p>
</li>
<li>
<p> Return <code>1 - (sum(weighted_edit_distances) / sum(weighted_max_edit_distance))</code>
</p>
</li>
</ul>
<h3>See Also</h3>

<p>Other weighted token functions: 
<code>lev_weighted_token_set_ratio()</code>,
<code>lev_weighted_token_sort_ratio()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">lev_weighted_token_ratio("jim ltd", "tim ltd")

lev_weighted_token_ratio("tim ltd", "jim ltd", weights = list(ltd = 0.1))
</code></pre>


</div>