<div class="container">

<table style="width: 100%;"><tr>
<td>loo-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Efficient LOO-CV and WAIC for Bayesian models</h2>

<h3>Description</h3>


<p><img src="../help/figures/stanlogo.png" width="50" alt="mc-stan.org"><em>Stan Development Team</em>
</p>
<p>This package implements the methods described in Vehtari, Gelman, and
Gabry (2017), Vehtari, Simpson, Gelman, Yao, and Gabry (2024), and
Yao et al. (2018). To get started see the <strong>loo</strong> package
<a href="https://mc-stan.org/loo/articles/index.html">vignettes</a>, the
<code>loo()</code> function for efficient approximate leave-one-out
cross-validation (LOO-CV), the <code>psis()</code> function for the Pareto
smoothed importance sampling (PSIS) algorithm, or
<code>loo_model_weights()</code> for an implementation of Bayesian stacking of
predictive distributions from multiple models.
</p>


<h3>Details</h3>

<p>Leave-one-out cross-validation (LOO-CV) and the widely applicable
information criterion (WAIC) are methods for estimating pointwise
out-of-sample prediction accuracy from a fitted Bayesian model using the
log-likelihood evaluated at the posterior simulations of the parameter
values. LOO-CV and WAIC have various advantages over simpler estimates of
predictive error such as AIC and DIC but are less used in practice because
they involve additional computational steps. This package implements the
fast and stable computations for approximate LOO-CV laid out in Vehtari,
Gelman, and Gabry (2017). From existing posterior simulation draws, we
compute LOO-CV using Pareto smoothed importance sampling (PSIS; Vehtari,
Simpson, Gelman, Yao, and Gabry, 2024), a new procedure for stabilizing
and diagnosing importance weights. As a byproduct of our calculations,
we also obtain approximate standard errors for estimated predictive
errors and for comparing of predictive errors between two models.
</p>
<p>We recommend PSIS-LOO-CV instead of WAIC, because PSIS provides useful
diagnostics and effective sample size and Monte Carlo standard error
estimates.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Jonah Gabry <a href="mailto:jsg2201@columbia.edu">jsg2201@columbia.edu</a>
</p>
<p>Authors:
</p>

<ul>
<li>
<p> Aki Vehtari <a href="mailto:Aki.Vehtari@aalto.fi">Aki.Vehtari@aalto.fi</a>
</p>
</li>
<li>
<p> Måns Magnusson
</p>
</li>
<li>
<p> Yuling Yao
</p>
</li>
<li>
<p> Paul-Christian Bürkner
</p>
</li>
<li>
<p> Topi Paananen
</p>
</li>
<li>
<p> Andrew Gelman
</p>
</li>
</ul>
<p>Other contributors:
</p>

<ul>
<li>
<p> Ben Goodrich [contributor]
</p>
</li>
<li>
<p> Juho Piironen [contributor]
</p>
</li>
<li>
<p> Bruno Nicenboim [contributor]
</p>
</li>
<li>
<p> Leevi Lindgren [contributor]
</p>
</li>
</ul>
<h3>References</h3>

<p>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413–1432. doi:10.1007/s11222-016-9696-4
(<a href="https://link.springer.com/article/10.1007/s11222-016-9696-4">journal version</a>,
<a href="https://arxiv.org/abs/1507.04544">preprint arXiv:1507.04544</a>).
</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>,
25(72):1-58.
<a href="https://jmlr.org/papers/v25/19-556.html">PDF</a>
</p>
<p>Yao, Y., Vehtari, A., Simpson, D., and Gelman, A. (2018) Using
stacking to average Bayesian predictive distributions.
<em>Bayesian Analysis</em>, advance publication,  doi:10.1214/17-BA1091.
(<a href="https://projecteuclid.org/euclid.ba/1516093227">online</a>).
</p>
<p>Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2019).
Leave-One-Out Cross-Validation for Large Data.
In <em>Thirty-sixth International Conference on Machine Learning</em>,
PMLR 97:4244-4253.
</p>
<p>Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2020).
Leave-One-Out Cross-Validation for Model Comparison in Large Data.
In <em>Proceedings of the 23rd International Conference on Artificial
Intelligence and Statistics (AISTATS)</em>, PMLR 108:341-351.
</p>
<p>Epifani, I., MacEachern, S. N., and Peruggia, M. (2008). Case-deletion
importance sampling estimators: Central limit theorems and related results.
<em>Electronic Journal of Statistics</em> <strong>2</strong>, 774-806.
</p>
<p>Gelfand, A. E. (1996). Model determination using sampling-based methods. In
<em>Markov Chain Monte Carlo in Practice</em>, ed. W. R. Gilks, S. Richardson,
D. J. Spiegelhalter, 145-162. London: Chapman and Hall.
</p>
<p>Gelfand, A. E., Dey, D. K., and Chang, H. (1992). Model determination using
predictive distributions with implementation via sampling-based methods. In
<em>Bayesian Statistics 4</em>, ed. J. M. Bernardo, J. O. Berger, A. P. Dawid,
and A. F. M. Smith, 147-167. Oxford University Press.
</p>
<p>Gelman, A., Hwang, J., and Vehtari, A. (2014). Understanding predictive
information criteria for Bayesian models. <em>Statistics and Computing</em>
<strong>24</strong>, 997-1016.
</p>
<p>Ionides, E. L. (2008). Truncated importance sampling. <em>Journal of
Computational and Graphical Statistics</em> <strong>17</strong>, 295-311.
</p>
<p>Koopman, S. J., Shephard, N., and Creal, D. (2009). Testing the assumptions
behind importance sampling. <em>Journal of Econometrics</em> <strong>149</strong>, 2-11.
</p>
<p>Peruggia, M. (1997). On the variability of case-deletion importance sampling
weights in the Bayesian linear model. <em>Journal of the American
Statistical Association</em> <strong>92</strong>, 199-207.
</p>
<p>Stan Development Team (2017). The Stan C++ Library, Version 2.17.0.
<a href="https://mc-stan.org">https://mc-stan.org</a>.
</p>
<p>Stan Development Team (2018). RStan: the R interface to Stan, Version 2.17.3.
<a href="https://mc-stan.org">https://mc-stan.org</a>.
</p>
<p>Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and
widely application information criterion in singular learning theory.
<em>Journal of Machine Learning Research</em> <strong>11</strong>, 3571-3594.
</p>
<p>Zhang, J., and Stephens, M. A. (2009). A new and efficient estimation method
for the generalized Pareto distribution. <em>Technometrics</em> <strong>51</strong>,
316-325.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://mc-stan.org/loo/">https://mc-stan.org/loo/</a>
</p>
</li>
<li> <p><a href="https://discourse.mc-stan.org">https://discourse.mc-stan.org</a>
</p>
</li>
<li>
<p> Report bugs at <a href="https://github.com/stan-dev/loo/issues">https://github.com/stan-dev/loo/issues</a>
</p>
</li>
</ul>
</div>