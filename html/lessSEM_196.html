<div class="container">

<table style="width: 100%;"><tr>
<td>lsp</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>lsp</h2>

<h3>Description</h3>

<p>Implements lsp regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \log(1 + |x_j|/\theta)</code>
</p>

<p>where <code class="reqn">\theta &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lsp(
  lavaanModel,
  regularized,
  lambdas,
  thetas,
  modifyModel = lessSEM::modifyModel(),
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta (see ?controlIsta)</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>lsp regularization:
</p>

<ul><li>
<p> Candès, E. J., Wakin, M. B., &amp; Boyd, S. P. (2008). Enhancing Sparsity by
Reweighted l1 Minimization. Journal of Fourier Analysis and Applications, 14(5–6),
877–905. https://doi.org/10.1007/s00041-008-9045-x
</p>
</li></ul>
<p>Regularized SEM
</p>

<ul>
<li>
<p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li>
<p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li>
</ul>
<p>For more details on GLMNET, see:
</p>

<ul>
<li>
<p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li>
<p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li>
<p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li>
</ul>
<p>For more details on ISTA, see:
</p>

<ul>
<li>
<p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li>
<p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li>
<p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li>
</ul>
<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- lsp(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 20),
  thetas = seq(0.01,2,length.out = 5))

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# fit Measures:
fitIndices(lsem)

# The best parameters can also be extracted with:
coef(lsem, criterion = "AIC")
# or
estimates(lsem, criterion = "AIC")

# optional: plotting the paths requires installation of plotly
# plot(lsem)
</code></pre>


</div>