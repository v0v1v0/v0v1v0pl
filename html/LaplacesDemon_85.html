<div class="container">

<table style="width: 100%;"><tr>
<td>BayesianBootstrap</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>The Bayesian Bootstrap</h2>

<h3>Description</h3>

<p>This function performs the Bayesian bootstrap of Rubin (1981),
returning either bootstrapped weights or statistics.
</p>


<h3>Usage</h3>

<pre><code class="language-R">BayesianBootstrap(X, n=1000, Method="weights", Status=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>This is a vector or matrix of data. When a matrix is
supplied, sampling is based on the first column.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>This is the number of bootstrapped replications.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Method</code></td>
<td>
<p>When <code>Method="weights"</code> (which is the default),
a matrix of row weights is returned. Otherwise, a function is
accepted. The function specifies the statistic to be
bootstrapped. The first argument of the function should be a matrix
of data, and the second argument should be a vector of weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Status</code></td>
<td>
<p>This determines the periodicity of status messages. When
<code>Status=100</code>, for example, a status message is displayed every
100 replications. Otherwise, <code>Status</code> defaults to <code>NULL</code>,
and status messages are not displayed.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The term, ‘bootstrap’, comes from the German novel <em>Adventures of
Baron Munchausen</em> by Rudolph Raspe, in which the hero saves himself
from drowning by pulling on his own bootstraps. The idea of the
statistical bootstrap is to evaluate properties of an estimator
through the empirical, rather than theoretical, CDF.
</p>
<p>Rubin (1981) introduced the Bayesian bootstrap. In contrast to the
frequentist bootstrap which simulates the sampling distribution of a
statistic estimating a parameter, the Bayesian bootstrap simulates the
posterior distribution.
</p>
<p>The data, <code class="reqn">\textbf{X}</code>, are assumed to be independent and
identically distributed (IID), and to be a representative sample of
the larger (bootstrapped) population. Given that the data has <code class="reqn">N</code>
rows in one bootstrap replication, the row weights are sampled from a
Dirichlet distribution with all <code class="reqn">N</code> concentration parameters equal
to <code class="reqn">1</code> (a uniform distribution over an open standard <code class="reqn">N-1</code>
simplex). The distributions of a parameter inferred from considering
many samples of weights are interpretable as posterior distributions
on that parameter.
</p>
<p>The Bayesian bootstrap is useful for estimating marginal posterior
covariance and standard deviations for the posterior modes of
<code>LaplaceApproximation</code>, especially when the model
dimension (the number of parameters) is large enough that estimating
the <code>Hessian</code> matrix of second partial derivatives is too
computationally demanding.
</p>
<p>Just as with the frequentist bootstrap, inappropriate use of the
Bayesian bootstrap can lead to inappropriate inferences. The Bayesian
bootstrap violates the likelihood principle, because the evaluation of
a statistic of interest depends on data sets other than the observed
data set. For more information on the likelihood principle, see
<a href="https://web.archive.org/web/20150213002158/http://www.bayesian-inference.com/likelihood#likelihoodprinciple">https://web.archive.org/web/20150213002158/http://www.bayesian-inference.com/likelihood#likelihoodprinciple</a>.
</p>
<p>The <code>BayesianBootstrap</code> function has many uses, including
creating test statistics on the population data given the observed
data (supported here), imputation (with this variation:
<code>ABB</code>), validation, and more.
</p>


<h3>Value</h3>

<p>When <code>Method="weights"</code>, this function returns a
<code class="reqn">N \times n</code> matrix of weights, where the number of rows
<code class="reqn">N</code> is equal to the number of rows in <code>X</code>.
</p>
<p>For statistics, a matrix or array is returned, depending on the number
of dimensions. The replicates are indexed by row in a matrix or in
the first dimension of the array.
</p>


<h3>Author(s)</h3>

<p>Bogumil Kaminski, <a href="mailto:bkamins@sgh.waw.pl">bkamins@sgh.waw.pl</a> and
Statisticat, LLC.</p>


<h3>References</h3>

<p>Rubin, D.B. (1981). "The Bayesian Bootstrap". <em>The Annals of
Statistics</em>, 9(1), p. 130–134.
</p>


<h3>See Also</h3>

<p><code>ABB</code>,
<code>Hessian</code>,
<code>LaplaceApproximation</code>, and
<code>LaplacesDemon</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(LaplacesDemon)

#Example 1: Samples
x &lt;- 1:2
BB &lt;- BayesianBootstrap(X=x, n=100, Method="weights"); BB

#Example 2: Mean, Univariate
x &lt;- 1:2
BB &lt;- BayesianBootstrap(X=x, n=100, Method=weighted.mean); BB

#Example 3: Mean, Multivariate
data(demonsnacks)
BB &lt;- BayesianBootstrap(X=demonsnacks, n=100,
     Method=function(x,w) apply(x, 2, weighted.mean, w=w)); BB

#Example 4: Correlation
dye &lt;- c(1.15, 1.70, 1.42, 1.38, 2.80, 4.70, 4.80, 1.41, 3.90)
efp &lt;- c(1.38, 1.72, 1.59, 1.47, 1.66, 3.45, 3.87, 1.31, 3.75)
X &lt;- matrix(c(dye,efp), length(dye), 2)
colnames(X) &lt;- c("dye","efp")
BB &lt;- BayesianBootstrap(X=X, n=100,
     Method=function(x,w) cov.wt(x, w, cor=TRUE)$cor); BB

#Example 5: Marginal Posterior Covariance
#The following example is commented out due to package build time.
#To run the following example, use the code from the examples in
#the LaplaceApproximation function for the data, model specification
#function, and initial values. Then perform the Laplace
#Approximation as below (with CovEst="Identity" and sir=FALSE) until
#convergence, set the latest initial values, then use the Bayesian
#bootstrap on the data, run the Laplace Approximation again to
#convergence, save the posterior modes, and repeat until S samples
#of the posterior modes are collected. Finally, calculate the
#parameter covariance or standard deviation.

#Fit &lt;- LaplaceApproximation(Model, Initial.Values, Data=MyData,
#     Iterations=1000, Method="SPG",  CovEst="Identity", sir=FALSE)
#Initial.Values &lt;- as.initial.values(Fit)
#S &lt;- 100 #Number of bootstrapped sets of posterior modes (parameters)
#Z &lt;- rbind(Fit$Summary1[,1]) #Bootstrapped parameters collected here
#N &lt;- nrow(MyData$X) #Number of records
#MyData.B &lt;- MyData
#for (s in 1:S) {
#     cat("\nIter:", s, "\n")
#     BB &lt;- BayesianBootstrap(MyData$y, n=N)
#     z &lt;- apply(BB, 2, function(x) sample.int(N, size=1, prob=x))
#     MyData.B$y &lt;- MyData$y[z]
#     MyData.B$X &lt;- MyData$X[z,]
#     Fit &lt;- LaplaceApproximation(Model, Initial.Values, Data=MyData.B,
#          Iterations=1000, Method="SPG", CovEst="Identity", sir=FALSE)
#     Z &lt;- rbind(Z, Fit$Summary1[,1])}
#cov(Z) #Bootstrapped marginal posterior covariance
#sqrt(diag(cov(Z))) #Bootstrapped marginal posterior standard deviations
</code></pre>


</div>