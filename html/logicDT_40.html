<div class="container">

<table style="width: 100%;"><tr>
<td>prune</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Post-pruning using a fixed complexity penalty</h2>

<h3>Description</h3>

<p>Using a fitted <code>logicDT</code> model and a fixed complexity penalty
<code>alpha</code>, its logic decision tree can be (post-)pruned.
</p>


<h3>Usage</h3>

<pre><code class="language-R">prune(model, alpha, simplify = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A fitted <code>logicDT</code> model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>A fixed complexity penalty value. This value should be
determined out-of-sample, e.g., performing hyperparameter optimization
on independent validation data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>simplify</code></td>
<td>
<p>Should the pruned model be simplified with regard to the
input terms, i.e., should terms that are no longer in the tree contained
be removed from the model?</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Similar to Breiman et al. (1984), we implement post-pruning by first
computing the optimal pruning path and then choosing the tree that is
pruned according to the specified complexity penalty.
</p>
<p>If no validation data is available or if the tree shall be automatically
optimally pruned, <code>cv.prune</code> should be used instead which
employs k-fold cross-validation for finding the best complexity penalty
value.
</p>


<h3>Value</h3>

<p>The new <code>logicDT</code> model containing the pruned tree
</p>


</div>