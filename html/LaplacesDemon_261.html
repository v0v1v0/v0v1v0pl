<div class="container">

<table style="width: 100%;"><tr>
<td>MISS</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multiple Imputation Sequential Sampling</h2>

<h3>Description</h3>

<p>This function performs multiple imputation (MI) on a numeric matrix
by sequentially sampling variables with missing values, given all
other variables in the data set.
</p>


<h3>Usage</h3>

<pre><code class="language-R">MISS(X, Iterations=100, Algorithm="GS", Fit=NULL, verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>This required argument accepts a numeric matrix of data that
contains both observed and missing values. Data set
<code class="reqn">\textbf{X}</code> must not have any rows or columns that are
completely missing. <code class="reqn">\textbf{X}</code> must not have any
constants. The user must apply any data transformations appropriate
for these models. Missing values are assumed to be Missing At Random
(MAR).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Iterations</code></td>
<td>
<p>This is the number of iterations to perform
sequential sampling via MCMC algorithms.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Algorithm</code></td>
<td>
<p>The MCMC algorithm defaults to the Gibbs Sampler (GS).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Fit</code></td>
<td>
<p>This optional argument accepts an object of class
<code>miss</code>. When supplied, <code>MISS</code> will continue where it left
off, provided the user does not change the algorithm (different
methods are used with different algortihms, so model parameters will
not match). In short, changing algorithms requires starting from
scratch.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Logical. When <code>FALSE</code>, only the iteration prints
to the console. When <code>TRUE</code>, which is the default, both the
iteration and which variable is being imputed are printed to the
console.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Imputation is a family of statistical methods for replacing missing
values with estimates. Introduced by Rubin and Schenker (1986) and
Rubin (1987), Multiple Imputation (MI) is a family of imputation
methods that includes multiple estimates, and therefore includes
variability of the estimates.
</p>
<p>The Multiple Imputation Sequential Sampler (MISS) function performs
MI by determining the type of variable and therefore the sampler for
each variable, and then sequentially progresses through each variable
in the data set that has missing values, updating its prediction of
those missing values given all other variables in the data set each
iteration.
</p>
<p>MI is best performed within a model, where it is called
full-likelihood imputation. Examples may be found in the "Examples"
vignette. However, sometimes it is impractical to impute within a
model when there are numerous missing values and a large number of
parameters are therefore added. As an alternative, MI may be
performed on the data set before the data is passed to the model,
such as in the <code>IterativeQuadrature</code>,
<code>LaplaceApproximation</code>, <code>LaplacesDemon</code>, or
<code>VariationalBayes</code> function. This is less desirable, but
MISS is available for MCMC-based MI in this case.
</p>
<p>Missing values are initially set to column means for continuous
variables, and are set to one for discrete variables.
</p>
<p>MISS uses the following methods and MCMC algorithms:
</p>
<p>Missing values of continuous variables are estimated with a
ridge-stabilized linear regression Gibbs sampler.
</p>
<p>Missing values of binary variables that have only 0 or 1 for values
are estimated either with a binary robit (t-link logistic
regression model) Gibbs sampler of Albert and Chib (1993).
</p>
<p>Missing values of discrete variables with 3 or more (ordered or
unordered) discrete values are considered continuous.
</p>
<p>In the presence of big data, it is suggested that the user
sequentially read in batches of data that are small enough to be
manageable, and then apply the MISS function to each batch. Each batch
should be representative of the whole, of course.
</p>
<p>It is common for multiple imputation functions to handle variable
transformations. MISS does not transform variables, but imputes what
it gets. For example, if a user has a variable that should be positive
only, then it is recommended here that the user log-transform the
variable, pass the data set to MISS, and when finished, exponentiate
both the observed and imputed values of that variable.
</p>
<p>The <code>CenterScale</code> function should also be considered to speed up
convergence.
</p>
<p>It is hoped that MISS is helpful, though it is not without limitation
and there are numerous alternatives outside of the
<code>LaplacesDemon</code> package. If MISS does not fulfill the needs of
the user, then the following packages are recommended: Amelia, mi, or
mice. MISS emphasizes MCMC more than these alternatives, though MISS is
not as extensive. When a data set does not have a simple structure,
such as merely continuous or binary or unordered discrete, the
<code>LaplacesDemon</code> function should be considered, where a
user can easily specify complicated structures such as multilevel,
spatial or temporal dependence, and more.
</p>
<p>Matrix inversions are required in the Gibbs sampler. Matrix inversions
become more cumbersome as the number <code class="reqn">J</code> of variables increases.
</p>
<p>If a large number of iterations is used, then the user may consider
studying the imputations for approximate convergence with the
<code>BMK.Diagnostic</code> function, by supplying the transpose of
codeFit$Imp. In the presence of numerous missing values, say more
than 100, the user may consider iterating through the study of the
imputations of 100 missing values at a time.
</p>


<h3>Value</h3>

<p>This function returns an object of class <code>miss</code> that is a list
with five components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Algorithm</code></td>
<td>
<p>This indicates which algorithm was selected.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Imp</code></td>
<td>
<p>This is a <code class="reqn">M \times T</code> matrix of <code class="reqn">M</code> missing
values and <code class="reqn">T</code> iterations that contains imputations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parm</code></td>
<td>
<p>This is a list of length <code class="reqn">J</code> for <code class="reqn">J</code> variables,
and each component of the list contains parameters associated with
the prediction of missing values for that variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>PostMode</code></td>
<td>
<p>This is a vector of posterior modes. If the user
intends to replace missing values in a data set with only one
estimate per missing values (single, not multiple imputation), then
this vector contains these values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Type</code></td>
<td>
<p>This is a vector of length <code class="reqn">J</code> for <code class="reqn">J</code> variables
that indicates the type of each variable, as MISS will consider it.
When <code>Type=1</code>, the variable is considered to be continuous.
When <code>Type=2</code>, only two discrete values (0 and 1) were found.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Statisticat, LLC <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Albert, J.H. and Chib, S. (1993). "Bayesian Analysis of Binary and
Polychotomous Response Data". <em>Journal of the American
Statistical Association</em>, 88(422), p. 669–679.
</p>
<p>Rubin, D.B. (1987). "Multiple Imputation for Nonresponse in
Surveys". John Wiley and Sons: New York, NY.
</p>
<p>Rubin, D.B. and Schenker, N. (1986). "Multiple Imputation for Interval
Estimation from Simple Random Samples with Ignorable
Nonresponse". <em>Journal of the American Statistical Association</em>,
81, p. 366–374.
</p>


<h3>See Also</h3>

<p><code>ABB</code>,
<code>BMK.Diagnostic</code>,
<code>CenterScale</code>,
<code>IterativeQuadrature</code>
<code>LaplaceApproximation</code>,
<code>LaplacesDemon</code>, and
<code>VariationalBayes</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">#library(LaplacesDemon)
### Create Data
#N &lt;- 20 #Number of Simulated Records
#J &lt;- 5 #Number of Simulated Variables
#pM &lt;- 0.25 #Percent Missing
#Sigma &lt;- as.positive.definite(matrix(runif(J*J),J,J))
#X &lt;- rmvn(N, rep(0,J), Sigma)
#m &lt;- sample.int(N*J, round(pM*N*J))
#X[m] &lt;- NA
#head(X)

### Begin Multiple Imputation
#Fit &lt;- MISS(X, Iterations=100, Algorithm="GS", verbose=TRUE)
#Fit
#summary(Fit)
#plot(Fit)
#plot(BMK.Diagnostic(t(Fit$Imp)))

### Continue Updating if Necessary
#Fit &lt;- MISS(X, Iterations=100, Algorithm="GS", Fit, verbose=TRUE)
#summary(Fit)
#plot(Fit)
#plot(BMK.Diagnostic(t(Fit$Imp)))

### Replace Missing Values in Data Set with Posterior Modes
#Ximp &lt;- X
#Ximp[which(is.na(X))] &lt;- Fit$PostMode

### Original and Imputed Data Sets
#head(X)
#head(Ximp)
#summary(X)
#summary(Ximp)

### or Multiple Data Sets, say 3
#Ximp &lt;- array(X, dim=c(nrow(X), ncol(X), 3))
#for (i in 1:3) {
#     Xi &lt;- X
#     Xi[which(is.na(X))] &lt;- Fit$Imp[,sample.int(ncol(Fit$Imp), 1)]
#     Ximp[,,i] &lt;- Xi}
#head(X)
#head(Ximp[,,1])
#head(Ximp[,,2])
#head(Ximp[,,3])

#End
</code></pre>


</div>