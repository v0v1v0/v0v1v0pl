<div class="container">

<table style="width: 100%;"><tr>
<td>adaptiveLasso</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>adaptiveLasso</h2>

<h3>Description</h3>

<p>Implements adaptive lasso regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = p( x_j) = \frac{1}{w_j}\lambda| x_j|</code>
</p>

<p>Adaptive lasso regularization will set parameters to zero if <code class="reqn">\lambda</code>
is large enough.
</p>


<h3>Usage</h3>

<pre><code class="language-R">adaptiveLasso(
  lavaanModel,
  regularized,
  weights = NULL,
  lambdas = NULL,
  nLambdas = NULL,
  reverse = TRUE,
  curve = 1,
  method = "glmnet",
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>labeled vector with weights for each of the parameters in the
model. If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object. If set to NULL,
the default weights will be used: the inverse of the absolute values of
the unregularized parameter estimates</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nLambdas</code></td>
<td>
<p>alternative to lambda: If alpha = 1, lessSEM can automatically
compute the first lambda value which sets all regularized parameters to zero.
It will then generate nLambda values between 0 and the computed lambda.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reverse</code></td>
<td>
<p>if set to TRUE and nLambdas is used, lessSEM will start with the
largest lambda and gradually decrease lambda. Otherwise, lessSEM will start with
the smallest lambda and gradually increase it.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>curve</code></td>
<td>
<p>Allows for unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20).
If curve is close to 1 all lambda values will be equally spaced, if curve is large
lambda values will be more concentrated close to 0. See ?lessSEM::curveLambda for more information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures
(currently gist).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Adaptive lasso regularization:
</p>

<ul><li>
<p> Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association,
101(476), 1418–1429. https://doi.org/10.1198/016214506000000735
</p>
</li></ul>
<p>Regularized SEM
</p>

<ul>
<li>
<p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li>
<p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li>
</ul>
<p>For more details on GLMNET, see:
</p>

<ul>
<li>
<p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li>
<p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li>
<p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li>
</ul>
<p>For more details on ISTA, see:
</p>

<ul>
<li>
<p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li>
<p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li>
<p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li>
</ul>
<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- adaptiveLasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  # in case of lasso and adaptive lasso, we can specify the number of lambda
  # values to use. lessSEM will automatically find lambda_max and fit
  # models for nLambda values between 0 and lambda_max. For the other
  # penalty functions, lambdas must be specified explicitly
  nLambdas = 50)

# use the plot-function to plot the regularized parameters:
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# fit Measures:
fitIndices(lsem)

# The best parameters can also be extracted with:
coef(lsem, criterion = "AIC")
# or
estimates(lsem, criterion = "AIC")

#### Advanced ###
# Switching the optimizer #
# Use the "method" argument to switch the optimizer. The control argument
# must also be changed to the corresponding function:
lsemIsta &lt;- adaptiveLasso(
  lavaanModel = lavaanModel,
  regularized = paste0("l", 6:15),
  nLambdas = 50,
  method = "ista",
  control = controlIsta())

# Note: The results are basically identical:
lsemIsta@parameters - lsem@parameters
</code></pre>


</div>