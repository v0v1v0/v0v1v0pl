<div class="container">

<table style="width: 100%;"><tr>
<td>RejectionSampling</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Rejection Sampling</h2>

<h3>Description</h3>

<p>The <code>RejectionSampling</code> function implements rejection sampling
of a target density given a proposal density.
</p>


<h3>Usage</h3>

<pre><code class="language-R">RejectionSampling(Model, Data, mu, S, df=Inf, logc, n=1000, CPUs=1, Type="PSOCK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Model</code></td>
<td>
<p>This is a model specification function. For more
information, see <code>LaplaceApproximation</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Data</code></td>
<td>
<p>This is a list of data. For more information, see
<code>LaplaceApproximation</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>This is a mean vector <code class="reqn">\mu</code> for the multivariate
normal or multivariate t proposal density.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>S</code></td>
<td>
<p>This is a convariance matrix <code class="reqn">\Sigma</code> for the
multivariate normal or multivariate t proposal density.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>This is a scalar degrees of freedom parameter
<code class="reqn">\nu</code>. It defaults to infinity, in which case the
multivariate normal density is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>logc</code></td>
<td>
<p>This is the logarithm of the rejection sampling constant.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>This is the number of independent draws to be simulated from
the proposal density.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CPUs</code></td>
<td>
<p>This argument accepts an integer that specifies the number
of central processing units (CPUs) of the multicore computer or
computer cluster. This argument defaults to <code>CPUs=1</code>, in which
parallel processing does not occur.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Type</code></td>
<td>
<p>This argument specifies the type of parallel processing to
perform, accepting either <code>Type="PSOCK"</code> or
<code>Type="MPI"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Rejection sampling (von Neumann, 1951) is a Monte Carlo method for
drawing independent samples from a distribution that is proportional
to the target distribution, <code class="reqn">f(x)</code>, given a sampling distribution,
<code class="reqn">g(x)</code>, from which samples can readily be drawn, and for which
there is a finite constant <code class="reqn">c</code>.
</p>
<p>Here, the target distribution, <code class="reqn">f(x)</code>, is the result of the
<code>Model</code> function. The sampling distribution, <code class="reqn">g(x)</code>, is
either a multivariate normal or multivariate t-distribution. The
parameters of <code class="reqn">g(x)</code> (<code>mu</code>, <code>S</code>, and <code>df</code>) are used
to create random draws, <code class="reqn">\theta</code>, of the sampling
distribution, <code class="reqn">g(x)</code>. These draws, <code class="reqn">\theta</code>, are used
to evaluate the target distribution, <code class="reqn">f(x)</code>, via the <code>Model</code>
specification function. The evaluations of the target distribution,
sampling distribution, and the constant are used to create a
probability of acceptance for each draw, by comparing to a vector of
<code class="reqn">n</code> uniform draws, <code class="reqn">u</code>. Each draw, <code class="reqn">\theta</code> is
accepted if </p>
<p style="text-align: center;"><code class="reqn">u \le \frac{f(\theta|\textbf{y})}{cg(\theta)}</code>
</p>

<p>Before beginning rejection sampling, a goal of the user is to find the
bounding constant, <code class="reqn">c</code>, such that <code class="reqn">f(\theta|\textbf{y}) \le
  cg(\theta)</code> for all
<code class="reqn">\theta</code>. These are all expressed in logarithms, so the
goal is to find <code class="reqn">\log f(\theta|\textbf{y}) - \log g(\theta) \le
  \log(c)</code> for all
<code class="reqn">\theta</code>. This is done by maximizing <code class="reqn">\log
  f(\theta|\textbf{y}) - \log g(\theta)</code> over all <code class="reqn">\theta</code>. By using, say,
<code>LaplaceApproximation</code> to find the modes of the
parameters of interest, and using the resultant <code>LP</code>, the mode
of the logarithm of the joint posterior distribution, as
<code class="reqn">\log(c)</code>.
</p>
<p>The <code>RejectionSampling</code> function performs one iteration of
rejection sampling. Rejection sampling is often iterated, then called
the rejection sampling algorithm, until a sufficient number or
proportion of <code class="reqn">\theta</code> is accepted. An efficient
rejection sampling algorithm has a high acceptance rate. However,
rejection sampling becomes less efficient as the model dimension (the
number of parameters) increases.
</p>
<p>Extensions of rejection sampling include Adaptive Rejection
Sampling (ARS) (either derivative-based or derivative-free) and
Adaptive Rejection Metropolis Sampling (ARMS), as in Gilks et
al. (1995). The random-walk Metropolis algorithm (Metropolis et al.,
1953) combined the rejection sampling (a method of Monte Carlo
simulation) of von Neumann (1951) with Markov chains.
</p>
<p>Parallel processing may be performed when the user specifies
<code>CPUs</code> to be greater than one, implying that the specified number
of CPUs exists and is available. Parallelization may be performed on a
multicore computer or a computer cluster. Either a Simple Network of
Workstations (SNOW) or Message Passing Interface (MPI) is used. With
small data sets and few samples, parallel processing may be slower,
due to computer network communication. With larger data sets and more
samples, the user should experience a faster run-time.
</p>
<p>This function is similar to the <code>rejectsampling</code> function in the
<code>LearnBayes</code> package.
</p>


<h3>Value</h3>

<p>The <code>RejectionSampling</code> function returns an object of class
<code>rejection</code>, which is a matrix of accepted, independent,
simulated draws from the target distribution.
</p>


<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Gilks, W.R., Best, N.G., Tan, K.K.C. (1995). "Adaptive Rejection
Metropolis Sampling within Gibbs Sampling". Journal of the Royal
Statistical Society. Series C (Applied Statistics), Vol. 44, No. 4,
p. 455–472.
</p>
<p>Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., and Teller,
E. (1953). "Equation of State Calculations by Fast Computing
Machines". Journal of Chemical Physics, 21, p. 1087-1092.
</p>
<p>von Neumann, J. (1951). "Various Techniques Used in Connection with
Random Digits. Monte Carlo Methods". National Bureau Standards, 12,
p. 36–38.
</p>


<h3>See Also</h3>

<p><code>dmvn</code>,
<code>dmvt</code>,
<code>IterativeQuadrature</code>,
<code>LaplaceApproximation</code>,
<code>LaplacesDemon</code>, and
<code>VariationalBayes</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(LaplacesDemon)
### Suppose an output object of class laplace is called Fit:
#rs &lt;- RejectionSampling(Model, MyData, mu=Fit$Summary1[,1],
#     S=Fit$Covar, df=Inf, logc=Fit$LP.Final, n=1000)
#rs
</code></pre>


</div>