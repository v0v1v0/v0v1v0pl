<div class="container">

<table style="width: 100%;"><tr>
<td>discrep.est</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Estimate Discrepancy in Calibration Model
</h2>

<h3>Description</h3>

<p>Estimates the Gaussian process discrepancy/bias and/or noise term in
a modularized calibration of a computer model (emulator) to field data,
and returns the log likelihood or posterior probability
</p>


<h3>Usage</h3>

<pre><code class="language-R">discrep.est(X, Y, Yhat, d, g, bias = TRUE, clean = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> containing
a design matrix of input locations for field data sites.  Any columns
of <code>X</code> without at least three unique input settings are dropped
in a pre-processing step
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>

<p>a vector of values with <code>length(Y) = ncol(X)</code>
containing the response from field data observations
at <code>X</code>.  A <code>Y</code>-vector with <code>length(Y) = k*ncol(X)</code>,
for positive integer <code>k</code>, can be supplied in which case
the multiple code <code>Y</code>-values will be treated as replicates
at the <code>X</code>-values
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Yhat</code></td>
<td>

<p>a vector with <code>length(Yhat) = length(Y)</code> containing
predictions at <code>X</code> from an emulator of a computer simulation
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d</code></td>
<td>

<p>a prior or initial setting for the (single/isotropic) lengthscale
parameter in a Gaussian correlation function; a (default)
<code>NULL</code> value triggers a sensible regularization (prior) and
initial setting to be generated via <code>darg</code>;
a scalar specifies an initial value, causing <code>darg</code>
to only generate the prior; otherwise,
a list or partial list matching the output
of <code>darg</code> can be used to specify a custom prior.  In the
case of a partial list, the only the missing entries will be
generated. Note that a default/generated list specifies MLE/MAP
inference for this parameter.  When specifying initial values, a
vector of length <code>nrow(XX)</code> can be provided, giving a
different initial value for each predictive location.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>g</code></td>
<td>

<p>a prior or initial setting for the nugget parameter; a 
<code>NULL</code> value causes a sensible regularization (prior) and
initial setting to be generated via <code>garg</code>; a scalar
(default <code>g = 1/1000</code>) specifies an initial value, causing <code>garg</code>
to only generate the prior; otherwise, a
list or partial list matching the output of <code>garg</code> can be used to
specify a custom prior.  In the case of a partial list, only the
missing entries will be generated. Note that a default/generated list
specifies <em>no</em> inference for this parameter; i.e., it is fixed
at its starting value, which may be appropriate for emulating 
deterministic computer code output
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias</code></td>
<td>

<p>a scalar logical indicating if a (isotropic) 
GP discrepancy should be estimated (<code>TRUE</code>)
or a Gaussian noise term only (<code>FALSE</code>)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clean</code></td>
<td>

<p>a scalar logical indicating if the C-side GP object should be freed before
returning.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Estimates an isotropic Gaussian correlation Gaussian process (GP) discrepancy
term for the difference between a computer model output (<code>Yhat</code>) and
field data observations (<code>Y</code>) at locations <code>X</code>. The computer model
predictions would typically come from a GP emulation from simulation data,
possibly via <code>aGP</code> if the computer experiment is large.
</p>
<p>This function is used primarily as a subroutine by <code>fcalib</code> which
defines an objective function for optimization in order to solve the
calibration problem via the method described by Gramacy, et al. (2015),
designed for large computer experiments.  However, once calibration is
performed this function can be useful for making comparisons to other methods.
Examples are provided in the <code>fcalib</code> documentation.
</p>
<p>When <code>bias=FALSE</code> no discrepancy is estimated; only a zero-mean 
Gaussian error distribution is assumed
</p>


<h3>Value</h3>

<p>The output object is comprised of the output of <code>jmleGP</code>, applied to a GP
object built with responses <code>Y - Yhat</code>.  That object is augmented with
a log likelihood, in <code>$ll</code>, and with a GP index <code>$gpi</code> when
<code>clean=FALSE</code>.  When <code>bias = FALSE</code> the output object retains the
same form as above, except with dummy zero-values since calling <code>jmleGP</code> is not
required
</p>


<h3>Note</h3>

<p>Note that in principle a separable correlation function could be used 
(e.g, via <code>newGPsep</code> and <code>mleGPsep</code>), however this is not implemented at this time
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 8.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p>R.B. Gramacy (2016). <em><span class="pkg">laGP</span>: Large-Scale Spatial Modeling via 
Local Approximate Gaussian Processes in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span>.</em>, Journal of Statistical 
Software, 72(1), 1-46; <a href="https://doi.org/10.18637/jss.v072.i01">doi:10.18637/jss.v072.i01</a> 
or see <code>vignette("laGP")</code>
</p>
<p>R.B. Gramacy, D. Bingham, JP. Holloway, M.J. Grosskopf, C.C. Kuranz, E. Rutter, 
M. Trantham, P.R. Drake (2015). <em>Calibrating a large computer 
experiment simulating radiative shock hydrodynamics.</em>  
Annals of Applied Statistics, 9(3) 1141-1168; preprint on arXiv:1410.3293 
<a href="https://arxiv.org/abs/1410.3293">https://arxiv.org/abs/1410.3293</a>
</p>
<p>F. Liu, M. Bayarri and J. Berger (2009). 
<em>Modularization in Bayesian analysis, with emphasis on analysis of computer 
models.</em> Bayesian Analysis, 4(1) 119-150.
</p>


<h3>See Also</h3>

<p><code>vignette("laGP")</code>,
<code>jmleGP</code>, <code>newGP</code>, <code>aGP.seq</code>, <code>fcalib</code> 
</p>


<h3>Examples</h3>

<pre><code class="language-R">## the example here combines aGP.seq and discrep.est functions; 
## it is comprised of snippets from demo("calib"), which contains
## code from the Calibration Section of vignette("laGP")

## Here we generate calibration data using a true calibration
## parameter, u, and then evaluate log posterior probabilities
## and out-of-sample RMSEs for that u value;  the fcalib 
## documentation repeats this with a single call to fcalib rather 
## than first aGP.seq and then discrep.est

## begin data-generation code identical to aGP.seq, discrep.est, fcalib
## example sections and demo("calib")

## M: computer model test functon used in Goh et al, 2013 (Technometrics)
## an elaboration of one from Bastos and O'Hagan, 2009 (Technometrics) 
M &lt;- function(x,u) 
  {
    x &lt;- as.matrix(x)
    u &lt;- as.matrix(u)
    out &lt;- (1-exp(-1/(2*x[,2]))) 
    out &lt;- out * (1000*u[,1]*x[,1]^3+1900*x[,1]^2+2092*x[,1]+60) 
    out &lt;- out / (100*u[,2]*x[,1]^3+500*x[,1]^2+4*x[,1]+20)  
    return(out)
  }
  
## bias: discrepancy function from Goh et al, 2013 
bias &lt;- function(x) 
  {
    x&lt;-as.matrix(x)   
    out&lt;- 2*(10*x[,1]^2+4*x[,2]^2) / (50*x[,1]*x[,2]+10)
    return(out)
  }

## beta.prior: marginal beta prior for u, 
## defaults to a mode at 1/2 in hypercube
beta.prior &lt;- function(u, a=2, b=2, log=TRUE)
{
  if(length(a) == 1) a &lt;- rep(a, length(u))
  else if(length(a) != length(u)) stop("length(a) must be 1 or length(u)")
  if(length(b) == 1) b &lt;- rep(b, length(u))
  else if(length(b) != length(u)) stop("length(b) must be 1 or length(u)")
  if(log) return(sum(dbeta(u, a, b, log=TRUE)))
  else return(prod(dbeta(u, a, b, log=FALSE)))
}

## tgp for LHS sampling
library(tgp)
rect &lt;- matrix(rep(0:1, 4), ncol=2, byrow=2)

## training and testing inputs
ny &lt;- 50; nny &lt;- 1000  
X &lt;- lhs(ny, rect[1:2,])    ## computer model train
XX &lt;- lhs(nny, rect[1:2,],) ## test

## true (but unknown) setting of the calibration parameter
## for the computer model
u &lt;- c(0.2, 0.1)
Zu &lt;- M(X, matrix(u, nrow=1)) 
ZZu &lt;- M(XX, matrix(u, nrow=1)) 

## field data response, biased and replicated
sd &lt;- 0.5
## Y &lt;- computer output + bias + noise
reps &lt;- 2 ## example from paper uses reps &lt;- 10
Y &lt;- rep(Zu,reps) + rep(bias(X),reps) + rnorm(reps*length(Zu), sd=sd) 
YYtrue &lt;- ZZu + bias(XX) 
## variations: remove the bias or change 2 to 1 to remove replicates

## computer model design
nz &lt;- 10000
XT &lt;- lhs(nz, rect)
nth &lt;- 1 ## number of threads to use in emulation, demo uses 8

## augment with physical model design points 
## with various u settings
XT2 &lt;- matrix(NA, nrow=10*ny, ncol=4)
for(i in 1:10) {
  I &lt;- ((i-1)*ny+1):(ny*i)
  XT2[I,1:2] &lt;- X
}
XT2[,3:4] &lt;- lhs(10*ny, rect[3:4,])
XT &lt;- rbind(XT, XT2)

## evaluate the computer model
Z &lt;- M(XT[,1:2], XT[,3:4])

## flag indicating if estimating bias/discrepancy or not
bias.est &lt;- TRUE
## two passes of ALC with MLE calculations for aGP.seq
methods &lt;- rep("alcray", 2) ## demo uses rep("alc", 2)

## set up priors
da &lt;- d &lt;- darg(NULL, XT)
g &lt;- garg(list(mle=TRUE), Y) 

## end identical data generation code

## now calculate log posterior and do out-of-sample RMSE calculation
## for true calibration parameter value (u).  You could repeat
## this for an estimate value from demo("calib"), for example
## u.hat &lt;- c(0.8236673, 0.1406989)

## first log posterior

## emulate at field-data locations Xu
Xu &lt;- cbind(X, matrix(rep(u, ny), ncol=2, byrow=TRUE))
ehat.u &lt;- aGP.seq(XT, Z, Xu, da, methods, ncalib=2, omp.threads=nth, verb=0)

## estimate discrepancy from the residual
cmle.u &lt;- discrep.est(X, Y, ehat.u$mean, d, g, bias.est, FALSE)
cmle.u$ll &lt;- cmle.u$ll + beta.prior(u)
print(cmle.u$ll)
## compare to same calculation with u.hat above

## now RMSE
## Not run: 
## predictive design with true calibration parameter
XXu &lt;- cbind(XX, matrix(rep(u, nny), ncol=2, byrow=TRUE))

## emulate at predictive design
ehat.oos.u &lt;- aGP.seq(XT, Z, XXu, da, methods, ncalib=2, 
  omp.threads=nth, verb=0)

## predict via discrepency
YYm.pred.u &lt;- predGP(cmle.u$gp, XX)

## add in emulation
YY.pred.u &lt;- YYm.pred.u$mean + ehat.oos.u$mean

## calculate RMSE
rmse.u &lt;- sqrt(mean((YY.pred.u - YYtrue)^2))
print(rmse.u)
## compare to same calculation with u.hat above

## clean up
deleteGP(cmle.u$gp)

## End(Not run)
</code></pre>


</div>