<div class="container">

<table style="width: 100%;"><tr>
<td>jsTopics</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Pairwise Jensen-Shannon Similarities (Divergences)</h2>

<h3>Description</h3>

<p>Calculates the similarity of all pairwise topic combinations using the
Jensen-Shannon Divergence.
</p>


<h3>Usage</h3>

<pre><code class="language-R">jsTopics(topics, epsilon = 1e-06, progress = TRUE, pm.backend, ncpus)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>topics</code></td>
<td>
<p>[<code>named matrix</code>]<br>
The counts of vocabularies/words (row wise) in topics (column wise).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>[<code>numeric(1)</code>]<br>
Numerical value added to <code>topics</code> to ensure computability. See details.
Default is <code>1e-06</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>progress</code></td>
<td>
<p>[<code>logical(1)</code>]<br>
Should a nice progress bar be shown? Turning it off, could lead to significantly
faster calculation. Default is <code>TRUE</code>.
If <code>pm.backend</code> is set, parallelization is done and no progress bar will be shown.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pm.backend</code></td>
<td>
<p>[<code>character(1)</code>]<br>
One of "multicore", "socket" or "mpi".
If <code>pm.backend</code> is set, <code>parallelStart</code> is
called before computation is started and <code>parallelStop</code>
is called after.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ncpus</code></td>
<td>
<p>[<code>integer(1)</code>]<br>
Number of (physical) CPUs to use. If <code>pm.backend</code> is passed,
default is determined by <code>availableCores</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The Jensen-Shannon Similarity for two topics <code class="reqn">\bm z_{i}</code> and
<code class="reqn">\bm z_{j}</code> is calculated by
</p>
<p style="text-align: center;"><code class="reqn">JS(\bm z_{i}, \bm z_{j}) = 1 - \left( KLD\left(\bm p_i, \frac{\bm p_i + \bm p_j}{2}\right) + KLD\left(\bm p_j, \frac{\bm p_i + \bm p_j}{2}\right) \right)/2</code>
</p>

<p style="text-align: center;"><code class="reqn">= 1 - KLD(\bm p_i, \bm p_i + \bm p_j)/2 - KLD(\bm p_j, \bm p_i + \bm p_j)/2 - \log(2)</code>
</p>

<p>with <code class="reqn">V</code> is the vocabulary size, <code class="reqn">\bm p_k = \left(p_k^{(1)}, ..., p_k^{(V)}\right)</code>,
and <code class="reqn">p_k^{(v)}</code> is the proportion of assignments of the
<code class="reqn">v</code>-th word to the <code class="reqn">k</code>-th topic. KLD defines the Kullback-Leibler
Divergence calculated by
</p>
<p style="text-align: center;"><code class="reqn">KLD(\bm p_{k}, \bm p_{\Sigma}) = \sum_{v=1}^{V} p_k^{(v)} \log{\frac{p_k^{(v)}}{p_{\Sigma}^{(v)}}}.</code>
</p>

<p>There is an <code>epsilon</code> added to every <code class="reqn">n_k^{(v)}</code>, the count
(not proportion) of assignments to ensure computability with respect to zeros.
</p>


<h3>Value</h3>

<p>[<code>named list</code>] with entries
</p>

<dl>
<dt><code>sims</code></dt>
<dd>
<p>[<code>lower triangular named matrix</code>] with all pairwise
similarities of the given topics.</p>
</dd>
<dt><code>wordslimit</code></dt>
<dd>
<p>[<code>integer</code>] = vocabulary size. See
<code>jaccardTopics</code> for original purpose.</p>
</dd>
<dt><code>wordsconsidered</code></dt>
<dd>
<p>[<code>integer</code>] = vocabulary size. See
<code>jaccardTopics</code> for original purpose.</p>
</dd>
<dt><code>param</code></dt>
<dd>
<p>[<code>named list</code>] with parameter specifications for
<code>type</code> [<code>character(1)</code>] <code>= "Cosine Similarity"</code> and
<code>epsilon</code> [<code>numeric(1)</code>]. See above for explanation.</p>
</dd>
</dl>
<h3>See Also</h3>

<p>Other TopicSimilarity functions: 
<code>cosineTopics()</code>,
<code>dendTopics()</code>,
<code>getSimilarity()</code>,
<code>jaccardTopics()</code>,
<code>rboTopics()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">res = LDARep(docs = reuters_docs, vocab = reuters_vocab, n = 4, K = 10, num.iterations = 30)
topics = mergeTopics(res, vocab = reuters_vocab)
js = jsTopics(topics)
js

sim = getSimilarity(js)
dim(sim)

js1 = jsTopics(topics, epsilon = 1)
sim1 = getSimilarity(js1)
summary((sim1-sim)[lower.tri(sim)])
plot(sim, sim1, xlab = "epsilon = 1e-6", ylab = "epsilon = 1")

</code></pre>


</div>