<div class="container">

<table style="width: 100%;"><tr>
<td>lime-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>lime: Local Interpretable Model-Agnostic Explanations</h2>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style="float: right" alt="logo" width="120"></p>
<p>When building complex models, it is often difficult to explain why the model should be trusted. While global measures such as accuracy are useful, they cannot be used for explaining why a model made a specific prediction. 'lime' (a port of the 'lime' 'Python' package) is a method for explaining the outcome of black box models by fitting a local model around the point in question an perturbations of this point. The approach is described in more detail in the article by Ribeiro et al. (2016) <a href="https://arxiv.org/abs/1602.04938">arXiv:1602.04938</a>.
</p>


<h3>Details</h3>

<p>This package is a port of the original Python lime package implementing the
prediction explanation framework laid out Ribeiro <em>et al.</em> (2016). The
package supports models from <code>caret</code> and <code>mlr</code> natively, but see
the docs for how to make it work for any model.
</p>
<p><strong>Main functions:</strong>
</p>
<p>Use of <code>lime</code> is mainly through two functions. First you create an
<code>explainer</code> object using the <code>lime()</code> function based on the training data and
the model, and then you can use the <code>explain()</code> function along with new data
and the explainer to create explanations for the model output.
</p>
<p>Along with these two functions, <code>lime</code> also provides the <code>plot_features()</code>
and <code>plot_text_explanations()</code> function to visualise the explanations
directly.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Emil Hvitfeldt <a href="mailto:emilhhvitfeldt@gmail.com">emilhhvitfeldt@gmail.com</a> (<a href="https://orcid.org/0000-0002-0679-1945">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li>
<p> Thomas Lin Pedersen <a href="mailto:thomasp85@gmail.com">thomasp85@gmail.com</a> (<a href="https://orcid.org/0000-0002-5147-4711">ORCID</a>)
</p>
</li>
<li>
<p> MichaÃ«l Benesty <a href="mailto:michael@benesty.fr">michael@benesty.fr</a>
</p>
</li>
</ul>
<h3>References</h3>

<p>Ribeiro, M.T., Singh, S., Guestrin, C. <em>"Why Should I Trust You?": Explaining the Predictions of Any Classifier</em>. 2016, <a href="https://arxiv.org/abs/1602.04938">https://arxiv.org/abs/1602.04938</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://lime.data-imaginist.com">https://lime.data-imaginist.com</a>
</p>
</li>
<li> <p><a href="https://github.com/thomasp85/lime">https://github.com/thomasp85/lime</a>
</p>
</li>
<li>
<p> Report bugs at <a href="https://github.com/thomasp85/lime/issues">https://github.com/thomasp85/lime/issues</a>
</p>
</li>
</ul>
</div>