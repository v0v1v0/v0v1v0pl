<div class="container">

<table style="width: 100%;"><tr>
<td>luz_callback_gradient_clip</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Gradient clipping callback</h2>

<h3>Description</h3>

<p>By adding the GradientClip callback, the gradient <code>norm_type</code> (default:2) norm
is clipped to at most <code>max_norm</code> (default:1) using <code>torch::nn_utils_clip_grad_norm_()</code>,
which can avoid loss divergence.
</p>


<h3>Usage</h3>

<pre><code class="language-R">luz_callback_gradient_clip(max_norm = 1, norm_type = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>max_norm</code></td>
<td>
<p>(float or int): max norm of the gradients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>norm_type</code></td>
<td>
<p>(float or int): type of the used p-norm. Can be <code>Inf</code> for
infinity norm.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>See FastAI <a href="https://docs.fast.ai/callback.training.html#GradientClip">documentation</a>
for the GradientClip callback.
</p>


</div>