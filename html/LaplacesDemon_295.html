<div class="container">

<table style="width: 100%;"><tr>
<td>PMC</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Population Monte Carlo</h2>

<h3>Description</h3>

<p>The <code>PMC</code> function updates a model with Population Monte Carlo.
Given a model specification, data, and initial values, <code>PMC</code>
maximizes the logarithm of the unnormalized joint
posterior density and provides samples of the marginal
posterior distributions, deviance, and other monitored variables.
</p>


<h3>Usage</h3>

<pre><code class="language-R">PMC(Model, Data, Initial.Values, Covar=NULL, Iterations=10, Thinning=1,
alpha=NULL, M=1, N=1000, nu=9, CPUs=1, Type="PSOCK")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Model</code></td>
<td>
<p>This is a model specification function. For more
information, see <code>LaplacesDemon</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Initial.Values</code></td>
<td>
<p>This is either a vector initial values, one for
each of <code class="reqn">K</code> parameters, or in the case of a mixture of <code class="reqn">M</code>
components, this is a <code class="reqn">M \times K</code> matrix of initial
values. If all initial values are zero in this vector, or in the
first row of a matrix, then <code>LaplaceApproximation</code> is
used to optimize initial values, in which case all mixture
components receive the same initial values and covariance matrix
from the object of class <code>laplace</code>. Parameters must be
continuous.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Data</code></td>
<td>
<p>This is a list of data. For more information, see
<code>LaplacesDemon</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Covar</code></td>
<td>
<p>This is a <code class="reqn">K \times K</code> covariance matrix for
<code class="reqn">K</code> parameters, or for multiple mixture components, this is a
<code class="reqn">K \times K \times M</code> array of <code class="reqn">M</code> covariance
matrices, where <code class="reqn">M</code> is the number of mixture components.
<code>Covar</code> defaults to <code>NULL</code>, in which case a scaled
identity matrix (with the same scale as in
<code>LaplacesDemon</code>) is applied to all mixture components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Iterations</code></td>
<td>
<p>This is the number of iterations during which PMC
will update the model. Updating the model for only one iteration is
the same as applying non-adaptive importance sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Thinning</code></td>
<td>
<p>This is the number by which the posterior is
thinned. To have 1,000 posterior samples with <code>M=3</code> mixture
components and <code>N=10000</code> samples each, <code>Thinning=30</code>. For
more information, see the <code>Thin</code> function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>This is a vector of length <code class="reqn">M</code>, the number of mixture
components. <code class="reqn">\alpha</code> is the probability of each mixture
component. The default value is <code>NULL</code>, which assigns an equal
probability to each component.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>M</code></td>
<td>
<p>This is the number <code class="reqn">M</code> of multivariate t distribution
mixture components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>N</code></td>
<td>
<p>This is the number <code class="reqn">N</code> of samples per mixture
component. The required number of samples increases with the number
<code class="reqn">K</code> of parameters. These samples are also called walkers or
particles.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nu</code></td>
<td>
<p>This is the degrees of freedom parameter <code class="reqn">\nu</code> for
the multivariate t distribution for each mixture component. If a
multivariate normal distribution is preferred, then set
<code class="reqn">\nu &gt; 1e4</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CPUs</code></td>
<td>
<p>This argument is required for parallel processing, and
and indicates the number of central processing units (CPUs) of the
computer or cluster. For example, when a user has a quad-core
computer, <code>CPUs=4</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Type</code></td>
<td>
<p>This argument defaults to <code>"PSOCK"</code> and uses the
Simple Network of Workstations (SNOW) for parallelization.
Alternatively, <code>Type="MPI"</code> may be specified to use Message
Passing Interface (MPI) for parallelization.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>PMC</code> function uses the adaptive importance sampling
algorithm of Wraith et al. (2009), also called Mixture PMC or M-PMC
(Cappe et al., 2008). Iterative adaptive importance sampling was
introduced in the 1980s. Modern PMC was introduced (Cappe et al.,
2004), and extended to multivariate Gaussian or t-distributed mixtures
(Cappe et al., 2008). This version uses a multivariate t distribution
for each mixture component, and also allows a multivariate normal
distribution when the degrees of freedom, <code class="reqn">\nu &gt; 1e4</code>. At each iteration, a mixture distribution is sampled with
importance sampling, and the samples (or populations) are adapted to
improve the importance sampling. Adaptation is a variant of EM
(Expectation-Maximization). The sample is self-normalized, and is an
example of self-normalized importance sampling (SNIS), or
self-importance sampling. The vector <code class="reqn">\alpha</code> contains the
probability of each mixture component. These, as well as multivariate
t distribution mixture parameters (except <code class="reqn">\nu</code>), are adapted
each iteration.
</p>
<p>Advantages of PMC over MCMC include:
</p>

<ul>
<li>
<p> It is difficult to assess convergence of MCMC chains, and this
is not necessary in PMC (Wraith et al., 2009).  
</p>
</li>
<li>
<p> MCMC chains have autocorrelation that effectively reduces
posterior samples. PMC produces independent samples that are not
reduced with autocorrelation.
</p>
</li>
<li>
<p> PMC has been reported to produce samples with less variance
than MCMC.
</p>
</li>
<li>
<p> It is difficult to parallelize MCMC. Posterior samples from
parallel chains can be pooled when all chains have converged, but
until this occurs, parallelization is unhelpful. PMC, on the other
hand, can parallelize the independent, Monte Carlo samples during each
iteration and reduce run-time as the number of processors
increases. Currently, PMC is not parallelized here.
</p>
</li>
<li>
<p> The multivariate mixture in PMC can represent a multimodal
posterior, where MCMC with parallel chains may be used to identify a
multimodal posterior, but probably will not yield combined samples
that proportionally represent it.
</p>
</li>
</ul>
<p>Disadvantages of PMC, compared to MCMC, include:
</p>

<ul>
<li>
<p> In PMC, the required number of samples at each iteration
increases quickly with respect to an increase in parameters. MCMC is
more suitable for models with large numbers of parameters, and
therefore, MCMC is more generalizable.
</p>
</li>
<li>
<p> PMC is more sensitive to initial values than MCMC, especially
as the number of parameters increases.
</p>
</li>
<li>
<p> PMC is more sensitive to the initial covariance matrix (or
matrices for mixture components) than adaptive MCMC. PMC requires more
information about the target distributions before updating. The
covariance matrix from a converged iterative quadrature algorithm,
Laplace Approximation, or Variational Bayes may be required (see
<code>IterativeQuadrature</code>, <code>LaplaceApproximation</code>,
or <code>VariationalBayes</code> for more information).
</p>
</li>
</ul>
<p>Since PMC requires better initial information than iterative
quadrature, Laplace Approximation, MCMC, and Variational Bayes, it is
not recommended to begin updating a model that has little prior
information with PMC, especially when the model has more than a few
parameters. Instead, iterative quadrature, Laplace Approximation,
MCMC, or Variational Bayes should be used. However, once convergence
is found or assumed, it is recommended to attempt to update the model
with PMC, given the latest parameters and convariance matrix from
iterative quadrature, Laplace Approximation, MCMC, or Variational
Bayes. Used in this way, PMC may improve the model fit obtained with
MCMC and should reduce the variance of the marginal posterior
distributions,  which is desirable for predictive modeling.
</p>
<p>Convergence is assessed by observing two outputs: normalized effective
sample size (<code>ESSN</code>) and normalized perplexity
(<code>Perplexity</code>). These are described below. PMC is considered to
have converged when these diagnostics stabilize (Wraith et al., 2009),
or when the normalized perplexity becomes sufficiently close to 1
(Cappe et al., 2008). If they do not stabilize, then it is suggested
to begin PMC again with a larger number <code>N</code> of samples, and
possibly with different initial values and covariance matrix or
matrices. <code>IterativeQuadrature</code>,
<code>LaplaceApproximation</code>, or <code>VariationalBayes</code>
may be helpful to provide better starting values for <code>PMC</code>.
</p>
<p>If a message appears that warns about ‘bad weights’, then <code>PMC</code>
is attempting to work with an iteration in which importance weights
are problematic. If this occurs in the first iteration, then all
importance weights are set to <code class="reqn">1/N</code>. If this occurs in other
iterations, then the information from the previous iteration is used
instead and different draws are made from that importance
distribution. This may allow <code>PMC</code> to eventually adapt
successfully to the target. If not, the user is advised to begin again
with a larger number <code class="reqn">N</code> of samples, and possibly different
initial values and covariance matrix or matrices, as above. PMC can
experience difficulty when it begins with poor initial conditions.
</p>
<p>The user may combine samples from previous iterations with samples
from the latest iteration for inference, if the algorithm converged
before the last iteration. Currently, a function is not provided for
combining previous samples.
</p>


<h3>Value</h3>

<p>The returned object is an object of class <code>pmc</code> with the
following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>This is a <code class="reqn">M \times T</code> matrix of the
probabilities of mixture components, where <code class="reqn">M</code> is the number of
mixture components and <code class="reqn">T</code> is the number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Call</code></td>
<td>
<p>This is the matched call of <code>PMC</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Covar</code></td>
<td>
<p>This stores the <code class="reqn">K \times K \times T \times M</code> proposal covariance matrix in an array, where <code class="reqn">K</code> is
the dimension or number of parameters or initial values, <code class="reqn">T</code> is
the number of iterations, and <code class="reqn">M</code> is the number of mixture
components. If the model is updated in the future, then the latest
covariance matrix for each mixture component can be extracted and
used to start the next update where the last update left off.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Deviance</code></td>
<td>
<p>This is a vector of the deviance of the model, with a
length equal to the number of thinned samples that were retained.
Deviance is useful for considering model fit, and is equal to the
sum of the log-likelihood for all rows in the data set, which is
then multiplied by negative two.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>DIC</code></td>
<td>
<p>This is a vector of three values: Dbar, pD, and DIC. Dbar
is the mean deviance, pD is a measure of model complexity indicating
the effective number of parameters, and DIC is the Deviance
Information Criterion, which is a model fit statistic that is the
sum of Dbar and pD. <code>DIC</code> is calculated over the thinned
samples. Note that pD is calculated as <code>var(Deviance)/2</code> as in
Gelman et al. (2004).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ESSN</code></td>
<td>
<p>This is a vector of length <code class="reqn">T</code> that contains the
normalized effective sample size (ESSN) per iteration across <code class="reqn">T</code>
iterations. ESSN is used as a convergence diagnostic. ESSN is
normalized between zero and one, and can be interpreted as the
proportion of samples with non-zero weight. Higher is better.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Initial.Values</code></td>
<td>
<p>This is the vector or matrix of
<code>Initial.Values</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Iterations</code></td>
<td>
<p>This reports the number of <code>Iterations</code> for
updating.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>LML</code></td>
<td>
<p>This is an approximation of the logarithm of the marginal
likelihood of the data (see the <code>LML</code> function for more
information). <code>LML</code> is estimated with nonparametric
self-normalized importance sampling (NSIS), given LL and the
marginal posterior samples of the parameters. <code>LML</code> is useful
for comparing multiple models with the <code>BayesFactor</code>
function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>M</code></td>
<td>
<p>This reports the number of mixture components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Minutes</code></td>
<td>
<p>This indicates the number of minutes that <code>PMC</code>
was running, and this includes the initial checks as well as time it
took to perform final sampling and create summaries.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Model</code></td>
<td>
<p>This contains the model specification <code>Model</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>N</code></td>
<td>
<p>This is the number of un-thinned samples per mixture
component.</p>
</td>
</tr>
</table>
<p>itemnuThis is the degrees of freedom parameter <code class="reqn">\nu</code> for
each multivariate t distribution in each mixture component.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Mu</code></td>
<td>
<p>This is a <code class="reqn">T \times K \times M</code> array of
means for the importance sampling distribution across <code class="reqn">T</code>
iterations, <code class="reqn">K</code> parameters, and <code class="reqn">M</code> mixture components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Monitor</code></td>
<td>
<p>This is a <code class="reqn">S \times J</code> matrix of
thinned samples of monitored variables, where <code class="reqn">S</code> is the number
of thinned samples and <code class="reqn">J</code> is the number of monitored
variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Parameters</code></td>
<td>
<p>This reports the number <code class="reqn">K</code> of parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Perplexity</code></td>
<td>
<p>This is a vector of length <code class="reqn">T</code> that contains the
normalized perplexity per iteration across <code class="reqn">T</code> iterations, and
is used as a convergence diagnostic. Perplexity is an approximation
of the negative of the Kullback-Leibler divergence (see
<code>KLD</code>) between the target and the importance function.
Perplexity is normalized between zero and one, and a higher
normalized perplexity relates to less divergence, so higher is
better. A normalized perplexity that is close to one indicates good
agreement between the target density and the importance
function. This is based on the Shannon entropy of the normalized
importance weights, which is used frequently to measure the quality
of importance samples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Posterior1</code></td>
<td>
<p>This is an <code class="reqn">N \times K \times T \times M</code> array of un-thinned posterior samples across <code class="reqn">N</code>
samples, <code class="reqn">K</code> parameters, <code class="reqn">T</code> iterations, and <code class="reqn">M</code> mixture
components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Posterior2</code></td>
<td>
<p>This is a <code class="reqn">S \times K</code> matrix of
thinned posterior samples, where <code class="reqn">S</code> is the number of thinned
samples and <code class="reqn">K</code> is the number of parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Summary</code></td>
<td>
<p>This is a matrix that summarizes the marginal
posterior distributions of the parameters, deviance, and monitored
variables from thinned samples. The following summary statistics are
included: mean, standard deviation, MCSE (Monte Carlo Standard
Error), ESS is the effective sample size due to autocorrelation, and
finally the 2.5%, 50%, and 97.5% quantiles are reported. MCSE is
essentially a standard deviation around the marginal posterior mean
that is due to uncertainty associated with using Monte Carlo
sampling. The acceptable size of the MCSE depends on the acceptable
uncertainty associated around the marginal posterior mean. The
default <code>IMPS</code> method is used. Next, the desired precision of
ESS depends on the user's goal.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Thinned.Samples</code></td>
<td>
<p>This is the number of thinned samples in
<code>Posterior2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Thinning</code></td>
<td>
<p>This is the amount of thinning requested by the user.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>W</code></td>
<td>
<p>This is a <code class="reqn">N \times T</code> matrix of normalized
importance weights, where <code class="reqn">N</code> is the number of un-thinned
samples per mixture component and <code class="reqn">T</code> is the number of
iterations. Computationally, the algorithm uses the logarithm of the
weights.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Statisticat, LLC. <a href="mailto:software@bayesian-inference.com">software@bayesian-inference.com</a></p>


<h3>References</h3>

<p>Cappe, O., Douc, R., Guillin, A., Marin, J.M., and Robert, C. (2008).
"Adaptive Importance Sampling in General Mixture Classes".
<em>Statistics and Computing</em>, 18, p. 587–600.
</p>
<p>Cappe, O., Guillin, A., Marin, J.M., and Robert, C. (2004). "Population 
Monte Carlo". <em>Journal of Computational and Graphical
Statistics</em>, 13, p. 907–929.
</p>
<p>Gelman, A., Carlin, J., Stern, H., and Rubin, D. (2004). "Bayesian
Data Analysis, Texts in Statistical Science, 2nd ed.". Chapman and
Hall, London.
</p>
<p>Wraith, D., Kilbinger, M., Benabed, K., Cappe, O., Cardoso, J.F.,
Fort, G., Prunet, S., and Robert, C.P. (2009). "Estimation of
Cosmological Parameters Using Adaptive Importance Sampling".
<em>Physical Review D</em>, 80(2), p. 023507.
</p>


<h3>See Also</h3>

<p><code>BayesFactor</code>,
<code>IterativeQuadrature</code>,
<code>LaplaceApproximation</code>,
<code>LML</code>,
<code>PMC.RAM</code>,
<code>Thin</code>, and
<code>VariationalBayes</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># The accompanying Examples vignette is a compendium of examples.
####################  Load the LaplacesDemon Library  #####################
library(LaplacesDemon)

##############################  Demon Data  ###############################
data(demonsnacks)
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(log(demonsnacks[,c(1,4,10)]+1)))
J &lt;- ncol(X)
for (j in 2:J) X[,j] &lt;- CenterScale(X[,j])

#########################  Data List Preparation  #########################
mon.names &lt;- "LP"
parm.names &lt;- as.parm.names(list(beta=rep(0,J), sigma=0))
pos.beta &lt;- grep("beta", parm.names)
pos.sigma &lt;- grep("sigma", parm.names)
PGF &lt;- function(Data) {
     beta &lt;- rnorm(Data$J)
     sigma &lt;- runif(1)
     return(c(beta, sigma))
     }
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, pos.beta=pos.beta, pos.sigma=pos.sigma, y=y)

##########################  Model Specification  ##########################
Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[Data$pos.beta]
     sigma &lt;- interval(parm[Data$pos.sigma], 1e-100, Inf)
     parm[Data$pos.sigma] &lt;- sigma
     ### Log-Prior
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu &lt;- tcrossprod(Data$X, t(beta))
     LL &lt;- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=LP,
          yhat=rnorm(length(mu), mu, sigma), parm=parm)
     return(Modelout)
     }

set.seed(666)

############################  Initial Values  #############################
Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)

########################  Population Monte Carlo  #########################
Fit &lt;- PMC(Model, MyData, Initial.Values, Covar=NULL, Iterations=5,
     Thinning=1, alpha=NULL, M=1, N=100, CPUs=1)
Fit
print(Fit)
PosteriorChecks(Fit)
caterpillar.plot(Fit, Parms="beta")
plot(Fit, BurnIn=0, MyData, PDF=FALSE)
Pred &lt;- predict(Fit, Model, MyData, CPUs=1)
summary(Pred, Discrep="Chi-Square")
plot(Pred, Style="Covariates", Data=MyData)
plot(Pred, Style="Density", Rows=1:9)
plot(Pred, Style="ECDF")
plot(Pred, Style="Fitted")
plot(Pred, Style="Jarque-Bera")
plot(Pred, Style="Predictive Quantiles")
plot(Pred, Style="Residual Density")
plot(Pred, Style="Residuals")
Levene.Test(Pred)
Importance(Fit, Model, MyData, Discrep="Chi-Square")

#End
</code></pre>


</div>