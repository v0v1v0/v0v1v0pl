<div class="container">

<table style="width: 100%;"><tr>
<td>kest.lmridge</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Computation of Ridge Biasing Parameter <code class="reqn">K</code>
</h2>

<h3>Description</h3>

<p>The <code>kest</code> function computes different biasing parameters available in the literature proposed by different researchers.
</p>


<h3>Usage</h3>

<pre><code class="language-R">kest(object, ...)
## S3 method for class 'lmridge'
kest(object, ...)
## S3 method for class 'klmridge'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>An object of class "lmridge" for the <code>kest</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>An object of class "klmridge" for the <code>print.kest.klmridge</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>kest</code> function computes different biasing parameter for the ordinary linear ridge regression. All these methods are already available in the literature and proposed by various authors. See reference section.</p>


<h3>Value</h3>

<p>The function returns the list of following biasing parameter methods proposed by various researchers.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>mHKB</code></td>
<td>
<p>By Thisted (1976), <code class="reqn">\frac{((p-2)*\hat{\sigma}^2)}{\sum(\beta^2)}</code> </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>LW</code></td>
<td>
<p>As in <code>lm.ridge</code> of <code>MASS</code> <code class="reqn">\frac{((p-2)*\hat{\sigma}^2*n)}{\sum(\hat{y}^2)}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>LW76</code></td>
<td>
<p>By Lawless and Wang (1976), <code class="reqn">\frac{p*\hat{\sigma}^22}{\sum(\lambda_j*\hat{\alpha}_j^2)}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CV</code></td>
<td>
<p>Value of Cross Validation (CV) for each biasing parameter <code class="reqn">K</code>, <code class="reqn">CV_k=\frac{1}{n}\sum_{j=1}^n (y_i-X_j \hat{\beta}_{j_K})^2 </code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kCV</code></td>
<td>
<p>Value of biasing parameter at which CV is small.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>HKB</code></td>
<td>
<p>By Hoerl and Kennard (1970), <code class="reqn">\frac{p*\hat{\sigma}^2}{\hat{\beta}'\hat{\beta}}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kibAM</code></td>
<td>
<p>By Kibria (2003), <code class="reqn">\frac{1}{p}*\sum(\frac{\hat{\sigma}^2}{\hat{\beta}_j^2)}</code> </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>GCV</code></td>
<td>
<p>Value of Generalized Cross Validation (GCV) for each biasing parameter <code class="reqn">K</code>, <code class="reqn">\frac{(y_i-X_j\hat{\beta}_{J_K})^2}{[n-(1+Trace(H_{R,k}))]^2}</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kcGCV</code></td>
<td>
<p>Value of biasing parameter at which GCV is small.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>DSK</code></td>
<td>
<p>By Dwividi and Shrivastava, (1978), <code class="reqn">\frac{\hat{\sigma}^2}{\hat{\beta}'\hat{\beta}}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kibGM</code></td>
<td>
<p>By Kibria (2003), <code class="reqn">\frac{\hat{\sigma}^2}{(\prod(\hat{\alpha}_j^2))^(1/p)}</code> </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kibMEd</code></td>
<td>
<p>By Kibria (2003), <code class="reqn">median(\frac{\hat{\sigma}^2}{\hat{\alpha}_j^2})</code> </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KM2</code></td>
<td>
<p>By Muniz and Kibria (2009), <code class="reqn">max[\frac{1}{\sqrt{\frac{\hat{\sigma}^2}{\hat{\alpha}^2_j}}}]</code> </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KM3</code></td>
<td>
<p>By Muniz and Kibria (2009), <code class="reqn">max[\sqrt{\frac{\hat{\sigma}^2}{\hat{\alpha}_j^2}}]</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KM4</code></td>
<td>
<p>By Muniz and Kibria (2009), <code class="reqn">[\prod\frac{1}{\sqrt{\frac{\hat{\sigma}^2}{\hat{\alpha}_j^2}}}]^\frac{1}{p}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KM5</code></td>
<td>
<p>By Muniz and Kibria (2009), <code class="reqn">[\prod \sqrt{\frac{\hat{\sigma}^2}{\hat{\alpha}_j^2}}]^{\frac{1}{p}}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KM6</code></td>
<td>
<p>By Muniz and Kibria (2009), <code class="reqn">Median[\frac{1}{\sqrt{\frac{\hat{\sigma}^2}{\hat{\alpha}^2_j}}}]</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KM8</code></td>
<td>
<p>By Muniz <em>et al.</em> (2012), <code class="reqn">max(\frac{1}{\sqrt{\frac{\lambda_{max} \hat{\sigma}^2} {(n-p)\hat{\sigma}^2+\lambda_{max}\hat{\alpha}^2_j}}})</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KM9</code></td>
<td>
<p>By Muniz <em>et al.</em> (2012), <code class="reqn">max[\sqrt{\frac{\lambda_{max}\hat{\sigma}^2}{(n-p)\hat{\sigma}^2}+\lambda_{max}\hat{\alpha}^2_j}]</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KM10</code></td>
<td>
<p>By Muniz <em>et al.</em> (2012), <code class="reqn">[\prod(\frac{1}{\sqrt{\frac{\lambda_{max}\hat{\sigma}^2}{(n-p)\hat{\sigma}^2+\lambda_{max}\hat{\alpha}^2_j}}})]^{\frac{1}{p}}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KM11</code></td>
<td>
<p>By Muniz <em>et al.</em> (2012), <code class="reqn">[\prod(\sqrt{\frac{\lambda_{max}\hat{\sigma}^2}{(n-p) \hat{\sigma}^2+\lambda_{max}\hat{\alpha}^2_j}})^{\frac{1}{p}}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KM12</code></td>
<td>
<p>By Muniz <em>et al.</em>, <code class="reqn">Median[\frac{1}{\sqrt{\frac{\lambda_{max}\hat{\sigma}^2}{(n-p)\hat{\sigma}^2+\lambda_{max}\hat{\alpha}^2_j}}}]</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KD</code></td>
<td>
<p>By Dorugade and Kashid (2012), <code class="reqn">0, \frac{p\hat{\sigma}^2}{\hat{\alpha}'\hat{\alpha}}-\frac{1}{n(VIF_j)_{max}}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>KAD4</code></td>
<td>
<p>By Dorugade and Kashid (2012), <code class="reqn">HM[\frac{2p}{\lambda_{max}} \sum(\frac{\hat{\sigma}^2}{\hat{\alpha}^2_j})]</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alphahat</code></td>
<td>
<p>The OLS estimator in canonical form, i.e., <code class="reqn">\hat{\alpha}=(P'X'XP)^{-1}X'^*y</code>, where <code class="reqn">X^*=XP</code> <code class="reqn">P</code> is eigenvector of <code class="reqn">X'X</code>.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Dorugade, A. and Kashid, D. (2010). Alternative Method for Choosing Ridge Parameter for Regression. <em>Applied Mathematical Sciences</em>, <strong>4</strong>(9), 447-456.
</p>
<p>Dorugade, A. (2014). New Ridge Parameters for Ridge Regression. <em>Journal of the Association of Arab Universities for Basic and Applied Sciences</em>, <strong>15</strong>, 94-99. <a href="https://doi.org/10.1016/j.jaubas.2013.03.005">doi:10.1016/j.jaubas.2013.03.005</a>.
</p>
<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>
<p>Kibria, B. (2003). Performance of Some New Ridge Regression Estimators. <em>Communications in Statistics-Simulation and Computation</em>, <strong>32</strong>(2), 491-435. <a href="https://doi.org/10.1081/SAC-120017499">doi:10.1081/SAC-120017499</a>.
</p>
<p>Lawless, J., and Wang, P. (1976). A Simulation Study of Ridge and Other Regression Estimators. <em>Communications in Statistics-Theory and Methods</em>, <strong>5</strong>(4), 307-323. <a href="https://doi.org/10.1080/03610927608827353">doi:10.1080/03610927608827353</a>.
</p>
<p>Muniz, G., and Kibria, B. (2009). On Some Ridge Regression Estimators: An Empirical Comparisons. <em>Communications in Statistics-Simulation and Computation</em>, <strong>38</strong>(3), 621-630. <a href="https://doi.org/10.1080/03610910802592838">doi:10.1080/03610910802592838</a>.
</p>
<p>Muniz, G., Kibria, B., Mansson, K., and Shukur, G. (2012). On developing Ridge Regression Parameters: A Graphical Investigation. <em>SORT-Statistics and Operations Research Transactions</em>, <strong>36</strong>(2), 115â€“138.
</p>
<p>Thisted, R. A. (1976). Ridge Regression, Minimax Estimation and Empirical Bayes Methods. <em>Technical Report 28, Division of Biostatistics</em>, Stanford University, California.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002). <em>Modern Applied Statistics with S</em>. Springer New York, 4th edition, ISBN 0-387-95457-0.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code>lmridge</code>, Ridge Var-Cov matrix <code>vcov</code></p>


<h3>Examples</h3>

<pre><code class="language-R">mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, 0.2, 0.001))
kest(mod)

## GCV values
kest(mod)$GCV

## minimum GCV value at certain k
kest(mod)$kGCV

## CV Values
kest(mod)$CV

## minimum CV value at certain k
kest(mod)$kCV

## Hoerl and Kennard (1970)
kest(mod)$HKB


</code></pre>


</div>